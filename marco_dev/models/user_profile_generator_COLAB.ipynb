{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cbd764960fb562ac",
      "metadata": {
        "id": "cbd764960fb562ac"
      },
      "source": [
        "# Script to Create User Profiles based on Past Item Interactions - [GOOGLE COLAB]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install missing libraries\n",
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqLbvK-KtwT2",
        "outputId": "ba0fb567-ad15-4cef-dfe4-8d29c1b15d7b"
      },
      "id": "yqLbvK-KtwT2",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "710c751b4d41e180",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:03.585863Z",
          "start_time": "2025-02-15T23:44:58.968305Z"
        },
        "id": "710c751b4d41e180"
      },
      "outputs": [],
      "source": [
        "#@title Import Libraries\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import io\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eadoi3fZq-fW",
        "outputId": "fdafb91e-ad9b-402d-e0db-8df7ace4b5bb"
      },
      "id": "eadoi3fZq-fW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFj79baFJy1T",
        "outputId": "231ba7de-85a5-4225-b104-b873ea16be60"
      },
      "id": "vFj79baFJy1T",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `MIDS-Capstone` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `MIDS-Capstone`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5c7095f1-ac36-49a5-b01e-55831a46a894",
      "metadata": {
        "id": "5c7095f1-ac36-49a5-b01e-55831a46a894"
      },
      "outputs": [],
      "source": [
        "# Function to download the data - AWS\n",
        "def load_dict_from_s3(bucket_name, file_key):\n",
        "    \"\"\"\n",
        "    Load dictionary from S3 pickle file.\n",
        "    \"\"\"\n",
        "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
        "    file_content = response['Body'].read()\n",
        "    data_dict = pickle.loads(file_content)\n",
        "\n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download the data - Google Colab\n",
        "def load_dict_from_drive(file_path):\n",
        "    \"\"\"\n",
        "    Load dictionary from Google Drive pickle file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data_dict = pickle.load(file)\n",
        "\n",
        "    return data_dict"
      ],
      "metadata": {
        "id": "lD3rgDvNqZpG"
      },
      "id": "lD3rgDvNqZpG",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "13aae32a57e0ac87",
      "metadata": {
        "id": "13aae32a57e0ac87"
      },
      "source": [
        "## Define LLM Task to create User Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb598ed-23ea-43f3-86a3-8f588310cd09",
      "metadata": {
        "id": "0eb598ed-23ea-43f3-86a3-8f588310cd09"
      },
      "outputs": [],
      "source": [
        "# AWS -- OUDATED\n",
        "class ChatBot:\n",
        "    def __init__(self, model_name=\"meta-llama-3-1-8b-instruct-005734\"):\n",
        "        # Initialize the SageMaker runtime client\n",
        "        self.sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
        "        self.endpoint_name = \"jumpstart-dft-llama-3-1-8b-instruct-20250225-005734\"\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def get_response(self, message):\n",
        "        try:\n",
        "            payload = json.dumps({\"inputs\": message})\n",
        "            response = self.sagemaker_runtime.invoke_endpoint(\n",
        "                EndpointName=self.endpoint_name,\n",
        "                ContentType='application/json',\n",
        "                Body=payload\n",
        "            )\n",
        "            result = json.loads(response['Body'].read().decode())\n",
        "\n",
        "            # Print raw response for debugging\n",
        "            print(\"\\n===== RAW RESPONSE FROM SAGEMAKER =====\")\n",
        "            print(result)\n",
        "            print(\"========================================\\n\")\n",
        "\n",
        "            # Extract and clean response\n",
        "            if isinstance(result, list) and len(result) > 0:\n",
        "                generated_text = result[0].get('generated_text', '').strip()\n",
        "            else:\n",
        "                generated_text = \"Error: Unexpected response format from SageMaker.\"\n",
        "\n",
        "            return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in get_response: {e}\")\n",
        "            return \"Error: Unable to generate a response.\"\n",
        "\n",
        "    def build_prompt(self, item_list, item_info_dict):\n",
        "        \"\"\"\n",
        "        Build a user profiling prompt based on books read\n",
        "        \"\"\"\n",
        "        content_begin = \"\"\"\n",
        "        Now, you are a book analyst tasked to identify a user's favorite book genres based on their past readings. I will provide you with a list of books that a user has watched in the past.\n",
        "        Each book contains two pieces of information: title and genre. Based on this information, identify in the provided data the user's TOP 3 most-read genres.\n",
        "\n",
        "        Here are some examples of what the output should look like:\n",
        "\n",
        "        ### Example 1:\n",
        "        **INPUT**\n",
        "        User's Reading History:\n",
        "        - Title: The Great Gatsby, Genre: Fiction\n",
        "        - Title: Thinking, Fast and Slow, Genre: Psychology\n",
        "        - Title: 1984, Genre: Dystopian\n",
        "        - Title: Brave New World, Genre: Dystopian\n",
        "        - Title: Fahrenheit 451, Genre: Dystopian\n",
        "        - Title: Blink, Genre: Psychology\n",
        "        - Title: The Catcher in the Rye, Genre: Fiction\n",
        "\n",
        "        **OUTPUT**\n",
        "        User favorite genres: [Dystopian], [Fiction], [Psychology]\n",
        "\n",
        "        ---\n",
        "\n",
        "        ### Here is the list of previously read books:\n",
        "        \"\"\"\n",
        "\n",
        "        content_end = \"\"\"\n",
        "        Please provide the profile strictly in the following format:\n",
        "        User interests: [Interest 1], [Interest 2], [Interest 3]\n",
        "\n",
        "        Emphasize that only the most likely three genres should be provided, and strictly adhere to the above format.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get Data and remove dups\n",
        "        unique_items = set()\n",
        "        filtered_list = []\n",
        "\n",
        "        for item_id in item_list:\n",
        "            title = item_info_dict.get(item_id, {}).get('standard_title', 'Unknown Title')\n",
        "            genre = item_info_dict.get(item_id, {}).get('parsed_categories', 'Unknown Genre')\n",
        "\n",
        "            if isinstance(genre, list):\n",
        "                genre = \", \".join(genre)\n",
        "\n",
        "            item_tuple = (title, genre)\n",
        "            if item_tuple not in unique_items:\n",
        "                unique_items.add(item_tuple)\n",
        "                filtered_list.append(f\"- Title: {title}, Genre: {genre}\")\n",
        "\n",
        "        # Join unique filtered items\n",
        "        content_item_list = \"\\n\".join(filtered_list)\n",
        "\n",
        "        return content_begin + content_item_list + content_end\n",
        "\n",
        "\n",
        "    def chat(self, item_list, item_info_dict, user_Id):\n",
        "        \"\"\"\n",
        "        Generate a user profile based on historical book interactions.\n",
        "        \"\"\"\n",
        "        content = self.build_prompt(item_list, item_info_dict)\n",
        "\n",
        "        # Print the raw prompt before sending it to the LLM\n",
        "        print(\"\\n===== RAW INPUT TO LLM =====\")\n",
        "        print(content)\n",
        "        print(\"============================\\n\")\n",
        "\n",
        "        response = self.get_response(content)\n",
        "\n",
        "        # Print the response received from the model\n",
        "        print(\"\\n===== RAW OUTPUT FROM LLM =====\")\n",
        "        print(response)\n",
        "        print(\"===============================\\n\")\n",
        "\n",
        "        return {user_Id: response}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab\n",
        "class ChatBot:\n",
        "    def __init__(self, model_name=\"meta-llama/Llama-3.1-8B-Instruct\"):\n",
        "        \"\"\"\n",
        "        Initialize the ChatBot with the specified model name.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Optimize model loading with 4-bit quantization\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config\n",
        "        )\n",
        "\n",
        "    def get_response(self, message):\n",
        "        try:\n",
        "          formatted_prompt = f\"[INST] {message} [/INST]\"\n",
        "          inputs = self.tokenizer(\n",
        "              formatted_prompt,\n",
        "              return_tensors=\"pt\",\n",
        "              padding=True,\n",
        "              truncation=True,\n",
        "              max_length=1024\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "          attention_mask = inputs.attention_mask\n",
        "\n",
        "          output = self.model.generate(\n",
        "              inputs.input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              max_new_tokens=256,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              do_sample=True,\n",
        "              pad_token_id=self.tokenizer.eos_token_id,\n",
        "              )\n",
        "\n",
        "          response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "          return response[len(formatted_prompt):].strip()\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Error in get_response: {e}\")\n",
        "          return \"Error: Unable to generate a response.\"\n",
        "\n",
        "    def chat(self, user_item_dict, item_info_dict, user_id, result_dict, user_preferences, recommended_item, user_feedback):\n",
        "        \"\"\"\n",
        "        Explain why recommendations were generated for a user\n",
        "\n",
        "        Args:\n",
        "            user_item_dict (dict): User-item interaction history.\n",
        "            item_info_dict (dict): Book metadata (category, title, etc.).\n",
        "            user_id (str): Target user ID.\n",
        "            recommended_item (str): The recommended book ID.\n",
        "            user_preferences (dict): Dictionary of user preferences {user_id: [categories]}.\n",
        "            user_feedback (dict): Dictionary of user feedback {user_id: {\"liked\": [book_ids], \"disliked\": [book_ids]}}.\n",
        "\n",
        "        Returns:\n",
        "            str: Explanation generated by the LLM\n",
        "        \"\"\"\n",
        "\n",
        "        # System Instructions\n",
        "        content_begin = (\n",
        "            \"[INST] <<SYS>>\\n\"\n",
        "            \"You are a recommendation assistant that explains why books were suggested to a user.\\n\"\n",
        "            \"Your reasoning should be based on:\\n\"\n",
        "            \"- The user's **past interactions** (books they have read before).\\n\"\n",
        "            \"- The user's **onboarding preferences** (favorite genres and categories).\\n\"\n",
        "            \"- The recommended books (collaborative filtering system output).\\n\"\n",
        "            \"- The user's **feedback** (liked/disliked books).\\n\"\n",
        "            \"If a recommended book belongs to a **new category** the user has never interacted with, assume it was suggested \"\n",
        "            \"based on preferences of similar users.\\n\"\n",
        "            \"<</SYS>>\\n\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "        # 1. User's historical interactions\n",
        "        content_user_history = f\"### User {user_id}'s Historical Interactions:\\n\"\n",
        "        user_categories = set()\n",
        "\n",
        "        for item_id in user_item_dict.get(user_id, []):\n",
        "            book_info = item_info_dict.get(item_id, {})\n",
        "            title = book_info.get('standard_title', 'Unknown Title')\n",
        "            categories = book_info.get('parsed_categories', [])\n",
        "\n",
        "            # Ensure is a list\n",
        "            if isinstance(categories, str):\n",
        "                categories = [categories]\n",
        "\n",
        "            user_categories.update(categories)\n",
        "            content_user_history += f\"- **{title}** (Genres: {', '.join(categories)})\\n\"\n",
        "\n",
        "        # 2. User's onboarding preferences\n",
        "        user_pref_categories = set(user_preferences.get(user_id, []))\n",
        "        content_user_prefs = f\"\\n### User {user_id}'s Onboarding Preferences:\\n\"\n",
        "        content_user_prefs += f\"- Preferred Categories: {', '.join(user_pref_categories)}\\n\"\n",
        "\n",
        "        # 3. Recommended book (Known vs. New Category)\n",
        "        book_info = item_info_dict.get(recommended_item, {})\n",
        "        title = book_info.get('standard_title', 'Unknown Title')\n",
        "        categories = book_info.get('parsed_categories', [])\n",
        "\n",
        "        if isinstance(categories, str):\n",
        "            categories = [categories]\n",
        "\n",
        "        # Determine recommendation type\n",
        "        if user_categories.intersection(categories) or user_pref_categories.intersection(categories):\n",
        "            recommendation_reason = \"based on the user's past interactions and interests.\"\n",
        "        else:\n",
        "            recommendation_reason = \"suggested based on similar users' preferences.\"\n",
        "\n",
        "        content_recommendation = (\n",
        "            f\"\\n### Recommended Book: **{title}**\\n\"\n",
        "            f\"- Genre(s): {', '.join(categories)}\\n\"\n",
        "            f\"- Reason: {recommendation_reason}\\n\"\n",
        "        )\n",
        "\n",
        "        # 4. User feedback (liked/disliked books)\n",
        "        content_feedback = f\"\\n### User {user_id}'s Feedback on Past Recommendations:\\n\"\n",
        "        liked_books = [item_info_dict.get(item_id, {}).get('standard_title', 'Unknown Title') for item_id in user_feedback.get(user_id, {}).get(\"liked\", [])]\n",
        "        disliked_books = [item_info_dict.get(item_id, {}).get('standard_title', 'Unknown Title') for item_id in user_feedback.get(user_id, {}).get(\"disliked\", [])]\n",
        "\n",
        "        if liked_books:\n",
        "            content_feedback += f\"- **Liked Books**: {', '.join(liked_books)}\\n\"\n",
        "        if disliked_books:\n",
        "            content_feedback += f\"- **Disliked Books**: {', '.join(disliked_books)}\\n\"\n",
        "        if not liked_books and not disliked_books:\n",
        "            content_feedback += \"- No feedback recorded.\\n\"\n",
        "\n",
        "        # 5. Explanation request\n",
        "        content_request = (\n",
        "            \"\\n### Explanation Task:\\n\"\n",
        "            \"Using the data above, explain why the recommended book was suggested. \"\n",
        "            \"For books that match the user’s past behavior, reinforce the alignment. \"\n",
        "            \"For books from new categories, mention that they were suggested based on similar users' preferences.\\n\\n\"\n",
        "            \"Format your response as follows:\\n\"\n",
        "            \"- **[Book Title]**: Explanation of why it was recommended.\\n\\n\"\n",
        "            \"[/INST]\"\n",
        "        )\n",
        "\n",
        "        # Combine all parts\n",
        "        content = content_begin + content_user_history + content_user_prefs + content_recommendation + content_feedback + content_request\n",
        "\n",
        "        print(\"SEND: \")\n",
        "        print(content)\n",
        "\n",
        "        # Get response\n",
        "        response = self.get_response(content)\n",
        "        result_dict[user_id] = response\n",
        "\n",
        "        print(\"GPT: \")\n",
        "        print(response)\n",
        "        print()"
      ],
      "metadata": {
        "id": "NiT0dRpCwNlL"
      },
      "id": "NiT0dRpCwNlL",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5aea29846f4cfd4e",
      "metadata": {
        "id": "5aea29846f4cfd4e"
      },
      "source": [
        "## Upload the Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32eef730-2533-439f-a3dd-f641f64ea8bb",
      "metadata": {
        "id": "32eef730-2533-439f-a3dd-f641f64ea8bb"
      },
      "outputs": [],
      "source": [
        "# Setup Paths - AWS\n",
        "s3 = boto3.client('s3')\n",
        "bucket_name = 'w210recsys'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User-Item Historical Interactions"
      ],
      "metadata": {
        "id": "6X7fCnk6eBpQ"
      },
      "id": "6X7fCnk6eBpQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6806b93a5f9f7b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:05.603208Z",
          "start_time": "2025-02-15T23:45:05.556138Z"
        },
        "id": "9b6806b93a5f9f7b"
      },
      "outputs": [],
      "source": [
        "# #Import User-Item interactions dictionary - AWS\n",
        "# file_key = 'processed/user_item_dict.pkl'\n",
        "# user_item_dict = load_dict_from_drive(bucket_name, file_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import User-Item interactions dictionary - Colab\n",
        "file_key = '/content/drive/My Drive/Capstone - Spring 2025 Personal/user_profiles/user_item_dict.pkl'\n",
        "user_item_dict = load_dict_from_drive(file_key)"
      ],
      "metadata": {
        "id": "YKyBWqtHrmCt"
      },
      "id": "YKyBWqtHrmCt",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dad33b8321efc159",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:06.641668Z",
          "start_time": "2025-02-15T23:45:06.639637Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dad33b8321efc159",
        "outputId": "a89443c2-9315-451e-80a4-590193abadfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User ID: A01038432MVI9JXYTTK5T, Items: [15693, 17623, 20231, 23671, 25313, 26282, 34776, 34901, 51220, 55669, 56795, 60236]\n",
            "User ID: A100NGGXRQF0AQ, Items: [141, 3330, 5426, 8736, 15511, 21557, 25655, 26605, 38825, 45116, 59372, 60614, 63574, 65047, 70451]\n",
            "User ID: A100V1W0C8BWOL, Items: [21901, 24501, 28429, 29020, 30221, 32803, 36113, 10022, 47988, 58373, 64427]\n",
            "User ID: A100YHBWL4TR4D, Items: [7209, 7396, 11726, 14023, 19355, 21738, 27003, 30061, 33300, 45369, 47419, 45667, 50844, 68750]\n",
            "User ID: A101446I5AWY0Z, Items: [2219, 2269, 3322, 4343, 5987, 8056, 10827, 15499, 16602, 17109, 23670, 31893, 32068, 32259, 32596, 39109, 40873, 41154, 48451, 51637, 55990, 66303, 67211, 68516, 72230]\n"
          ]
        }
      ],
      "source": [
        "for i, (key, value) in enumerate(user_item_dict.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"User ID: {key}, Items: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ccfc498c1450f03c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:08.220596Z",
          "start_time": "2025-02-15T23:45:08.217292Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccfc498c1450f03c",
        "outputId": "ca82f9ec-f2b6-4669-d4eb-2c2965e5bb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A1DI9POOHWJTYA': [2183, 3986, 12346, 32785, 38824, 39615, 46918, 59047, 60219, 63388, 69533]}\n"
          ]
        }
      ],
      "source": [
        "###### FOR TESTING ONLY ######\n",
        "N = 1\n",
        "sampled_keys = random.sample(list(user_item_dict.keys()), N)\n",
        "sampled_dict = {key: user_item_dict[key] for key in sampled_keys}\n",
        "\n",
        "print(sampled_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Books Metadata"
      ],
      "metadata": {
        "id": "Od9iUB0DeINC"
      },
      "id": "Od9iUB0DeINC"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a62bc8ca0dd5d3bf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:09.592864Z",
          "start_time": "2025-02-15T23:45:09.585606Z"
        },
        "id": "a62bc8ca0dd5d3bf"
      },
      "outputs": [],
      "source": [
        "# #Import Items (books) metadata dictionary - AWS\n",
        "# file_key = 'processed/item_info_dict.pkl'\n",
        "# item_info_dict = load_dict_from_s3(bucket_name, file_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Items (books) metadata dictionary - Colab\n",
        "file_key ='/content/drive/My Drive/Capstone - Spring 2025 Personal/user_profiles/item_info_dict.pkl'\n",
        "item_info_dict = load_dict_from_drive(file_key)"
      ],
      "metadata": {
        "id": "AIVVSletr1rP"
      },
      "id": "AIVVSletr1rP",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "118dade067069abd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:10.917824Z",
          "start_time": "2025-02-15T23:45:10.915275Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "118dade067069abd",
        "outputId": "a5d4add5-7507-4f10-88a5-bb0056034115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : {'standard_title': 'dr seuss american icon', 'parsed_categories': ['Biography & Autobiography']}\n",
            "1 : {'standard_title': 'wonderful worship in smaller churches', 'parsed_categories': ['Religion']}\n",
            "2 : {'standard_title': 'rising sons and daughters life among japans new young', 'parsed_categories': ['Social Science']}\n",
            "3 : {'standard_title': 'muslim womens choices religious belief and social reality', 'parsed_categories': ['Religion']}\n",
            "4 : {'standard_title': 'dramatica for screenwriters', 'parsed_categories': ['Reference']}\n"
          ]
        }
      ],
      "source": [
        "for key, value in itertools.islice(item_info_dict.items(), 5):\n",
        "    print(key, \":\", value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User's Onboarding Input"
      ],
      "metadata": {
        "id": "u01lo2OzeN5v"
      },
      "id": "u01lo2OzeN5v"
    },
    {
      "cell_type": "code",
      "source": [
        "file_key = '/content/drive/My Drive/Capstone - Spring 2025 Personal/user_profiles/onboarding_dict.pkl'\n",
        "onboarding_dict = load_dict_from_drive(file_key)"
      ],
      "metadata": {
        "id": "HIYlU3KEeRRt"
      },
      "id": "HIYlU3KEeRRt",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in itertools.islice(onboarding_dict.items(), 5):\n",
        "    print(key, \":\", value)"
      ],
      "metadata": {
        "id": "9DPxI8YgeRbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cab12c8-504b-41a8-840a-1df4759b6b0e"
      },
      "id": "9DPxI8YgeRbD",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A01038432MVI9JXYTTK5T : ['American fiction', 'POETRY', 'Materia medica', 'Medieval']\n",
            "A100NGGXRQF0AQ : ['Fruit', 'French drama']\n",
            "A100V1W0C8BWOL : ['Athens (Greece)', 'Imagination', 'ices', 'Cross-country (Horsemanship)', 'Coming of age']\n",
            "A100YHBWL4TR4D : ['Israel', 'Nathaniel']\n",
            "A101446I5AWY0Z : ['Kuṇḍalinī']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User's Feedback Data"
      ],
      "metadata": {
        "id": "LcPVeE0MeRyX"
      },
      "id": "LcPVeE0MeRyX"
    },
    {
      "cell_type": "code",
      "source": [
        "file_key = '/content/drive/My Drive/Capstone - Spring 2025 Personal/user_profiles/feedback_dict.pkl'\n",
        "feedback_dict = load_dict_from_drive(file_key)"
      ],
      "metadata": {
        "id": "vxTlRqaaeVsE"
      },
      "id": "vxTlRqaaeVsE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in itertools.islice(feedback_dict.items(), 5):\n",
        "    print(key, \":\", value)"
      ],
      "metadata": {
        "id": "Hkc-5R9OeVvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85150c7b-842f-4a84-f873-e7f57f1583bd"
      },
      "id": "Hkc-5R9OeVvB",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A01038432MVI9JXYTTK5T : {'liked': [], 'disliked': [25313]}\n",
            "A100NGGXRQF0AQ : {'liked': [3330], 'disliked': [141]}\n",
            "A100V1W0C8BWOL : {'liked': [], 'disliked': [30221, 10022, 58373, 64427]}\n",
            "A100YHBWL4TR4D : {'liked': [7209, 11726, 47419], 'disliked': [27003]}\n",
            "A101446I5AWY0Z : {'liked': [], 'disliked': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommendations"
      ],
      "metadata": {
        "id": "LMctXsQkeV96"
      },
      "id": "LMctXsQkeV96"
    },
    {
      "cell_type": "code",
      "source": [
        "file_key = '/content/drive/My Drive/Capstone - Spring 2025 Personal/user_profiles/cf_dict.pkl'\n",
        "cf_dict = load_dict_from_drive(file_key)"
      ],
      "metadata": {
        "id": "2aDpHXLAeYkB"
      },
      "id": "2aDpHXLAeYkB",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in itertools.islice(cf_dict.items(), 5):\n",
        "    print(key, \":\", value)"
      ],
      "metadata": {
        "id": "BLo5283ceYmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "119940a4-2f14-478b-e5e2-0e0374011309"
      },
      "id": "BLo5283ceYmw",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A01038432MVI9JXYTTK5T : [70013, 65089, 48129, 48178, 33184, 56203]\n",
            "A100NGGXRQF0AQ : [39119, 38995, 38952, 68863, 2071, 40865]\n",
            "A100V1W0C8BWOL : [54344, 18725, 33473, 49862]\n",
            "A100YHBWL4TR4D : [57027, 17324, 67786]\n",
            "A101446I5AWY0Z : [9830, 69737, 24746, 5877]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d112b8cea0b1689f",
      "metadata": {
        "id": "d112b8cea0b1689f"
      },
      "source": [
        "### Load the LLM model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a model\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
      ],
      "metadata": {
        "id": "pHXeOxapz6HM"
      },
      "id": "pHXeOxapz6HM",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d8513fb5-ad23-4315-bc09-eda755d37f2c",
      "metadata": {
        "id": "d8513fb5-ad23-4315-bc09-eda755d37f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "127fe6abddad47e7bea0a5d380e92e9a",
            "b748baa539eb4391a07ba736c56a86cf",
            "cc2036db10314d97abce42861e8e0626",
            "64f7ab501c9b486fb91c16d3b5b80773",
            "a8ff348ff2804c1a95cb0c0b886abef3",
            "033b3c54b74e4ecab9ed4f44bcc76493",
            "c6ea987cd7fd4148bd73c19879c3684e",
            "7658f91b6c5441d4b3c4b3ddfc0f96a7",
            "35a8efbf591d4b1093c0c6e14554d06d",
            "8df40e1337a74b1b8b241c7ded9c44af",
            "31e11858aa0d4e2e99d7e4b18474919b"
          ]
        },
        "outputId": "9fad0493-baa5-419c-a37d-21ac3d7a1887"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "127fe6abddad47e7bea0a5d380e92e9a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create an instance of the ChatBot class\n",
        "chatbot = ChatBot(model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "249eb0dd5f077ce",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T22:25:37.410142Z",
          "start_time": "2025-02-15T22:25:37.403098Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "249eb0dd5f077ce",
        "outputId": "f73bad86-cd58-4fd0-e5ac-09b503d3a424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique users: 14402\n",
            "Number of unique books: 72308\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of unique users: {len(user_item_dict)}\")\n",
        "print(f\"Number of unique books: {len(item_info_dict)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e73a2d7fe947a097",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-15T23:45:12.908739Z",
          "start_time": "2025-02-15T23:45:12.396723Z"
        },
        "id": "e73a2d7fe947a097"
      },
      "outputs": [],
      "source": [
        "# Create dictionary to store results\n",
        "result_dict = {}\n",
        "counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "10be95f584332d8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "10be95f584332d8b",
        "outputId": "4c9bae99-108c-43d4-faad-fd7bade1a541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEND: \n",
            "[INST] <<SYS>>\n",
            "You are a recommendation assistant that explains why books were suggested to a user.\n",
            "Your reasoning should be based on:\n",
            "- The user's **past interactions** (books they have read before).\n",
            "- The user's **onboarding preferences** (favorite genres and categories).\n",
            "- The recommended books (collaborative filtering system output).\n",
            "- The user's **feedback** (liked/disliked books).\n",
            "If a recommended book belongs to a **new category** the user has never interacted with, assume it was suggested based on preferences of similar users.\n",
            "<</SYS>>\n",
            "\n",
            "### User A1DI9POOHWJTYA's Historical Interactions:\n",
            "- **eragon inheritance, book one** (Genres: Dragons)\n",
            "- **chinese** (Genres: Family & Relationships)\n",
            "- **the lord of the rings - boxed set** (Genres: Young Adult Fiction)\n",
            "- **the lord of the rings** (Genres: Baggins, Bilbo (Fictitious character))\n",
            "- **the lord of the rings trilogy** (Genres: Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **dreams of terror and death the dream cycle of h p lovecraft** (Genres: Fiction)\n",
            "- **eldest** (Genres: Juvenile Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **the lord of the rings trilogy 3 volumes** (Genres: Baggins, Frodo (Fictitious character))\n",
            "- **the lord of the rings three volume boxed set** (Genres: Fantasy fiction)\n",
            "\n",
            "### User A1DI9POOHWJTYA's Onboarding Preferences:\n",
            "- Preferred Categories: Anthroposophy\n",
            "\n",
            "### Recommended Book: **schaums outline of software engineering**\n",
            "- Genre(s): Computers\n",
            "- Reason: suggested based on similar users' preferences.\n",
            "\n",
            "### User A1DI9POOHWJTYA's Feedback on Past Recommendations:\n",
            "- No feedback recorded.\n",
            "\n",
            "### Explanation Task:\n",
            "Using the data above, explain why the recommended book was suggested. For books that match the user’s past behavior, reinforce the alignment. For books from new categories, mention that they were suggested based on similar users' preferences.\n",
            "\n",
            "Format your response as follows:\n",
            "- **[Book Title]**: Explanation of why it was recommended.\n",
            "\n",
            "[/INST]\n",
            "GPT: \n",
            "Great, let's dive into why the recommended book \"Schaum's Outline of Software Engineering\" was suggested to User A1DI9POOHWJTYA.\n",
            "\n",
            "**Schaum's Outline of Software Engineering**: This book was recommended based on the user's past interactions and onboarding preferences. Since User A1DI9POOHWJTYA has shown a preference for Anthroposophy, a collaborative filtering system was used to suggest books that are similar to what they have read before. However, since \"Schaum's Outline of Software Engineering\" does not belong to any of the genres or categories that User A1DI9POOHWJTYA has interacted with before, it was suggested based on the preferences of similar users.\n",
            "\n",
            "It's possible that User A1DI9POOHWJTYA may not have interacted with many books in the field of software engineering, but they have shown an interest in Anthroposophy, which suggests that they may be open to exploring new topics and genres. By suggesting \"Schaum's Outline of Software Engineering\", the system aims to expand User A1DI9\n",
            "\n",
            "User A1DI9POOHWJTYA, Book 28864 Response: None\n",
            "1\n",
            "SEND: \n",
            "[INST] <<SYS>>\n",
            "You are a recommendation assistant that explains why books were suggested to a user.\n",
            "Your reasoning should be based on:\n",
            "- The user's **past interactions** (books they have read before).\n",
            "- The user's **onboarding preferences** (favorite genres and categories).\n",
            "- The recommended books (collaborative filtering system output).\n",
            "- The user's **feedback** (liked/disliked books).\n",
            "If a recommended book belongs to a **new category** the user has never interacted with, assume it was suggested based on preferences of similar users.\n",
            "<</SYS>>\n",
            "\n",
            "### User A1DI9POOHWJTYA's Historical Interactions:\n",
            "- **eragon inheritance, book one** (Genres: Dragons)\n",
            "- **chinese** (Genres: Family & Relationships)\n",
            "- **the lord of the rings - boxed set** (Genres: Young Adult Fiction)\n",
            "- **the lord of the rings** (Genres: Baggins, Bilbo (Fictitious character))\n",
            "- **the lord of the rings trilogy** (Genres: Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **dreams of terror and death the dream cycle of h p lovecraft** (Genres: Fiction)\n",
            "- **eldest** (Genres: Juvenile Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **the lord of the rings trilogy 3 volumes** (Genres: Baggins, Frodo (Fictitious character))\n",
            "- **the lord of the rings three volume boxed set** (Genres: Fantasy fiction)\n",
            "\n",
            "### User A1DI9POOHWJTYA's Onboarding Preferences:\n",
            "- Preferred Categories: Anthroposophy\n",
            "\n",
            "### Recommended Book: **eternity - sequel to eon**\n",
            "- Genre(s): Fiction\n",
            "- Reason: based on the user's past interactions and interests.\n",
            "\n",
            "### User A1DI9POOHWJTYA's Feedback on Past Recommendations:\n",
            "- No feedback recorded.\n",
            "\n",
            "### Explanation Task:\n",
            "Using the data above, explain why the recommended book was suggested. For books that match the user’s past behavior, reinforce the alignment. For books from new categories, mention that they were suggested based on similar users' preferences.\n",
            "\n",
            "Format your response as follows:\n",
            "- **[Book Title]**: Explanation of why it was recommended.\n",
            "\n",
            "[/INST]\n",
            "GPT: \n",
            "Sure, I'd be happy to explain why the recommended book \"eternity - sequel to eon\" was suggested to User A1DI9POOHWJTYA.\n",
            "\n",
            "**Eternity - Sequel to Eon**: This book was recommended to User A1DI9POOHWJTYA based on their past interactions and interests. The book falls under the genre of Fiction, which is the same genre as the majority of the user's past interactions. For example, the user has interacted with books in the genres of Dragons (with \"eragon inheritance, book one\"), Family & Relationships (with \"chinese\"), and Young Adult Fiction (with \"the lord of the rings - boxed set\" and \"the lord of the rings\"). These interactions suggest that the user enjoys reading fiction books, which is why the recommended book \"eternity - sequel to eon\" was suggested.\n",
            "\n",
            "Additionally, the user's onboarding preferences, which include the category Anthroposophy, suggest that they may be interested in books that explore philosophical or spiritual themes. While \"eternity - sequel to eon\" does\n",
            "\n",
            "User A1DI9POOHWJTYA, Book 39869 Response: None\n",
            "2\n",
            "SEND: \n",
            "[INST] <<SYS>>\n",
            "You are a recommendation assistant that explains why books were suggested to a user.\n",
            "Your reasoning should be based on:\n",
            "- The user's **past interactions** (books they have read before).\n",
            "- The user's **onboarding preferences** (favorite genres and categories).\n",
            "- The recommended books (collaborative filtering system output).\n",
            "- The user's **feedback** (liked/disliked books).\n",
            "If a recommended book belongs to a **new category** the user has never interacted with, assume it was suggested based on preferences of similar users.\n",
            "<</SYS>>\n",
            "\n",
            "### User A1DI9POOHWJTYA's Historical Interactions:\n",
            "- **eragon inheritance, book one** (Genres: Dragons)\n",
            "- **chinese** (Genres: Family & Relationships)\n",
            "- **the lord of the rings - boxed set** (Genres: Young Adult Fiction)\n",
            "- **the lord of the rings** (Genres: Baggins, Bilbo (Fictitious character))\n",
            "- **the lord of the rings trilogy** (Genres: Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **dreams of terror and death the dream cycle of h p lovecraft** (Genres: Fiction)\n",
            "- **eldest** (Genres: Juvenile Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **the lord of the rings trilogy 3 volumes** (Genres: Baggins, Frodo (Fictitious character))\n",
            "- **the lord of the rings three volume boxed set** (Genres: Fantasy fiction)\n",
            "\n",
            "### User A1DI9POOHWJTYA's Onboarding Preferences:\n",
            "- Preferred Categories: Anthroposophy\n",
            "\n",
            "### Recommended Book: **god knows life gets hard 10 tips for tough times**\n",
            "- Genre(s): Self-Help\n",
            "- Reason: suggested based on similar users' preferences.\n",
            "\n",
            "### User A1DI9POOHWJTYA's Feedback on Past Recommendations:\n",
            "- No feedback recorded.\n",
            "\n",
            "### Explanation Task:\n",
            "Using the data above, explain why the recommended book was suggested. For books that match the user’s past behavior, reinforce the alignment. For books from new categories, mention that they were suggested based on similar users' preferences.\n",
            "\n",
            "Format your response as follows:\n",
            "- **[Book Title]**: Explanation of why it was recommended.\n",
            "\n",
            "[/INST]\n",
            "GPT: \n",
            "Great, let's dive into the explanation for the recommended book, \"God Knows Life Gets Hard: 10 Tips for Tough Times\"!\n",
            "\n",
            "**[God Knows Life Gets Hard]**: The reason why this book was suggested to you is because of your past interactions with books in the Self-Help genre. You have shown a preference for books that offer practical advice and tips for navigating difficult situations, which is exactly what \"God Knows Life Gets Hard\" promises. The book's focus on providing actionable strategies for dealing with tough times aligns well with your onboarding preferences, which include Anthroposophy.\n",
            "\n",
            "While you have not provided any feedback on past recommendations, the system has taken into account your past interactions and onboarding preferences to suggest this book. Similar users have also shown an interest in Self-Help books, which is why the system has suggested \"God Knows Life Gets Hard\" based on preferences of similar users.\n",
            "\n",
            "Overall, the system has suggested this book because it believes it will resonate with your interests and preferences, and provide you with valuable insights and strategies\n",
            "\n",
            "User A1DI9POOHWJTYA, Book 16222 Response: None\n",
            "3\n",
            "SEND: \n",
            "[INST] <<SYS>>\n",
            "You are a recommendation assistant that explains why books were suggested to a user.\n",
            "Your reasoning should be based on:\n",
            "- The user's **past interactions** (books they have read before).\n",
            "- The user's **onboarding preferences** (favorite genres and categories).\n",
            "- The recommended books (collaborative filtering system output).\n",
            "- The user's **feedback** (liked/disliked books).\n",
            "If a recommended book belongs to a **new category** the user has never interacted with, assume it was suggested based on preferences of similar users.\n",
            "<</SYS>>\n",
            "\n",
            "### User A1DI9POOHWJTYA's Historical Interactions:\n",
            "- **eragon inheritance, book one** (Genres: Dragons)\n",
            "- **chinese** (Genres: Family & Relationships)\n",
            "- **the lord of the rings - boxed set** (Genres: Young Adult Fiction)\n",
            "- **the lord of the rings** (Genres: Baggins, Bilbo (Fictitious character))\n",
            "- **the lord of the rings trilogy** (Genres: Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **dreams of terror and death the dream cycle of h p lovecraft** (Genres: Fiction)\n",
            "- **eldest** (Genres: Juvenile Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **the lord of the rings trilogy 3 volumes** (Genres: Baggins, Frodo (Fictitious character))\n",
            "- **the lord of the rings three volume boxed set** (Genres: Fantasy fiction)\n",
            "\n",
            "### User A1DI9POOHWJTYA's Onboarding Preferences:\n",
            "- Preferred Categories: Anthroposophy\n",
            "\n",
            "### Recommended Book: **stories from where we live -- the great lakes**\n",
            "- Genre(s): Literary Collections\n",
            "- Reason: suggested based on similar users' preferences.\n",
            "\n",
            "### User A1DI9POOHWJTYA's Feedback on Past Recommendations:\n",
            "- No feedback recorded.\n",
            "\n",
            "### Explanation Task:\n",
            "Using the data above, explain why the recommended book was suggested. For books that match the user’s past behavior, reinforce the alignment. For books from new categories, mention that they were suggested based on similar users' preferences.\n",
            "\n",
            "Format your response as follows:\n",
            "- **[Book Title]**: Explanation of why it was recommended.\n",
            "\n",
            "[/INST]\n",
            "GPT: \n",
            "Great! Based on the information provided, here's why the recommended book \"Stories from Where We Live -- The Great Lakes\" was suggested to User A1DI9POOHWJTYA:\n",
            "\n",
            "**Stories from Where We Live -- The Great Lakes**: This book was recommended to User A1DI9POOHWJTYA based on the similar preferences of other users who have interacted with similar genres and categories. While User A1DI9POOHWJTYA has primarily interacted with dragons, family & relationships, and young adult fiction, the recommended book is from the literary collections genre, which is a new category for them. However, since User A1DI9POOHWJTYA has shown a preference for anthroposophy, the system has suggested a book that aligns with their interests in this area.\n",
            "\n",
            "The reason for this suggestion is that the book \"Stories from Where We Live -- The Great Lakes\" is a collection of stories that explore the human experience, themes of identity, and the relationship between people and their environments. These themes align with User A1DI9POOHWJTYA's onboarding preferences\n",
            "\n",
            "User A1DI9POOHWJTYA, Book 5855 Response: None\n",
            "4\n",
            "SEND: \n",
            "[INST] <<SYS>>\n",
            "You are a recommendation assistant that explains why books were suggested to a user.\n",
            "Your reasoning should be based on:\n",
            "- The user's **past interactions** (books they have read before).\n",
            "- The user's **onboarding preferences** (favorite genres and categories).\n",
            "- The recommended books (collaborative filtering system output).\n",
            "- The user's **feedback** (liked/disliked books).\n",
            "If a recommended book belongs to a **new category** the user has never interacted with, assume it was suggested based on preferences of similar users.\n",
            "<</SYS>>\n",
            "\n",
            "### User A1DI9POOHWJTYA's Historical Interactions:\n",
            "- **eragon inheritance, book one** (Genres: Dragons)\n",
            "- **chinese** (Genres: Family & Relationships)\n",
            "- **the lord of the rings - boxed set** (Genres: Young Adult Fiction)\n",
            "- **the lord of the rings** (Genres: Baggins, Bilbo (Fictitious character))\n",
            "- **the lord of the rings trilogy** (Genres: Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **dreams of terror and death the dream cycle of h p lovecraft** (Genres: Fiction)\n",
            "- **eldest** (Genres: Juvenile Fiction)\n",
            "- **the zero stone** (Genres: Fiction)\n",
            "- **the lord of the rings trilogy 3 volumes** (Genres: Baggins, Frodo (Fictitious character))\n",
            "- **the lord of the rings three volume boxed set** (Genres: Fantasy fiction)\n",
            "\n",
            "### User A1DI9POOHWJTYA's Onboarding Preferences:\n",
            "- Preferred Categories: Anthroposophy\n",
            "\n",
            "### Recommended Book: **saracen island the poems of andreas karavis**\n",
            "- Genre(s): Poetry\n",
            "- Reason: suggested based on similar users' preferences.\n",
            "\n",
            "### User A1DI9POOHWJTYA's Feedback on Past Recommendations:\n",
            "- No feedback recorded.\n",
            "\n",
            "### Explanation Task:\n",
            "Using the data above, explain why the recommended book was suggested. For books that match the user’s past behavior, reinforce the alignment. For books from new categories, mention that they were suggested based on similar users' preferences.\n",
            "\n",
            "Format your response as follows:\n",
            "- **[Book Title]**: Explanation of why it was recommended.\n",
            "\n",
            "[/INST]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c407227237ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbook_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecommended_books\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         response = chatbot.chat(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0muser_item_dict\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# User-item interactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mitem_info_dict\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# Book metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-eed6c48083fc>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, user_item_dict, item_info_dict, user_id, result_dict, user_preferences, recommended_item, user_feedback)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Get response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mresult_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-eed6c48083fc>\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           output = self.model.generate(\n\u001b[0m\u001b[1;32m     37\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m               \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m                 )\n\u001b[1;32m    591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    593\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# Fully Connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Loop through sampled users and get explanations\n",
        "for user_id in sampled_dict:\n",
        "    recommended_books = cf_dict.get(user_id, [])  # Get recommendations for user\n",
        "\n",
        "    result_dict[user_id] = []\n",
        "\n",
        "    for book_id in recommended_books:\n",
        "        # Get response\n",
        "        response = chatbot.chat(\n",
        "            user_item_dict,        # User-item interactions\n",
        "            item_info_dict,        # Book metadata\n",
        "            user_id,               # Current user\n",
        "            {},                    # Empty result_dict\n",
        "            onboarding_dict,       # User preferences\n",
        "            book_id,               # Recommended book ID\n",
        "            feedback_dict          # User feedback\n",
        "        )\n",
        "\n",
        "        # Store responses\n",
        "        result_dict[user_id].append({book_id: response})\n",
        "\n",
        "        # Print response\n",
        "        print(f\"User {user_id}, Book {book_id} Response: {response}\")\n",
        "\n",
        "        # increment and print counter\n",
        "        counter += 1\n",
        "        print(counter)\n",
        "\n",
        "# Final counter\n",
        "print(\"Total Books Processed:\", counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b0346cd6-4cf8-4695-bec6-91ea8b75c93e",
      "metadata": {
        "id": "b0346cd6-4cf8-4695-bec6-91ea8b75c93e",
        "outputId": "7ff2890d-ea65-48c8-af6d-3b765000d449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A3T46YOGTFC5IG': None}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b2e09d1588f93d",
      "metadata": {
        "id": "c0b2e09d1588f93d"
      },
      "source": [
        "### NEXT STEPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfadfa65-54bf-4f6f-a1fb-33e0a27e2cfe",
      "metadata": {
        "id": "dfadfa65-54bf-4f6f-a1fb-33e0a27e2cfe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "127fe6abddad47e7bea0a5d380e92e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b748baa539eb4391a07ba736c56a86cf",
              "IPY_MODEL_cc2036db10314d97abce42861e8e0626",
              "IPY_MODEL_64f7ab501c9b486fb91c16d3b5b80773"
            ],
            "layout": "IPY_MODEL_a8ff348ff2804c1a95cb0c0b886abef3"
          }
        },
        "b748baa539eb4391a07ba736c56a86cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033b3c54b74e4ecab9ed4f44bcc76493",
            "placeholder": "​",
            "style": "IPY_MODEL_c6ea987cd7fd4148bd73c19879c3684e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cc2036db10314d97abce42861e8e0626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7658f91b6c5441d4b3c4b3ddfc0f96a7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35a8efbf591d4b1093c0c6e14554d06d",
            "value": 2
          }
        },
        "64f7ab501c9b486fb91c16d3b5b80773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df40e1337a74b1b8b241c7ded9c44af",
            "placeholder": "​",
            "style": "IPY_MODEL_31e11858aa0d4e2e99d7e4b18474919b",
            "value": " 2/2 [01:02&lt;00:00, 28.85s/it]"
          }
        },
        "a8ff348ff2804c1a95cb0c0b886abef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033b3c54b74e4ecab9ed4f44bcc76493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ea987cd7fd4148bd73c19879c3684e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7658f91b6c5441d4b3c4b3ddfc0f96a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a8efbf591d4b1093c0c6e14554d06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8df40e1337a74b1b8b241c7ded9c44af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e11858aa0d4e2e99d7e4b18474919b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}