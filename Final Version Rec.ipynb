{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f26477-93f0-4c57-bf39-d9b27bf9d7c5",
   "metadata": {},
   "source": [
    "In this notebook I will build off of my previous work with the TFRS pipline to simplify the model's towers and to also improve the users' embeddings by incorperating additional session metrics to pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62768ec6-1f07-4b54-a478-df4351130da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e135a17f-5a1a-44da-b4b3-9261d17518a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f492ade-b829-483a-a600-642d49c895f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 00:16:58.631970: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-14 00:16:58.656358: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "\n",
    "import io\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "from typing import List, Union, Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import plotnine\n",
    "import gdown\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decfd9ad-83b3-44a6-817f-40a7f53ae3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: []\n",
      "Logical devices configured.\n"
     ]
    }
   ],
   "source": [
    "# Ensure GPUs are visible\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "if gpus:\n",
    "    # Set memory growth to avoid allocation errors\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Set logical device configuration for CPU\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "if cpus:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        cpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration()]\n",
    "    )\n",
    "\n",
    "print(\"Logical devices configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db5b8b-b45a-4d2c-94f7-6692aa2f588f",
   "metadata": {},
   "source": [
    "Initialize access to s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b922b-20d7-4dd7-a405-7e5dbb3ef8bf",
   "metadata": {},
   "source": [
    "To start off we import the datasets for books and their reviews, performing some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6800c51-b215-4095-b606-c5113bd45999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import books and reviews dataset\n",
    "# books_data_location = 's3://w210recsys/book_raw/books_data.csv'\n",
    "#review_location = 's3://w210recsys/book_raw/Books_rating.csv'\n",
    "\n",
    "books_data_location = \"s3://w210recsys/book_clean/books_data_clean.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6121462-e376-432a-aca5-776974fbf3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321301, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df = pd.read_pickle(books_data_location) \n",
    "\n",
    "books_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e24fd-359c-434f-b294-d7dae1b470b3",
   "metadata": {},
   "source": [
    "Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36377777-5dfb-48c3-9218-4c06e951d753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                    0\n",
       "book_id                    0\n",
       "title                      0\n",
       "author                     0\n",
       "publish_year             842\n",
       "description                0\n",
       "preview_link               0\n",
       "normalized_popularity      0\n",
       "genre_general              0\n",
       "genre_specific             0\n",
       "genre_combined             0\n",
       "genre_consolidated         0\n",
       "review_helpfulness         0\n",
       "review_score               0\n",
       "review_time                0\n",
       "review_summary             1\n",
       "review_text                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3747-58c2-45b5-b692-8b40324e7e6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Augment Dataset (Do Not Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f6a4f-1b87-4129-b584-5cde78207737",
   "metadata": {},
   "source": [
    "As was demonstrated by Ben's EDA on user rating, we see an overwheling amount of ratings are positively skewwed and similarly that many people only have a few review they ever leave. The twin tower model will perform better if it gets example of both what the user likes and what they don't like so we want to augment user's reviews with books they did not interact with compared to the ones they did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f6bfb-293d-4a18-9563-d291a0f55268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aug_data(catalog, user_data_dict, user_last_dates, k_per_user):\n",
    "#     \"\"\"\n",
    "#     Returns a new book sample set from the catalog for multiple users at once, ensuring dates are not earlier than \n",
    "#     the last review date per user.\n",
    "\n",
    "#     Parameters:\n",
    "#     - catalog (pd.DataFrame): Full book catalog.\n",
    "#     - user_data_dict (dict): A dictionary mapping users to a set of interacted books.\n",
    "#     - user_last_dates (dict): A dictionary mapping users to their last review date.\n",
    "#     - k_per_user (int): Number of new books per user.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: A DataFrame with new book samples for all users.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Flatten all interacted books into a set (fast filtering)\n",
    "#     interacted_books = set.union(*user_data_dict.values())\n",
    "\n",
    "#     # Efficient filtering: Remove all interacted books at once\n",
    "#     filtered_catalog = catalog[~catalog[['title', 'author']].apply(tuple, axis=1).isin(interacted_books)]\n",
    "\n",
    "#     # Prepare a list to store new samples\n",
    "#     new_samples = []\n",
    "\n",
    "#     for user, _ in user_data_dict.items():\n",
    "#         # Randomly sample `k_per_user` books\n",
    "#         sampled_books = filtered_catalog.sample(n=min(k_per_user, len(filtered_catalog)), random_state=42).copy()\n",
    "#         sampled_books['user_id'] = user  # Assign user ID\n",
    "#         sampled_books['review_score'] = 0  # The user didn't interact with it\n",
    "\n",
    "#         # Get the last review date for this user\n",
    "#         last_review_date = user_last_dates.get(user, pd.Timestamp.now())  # Default to now if no history\n",
    "        \n",
    "#         # Ensure last_review_date is a pandas Timestamp (datetime64)\n",
    "#         last_review_date = pd.Timestamp(last_review_date)\n",
    "        \n",
    "#         # Generate random timedelta and subtract from last_review_date\n",
    "#         sampled_books['review_time'] = last_review_date - pd.to_timedelta(\n",
    "#             [random.randint(1, 30) for _ in range(len(sampled_books))], unit=\"D\"\n",
    "#         )\n",
    "\n",
    "#         new_samples.append(sampled_books)\n",
    "\n",
    "#     # Concatenate all samples into a single DataFrame\n",
    "#     return pd.concat(new_samples, ignore_index=True)\n",
    "\n",
    "# # Sample Data\n",
    "# temp = books_df#.iloc[0:100]\n",
    "\n",
    "# # Step 1: Convert user interactions into a dictionary {user_id: {(title, author), ...}}\n",
    "# user_data_dict = (\n",
    "#     temp.groupby('user_id')[['title', 'author']]\n",
    "#     .apply(lambda df: set(df.itertuples(index=False, name=None)))\n",
    "#     .to_dict()\n",
    "# )\n",
    "\n",
    "# # Step 2: Extract last review date per user\n",
    "# user_last_dates = (\n",
    "#     temp.groupby('user_id')['review_time']\n",
    "#     .max()\n",
    "#     .to_dict()\n",
    "# )\n",
    "\n",
    "# # Step 3: Call optimized function for batch augmentation\n",
    "# augmented_data = aug_data(books_df, user_data_dict, user_last_dates, k_per_user=3)\n",
    "\n",
    "# # Step 4: Append new recommendations to merged_df efficiently\n",
    "# temp = pd.concat([temp, augmented_data], ignore_index=True)\n",
    "\n",
    "# print(f\"Augmented dataset size: {temp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09785183-b12e-410e-af01-16ce3e71edd3",
   "metadata": {},
   "source": [
    "Save the data to pickle file in the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bdbb4-68db-4db3-9de2-32a2060baf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.to_pickle(\"s3://w210recsys/aug_data/clean_augmented_data_v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a3b66-9427-4a82-aa5c-fb5968e017eb",
   "metadata": {},
   "source": [
    "Load the data to avoid overheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6964a-653a-442a-847f-d775e6738eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.read_pickle(\"s3://w210recsys/aug_data/clean_augmented_data_v1.pkl\")\n",
    "\n",
    "# merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453849a-6394-4746-83ef-8ea35f2a076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df[merged_df['user_id'] == 'A1SMFD252FTJP9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6685a9b-198f-4fce-9e6b-117a414bc16c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Session-ize Data + Split Dataset for Validation based on Users\n",
    "In the earlier versions of our twin-tower recommendations model we were splitting the data based on dates to isolate the last interaction as our validation data and passing in other interactions, line by line, in as training data. However, upon review we realized that this approach has some flaws:\n",
    "\n",
    "1. Data Leakage - By passing in the same users in training and testing we may be getting an inflated sense of how good the model is doing.\n",
    "2. Line by line data - The goal of our recommendation system is to take in a user's metrics at once and provide a user embedding that will likely set them closer tot he vooks they like in the embedding space. However, by passing in data line by line, we don't aggregate this data in the same way and the model may not be learning that\n",
    "\n",
    "For these reasons, we opt to split the data by users, holding out their last interaction as the label, rather than by date.\n",
    "\n",
    "Session-izing Data:\n",
    "\n",
    "We want to group the historical data for each user in a way that allows us to mimic the session data we will collect from users in deployment. The basic structure of which will be to summarize past interactions and hold out a separate book interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe4205-6e65-439c-8a5a-19492bf98ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test on a smaller sub-set of the data\n",
    "\n",
    "# merged_df = merged_df.sort_values(by=['user_id', 'review_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4644af-a0cd-4793-a92a-2fe414d04641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833f86c-596f-4686-83a9-328ee63c5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = merged_df[merged_df['user_id'] == 'A1SMFD252FTJP9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd33170-515b-4c3e-af7b-3fa5ea63a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def session_summary(user_data):\n",
    "\n",
    "#     \"\"\"\n",
    "#     session_summary takes in each user's session data and returns a summarized verison of it\n",
    "\n",
    "#     inputs:\n",
    "#     user_data: user's interaction data\n",
    "#     interest_cols: columns of interest wanting to be summarized\n",
    "\n",
    "#     outputs:\n",
    "#     summarized_data: the user's summarized data\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     user_data = user_data.sort_values(by=['user_id', 'review_time'])  # Sort by user & time\n",
    "    \n",
    "#     session_data = []\n",
    "    \n",
    "#     for user, user_df in user_data.groupby('user_id'):\n",
    "#         if len(user_df) < 2:\n",
    "#             continue  # Skip users with only one interaction\n",
    "        \n",
    "#         # Last interaction is the target (book user last interacted with)\n",
    "#         target_row = user_df.iloc[-1]\n",
    "#         target_book = target_row['title']\n",
    "#         target_book_rating = target_row['review_score']\n",
    "        \n",
    "#         # Previous interactions (session history3\n",
    "#         history_df = user_df.iloc[:-1]  # Exclude last row]\n",
    "\n",
    "#         summary = {\n",
    "#             'user_id': user,\n",
    "#             'liked_books': list(history_df.loc[history_df['review_score'] >= 3, 'title']),\n",
    "#             'disliked_books': list(history_df.loc[history_df['review_score'] < 3, 'title']),\n",
    "#             'liked_genres': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] >= 3, 'genre_consolidated'])))),\n",
    "#             'disliked_genres': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] < 3, 'genre_consolidated'])))),\n",
    "#             'liked_authors': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] >= 3, 'author'])))),\n",
    "#             'disliked_authors': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] < 3, 'author'])))),\n",
    "#             'liked_ratings': list(history_df.loc[history_df['review_score'] >= 3, 'review_score']),\n",
    "#             'disliked_ratings': list(history_df.loc[history_df['review_score'] < 3, 'review_score']),\n",
    "#             'target_book': target_book,\n",
    "#             'target_book_rating': target_book_rating\n",
    "#         }\n",
    "        \n",
    "#         session_data.append(summary)\n",
    "    \n",
    "#     return pd.DataFrame(session_data)\n",
    "\n",
    "# # Generate session summaries with held-out target sample\n",
    "# # session_summary(temp)\n",
    "\n",
    "# # Generate a full df of session summaries\n",
    "# # sessionized_df = session_summary(merged_df)\n",
    "# sessionized_df = session_summary(sampled_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3c66d-398a-4c6c-8f11-aef77b34e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessionized_df.to_pickle(\"s3://w210recsys/aug_data/cleaned_sessionized_data_v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cdd4d-af93-4912-99c2-9ce4423e4f1e",
   "metadata": {},
   "source": [
    "To run this function in a timely manner I had to leverage a much larger compute instance than the one this notebook was created on. I saved the result to the team s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120e4f4-d899-48c0-a3ee-ed1b790c0dfc",
   "metadata": {},
   "source": [
    "## Load Sessionized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c11792-fed9-4bef-bbb6-691a195b1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionized_df = pd.read_pickle(\"s3://w210recsys/aug_data/cleaned_sessionized_data_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3155b19-89f4-47ec-af3f-89f955e75d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15839, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessionized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58c43ec-68f3-4835-9e94-c303b0e616ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>liked_books</th>\n",
       "      <th>disliked_books</th>\n",
       "      <th>liked_genres</th>\n",
       "      <th>disliked_genres</th>\n",
       "      <th>liked_authors</th>\n",
       "      <th>disliked_authors</th>\n",
       "      <th>liked_ratings</th>\n",
       "      <th>disliked_ratings</th>\n",
       "      <th>target_book</th>\n",
       "      <th>target_book_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A100NGGXRQF0AQ</td>\n",
       "      <td>[Lenin's Tomb: The Last Days of the Soviet Emp...</td>\n",
       "      <td>[Mathematics and Sex, Spook: Science Tackles t...</td>\n",
       "      <td>[History / Social History, Nature / General, S...</td>\n",
       "      <td>[Family &amp; Relationships / General, Psychology ...</td>\n",
       "      <td>[Primo Levi, Andrew Newberg, Alison Gopnik, Va...</td>\n",
       "      <td>[Mary Roach, Renee Fredrickson, Stephen J. Ceci]</td>\n",
       "      <td>[5.0, 5.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0]</td>\n",
       "      <td>The Official Soviet SVD Manual</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A100YHBWL4TR4D</td>\n",
       "      <td>[The Book of Three, Drums of Autumn Hardcover ...</td>\n",
       "      <td>[Highland Desire]</td>\n",
       "      <td>[Juvenile Fiction / General, Fiction / Sea Sto...</td>\n",
       "      <td>[Fiction / Literary]</td>\n",
       "      <td>[Lloyd Alexander, Carlos Ruiz Zafon, F. Scott ...</td>\n",
       "      <td>[Amy Jarecki]</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>Tea Rose</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A101446I5AWY0Z</td>\n",
       "      <td>[1632 (The Assiti Shards), Anonymous Rex, Hist...</td>\n",
       "      <td>[The Starchild Trilogy, Washington Goes To War...</td>\n",
       "      <td>[History / Historiography, History / Maritime ...</td>\n",
       "      <td>[Fiction / Science Fiction, History / Indigeno...</td>\n",
       "      <td>[John Garth, Eric Flint, Roy Adkins, Herwig Wo...</td>\n",
       "      <td>[Barry Fell, Gunther E. Rothenberg, Ian W. Tol...</td>\n",
       "      <td>[3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, ...</td>\n",
       "      <td>[2.0, 2.0, 2.0, 2.0, 2.0, 2.0]</td>\n",
       "      <td>The Congress of Vienna: A Study in Allied Unit...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1016MYYF5QSTY</td>\n",
       "      <td>[1 Ragged Ridge Road, Pies (Company's Coming),...</td>\n",
       "      <td>[Buster Midnight's Cafe]</td>\n",
       "      <td>[Cooking / General, Travel / General, Fiction ...</td>\n",
       "      <td>[Fiction / Noir]</td>\n",
       "      <td>[Leonard Foglia, Marguerite Henry, Jean Paré, ...</td>\n",
       "      <td>[Sandra Dallas]</td>\n",
       "      <td>[4.0, 5.0, 5.0, 3.0, 5.0, 5.0, 5.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>A Sudden, Fearful Death</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A101BVV4DR3G81</td>\n",
       "      <td>[Memories, Dreams, Reflections, Complete Tai C...</td>\n",
       "      <td>[The Great Stillness: The Water Method of Taoi...</td>\n",
       "      <td>[Drama / Shakespeare, History / General, Ficti...</td>\n",
       "      <td>[Body, Mind &amp; Spirit / General]</td>\n",
       "      <td>[Dan Docherty, B. K. S. Iyengar, Tom Stoppard,...</td>\n",
       "      <td>[Bruce Kumar Frantzis]</td>\n",
       "      <td>[5.0, 3.0, 5.0, 3.0, 5.0, 5.0, 5.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>Travesties</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id                                        liked_books  \\\n",
       "0  A100NGGXRQF0AQ  [Lenin's Tomb: The Last Days of the Soviet Emp...   \n",
       "1  A100YHBWL4TR4D  [The Book of Three, Drums of Autumn Hardcover ...   \n",
       "2  A101446I5AWY0Z  [1632 (The Assiti Shards), Anonymous Rex, Hist...   \n",
       "3  A1016MYYF5QSTY  [1 Ragged Ridge Road, Pies (Company's Coming),...   \n",
       "4  A101BVV4DR3G81  [Memories, Dreams, Reflections, Complete Tai C...   \n",
       "\n",
       "                                      disliked_books  \\\n",
       "0  [Mathematics and Sex, Spook: Science Tackles t...   \n",
       "1                                  [Highland Desire]   \n",
       "2  [The Starchild Trilogy, Washington Goes To War...   \n",
       "3                           [Buster Midnight's Cafe]   \n",
       "4  [The Great Stillness: The Water Method of Taoi...   \n",
       "\n",
       "                                        liked_genres  \\\n",
       "0  [History / Social History, Nature / General, S...   \n",
       "1  [Juvenile Fiction / General, Fiction / Sea Sto...   \n",
       "2  [History / Historiography, History / Maritime ...   \n",
       "3  [Cooking / General, Travel / General, Fiction ...   \n",
       "4  [Drama / Shakespeare, History / General, Ficti...   \n",
       "\n",
       "                                     disliked_genres  \\\n",
       "0  [Family & Relationships / General, Psychology ...   \n",
       "1                               [Fiction / Literary]   \n",
       "2  [Fiction / Science Fiction, History / Indigeno...   \n",
       "3                                   [Fiction / Noir]   \n",
       "4                    [Body, Mind & Spirit / General]   \n",
       "\n",
       "                                       liked_authors  \\\n",
       "0  [Primo Levi, Andrew Newberg, Alison Gopnik, Va...   \n",
       "1  [Lloyd Alexander, Carlos Ruiz Zafon, F. Scott ...   \n",
       "2  [John Garth, Eric Flint, Roy Adkins, Herwig Wo...   \n",
       "3  [Leonard Foglia, Marguerite Henry, Jean Paré, ...   \n",
       "4  [Dan Docherty, B. K. S. Iyengar, Tom Stoppard,...   \n",
       "\n",
       "                                    disliked_authors  \\\n",
       "0   [Mary Roach, Renee Fredrickson, Stephen J. Ceci]   \n",
       "1                                      [Amy Jarecki]   \n",
       "2  [Barry Fell, Gunther E. Rothenberg, Ian W. Tol...   \n",
       "3                                    [Sandra Dallas]   \n",
       "4                             [Bruce Kumar Frantzis]   \n",
       "\n",
       "                                       liked_ratings  \\\n",
       "0  [5.0, 5.0, 4.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, ...   \n",
       "1      [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0]   \n",
       "2  [3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, ...   \n",
       "3                [4.0, 5.0, 5.0, 3.0, 5.0, 5.0, 5.0]   \n",
       "4                [5.0, 3.0, 5.0, 3.0, 5.0, 5.0, 5.0]   \n",
       "\n",
       "                 disliked_ratings  \\\n",
       "0                 [2.0, 2.0, 2.0]   \n",
       "1                           [1.0]   \n",
       "2  [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]   \n",
       "3                           [2.0]   \n",
       "4                           [1.0]   \n",
       "\n",
       "                                         target_book  target_book_rating  \n",
       "0                     The Official Soviet SVD Manual                 4.0  \n",
       "1                                           Tea Rose                 5.0  \n",
       "2  The Congress of Vienna: A Study in Allied Unit...                 4.0  \n",
       "3                            A Sudden, Fearful Death                 4.0  \n",
       "4                                         Travesties                 5.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessionized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af4b54-71fc-4bc2-8122-2d7be65d9d4d",
   "metadata": {},
   "source": [
    "## Augment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69648a6c-71b7-44f8-bfb0-c643ac4673e2",
   "metadata": {},
   "source": [
    "Down the line I realized that when training we'll want to pass in more than just the book's title to our book tower so I want to augment the dataset with the author and the summaries of all of those books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972eaa46-9ed8-44f8-9277-266e8260935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'book_id', 'title', 'author', 'publish_year', 'description',\n",
       "       'preview_link', 'normalized_popularity', 'genre_general',\n",
       "       'genre_specific', 'genre_combined', 'genre_consolidated',\n",
       "       'review_helpfulness', 'review_score', 'review_time', 'review_summary',\n",
       "       'review_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb05bae4-e1ce-4398-89a5-1f510d151597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Books: 100%|██████████| 15839/15839 [00:00<00:00, 696084.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Keep just the first instance of each book since we only care about the books' metadata, which should be the same regardless\n",
    "books_df_unique = books_df.drop_duplicates(subset=['title'], keep='first')\n",
    "\n",
    "# Convert books_df to a dictionary for fast lookup\n",
    "books_dict = books_df_unique.set_index('title')[['author', 'description', 'genre_consolidated']].to_dict(orient='index')\n",
    "\n",
    "target_book_author = []\n",
    "target_book_summary = []\n",
    "target_book_categories = []\n",
    "\n",
    "for title in tqdm(sessionized_df['target_book'], desc=\"Processing Books\"):\n",
    "    book_info = books_dict.get(title, {'author': '', 'description': '', 'genre_consolidated': ''})\n",
    "\n",
    "    target_book_author.append(book_info['author'])\n",
    "    target_book_summary.append(book_info['description'])\n",
    "    target_book_categories.append(book_info['genre_consolidated'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf57b7d0-2ad4-4818-98fe-9f44648ac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Though I'm keeping the column name as 'categories' it should reflect 'genre_consolidated' at all times henceforth\n",
    "\n",
    "sessionized_df['authors'] = target_book_author\n",
    "sessionized_df['description'] = target_book_summary\n",
    "sessionized_df['categories'] = target_book_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249e5f9-b813-44a6-8aa0-99f98c83cbc6",
   "metadata": {},
   "source": [
    "Now that we have the sessionized data, we can move into the splitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382228f0-e2a4-48f9-b255-a7b48a6c3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = train_test_split(sessionized_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023b23b-b84c-4f87-a834-98f0ce69d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e26a4c-4ab0-48a0-bd28-c95f10b36bec",
   "metadata": {},
   "source": [
    "## Converting string input to numerical\n",
    "\n",
    "So the model, when running on CPUs, is able to take care of string entries to numerical via our lookup layers internally. However, when trying to utilize the GPU for faster training, it appears as through string tensor conversions from CPU to GPU aren't supported. As a result, I'm looking into converting our string entries to numerical values prior to passing it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7244ecd6-ce51-40e9-94bb-720d4c116eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique values for user and book metadata\n",
    "unique_user_ids = sessionized_df['user_id'].astype(str).unique().tolist()\n",
    "unique_book_titles = books_df['title'].astype(str).unique().tolist()\n",
    "unique_genres = books_df['genre_consolidated'].astype(str).unique().tolist()\n",
    "unique_authors = books_df['author'].astype(str).unique().tolist()\n",
    "unique_summaries = books_df['description'].astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564eb473-7982-4f15-a5b8-5f511b63d09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15839, 71696, 102, 50079)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_user_ids), len(unique_book_titles), len(unique_genres), len(unique_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210a7e5c-a089-46dd-958d-d4ff4d9fc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions = 64 # 64\n",
    "\n",
    "# Create a StringLookup layer for user_id\n",
    "user_id_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_user_ids,\n",
    "    mask_token=None,\n",
    "    name='user_id_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for user_id\n",
    "user_id_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_user_ids) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='user_id_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for book_title\n",
    "book_title_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_book_titles,\n",
    "    mask_token=None,\n",
    "    name='book_title_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for book_title\n",
    "book_title_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_book_titles) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_title_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for genre\n",
    "book_genre_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_genres,\n",
    "    mask_token=None,\n",
    "    name='book_genre_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for genre\n",
    "book_genre_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_genres) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_genre_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for authors\n",
    "book_authors_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_authors,\n",
    "    mask_token=None,\n",
    "    name='book_authors_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for authors\n",
    "book_authors_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_authors) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_authors_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for description\n",
    "book_description_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_summaries,\n",
    "    mask_token=None,\n",
    "    name='book_description_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for descriptions\n",
    "book_description_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_summaries) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_description_embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87ff8e-6301-4ce3-9479-05cd77645bbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save the StringLookUp layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fc66e-b9ee-4d65-8bfb-95190f920305",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary\": unique_user_ids,\n",
    "    \"oov_token\": user_id_vocab_layer.oov_token\n",
    "}\n",
    "\n",
    "with open('user_id_vocab_layer.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f4ee5-6402-4191-ae32-52d86a8d998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A014566028TLL40XCY1YR\n",
    "\n",
    "with open(\"user_id_vocab_layer.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "user_id_vocab_layer = tf.keras.layers.StringLookup(vocabulary=config['vocabulary'], oov_token=config['oov_token'])\n",
    "\n",
    "input_data = \"A3A48XEYWLWH7T\"\n",
    "\n",
    "output = user_id_vocab_layer(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04321ddd-4146-4e38-b876-3e251810f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614b279-9887-4c9f-b454-3648b6a308b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "import boto3\n",
    "# import subprocess\n",
    "# import os\n",
    "# import pickle\n",
    "# import joblib\n",
    "# import tarfile\n",
    "# import shutil\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd63174-4ba0-437c-ba62-877b5d71d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3://w210recsys/model/recModel/modelFiles/\n",
    "bucket_name=\"w210recsys\"\n",
    "key_prefix=\"model/recModel/modelFiles\"\n",
    "s3_response = sm_session.upload_data(\"book_authors_vocab_layer.json\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcf188-f2db-4700-8f33-250aad74f18b",
   "metadata": {},
   "source": [
    "# Convert the information from books_df to be numerical using the string lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f65d9e78-abf5-42c5-b069-b53ae0444b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jane Eyre (Everyman's Classics)</td>\n",
       "      <td>Charlotte Brontë</td>\n",
       "      <td>Fiction / Mystery &amp; Detective</td>\n",
       "      <td>Jane Eyre (1847) has enjoyed huge popularity s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Pride &amp; Prejudice (Penguin Classics)</td>\n",
       "      <td>Ibi Zoboi</td>\n",
       "      <td>Young Adult Fiction / General</td>\n",
       "      <td>In a timely update of Jane Austen's Pride and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>Harper Lee</td>\n",
       "      <td>Drama / General</td>\n",
       "      <td>Harper Lee's classic novel of a lawyer in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Harry Potter and The Sorcerer's Stone</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>Juvenile Fiction / Fantasy &amp; Magic</td>\n",
       "      <td>Celebrate 20 years of Harry Potter magic! Harr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>The Hobbit</td>\n",
       "      <td>J. R. R. Tolkien</td>\n",
       "      <td>Juvenile Fiction / General</td>\n",
       "      <td>Celebrating 75 years of one of the world's mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321296</th>\n",
       "      <td>Seeking His Mind (Voice from the Monastery)</td>\n",
       "      <td>Michael Casey</td>\n",
       "      <td>Religion / General</td>\n",
       "      <td>Michael Casey, a monk and scholar who has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321297</th>\n",
       "      <td>Jane's Marine Propulsion</td>\n",
       "      <td>Keith Henderson</td>\n",
       "      <td>Technology &amp; Engineering / General</td>\n",
       "      <td>Jane's Marine propulsion is an exceptional one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321298</th>\n",
       "      <td>Across the Bridge</td>\n",
       "      <td>John Lewis</td>\n",
       "      <td>Political Science / General</td>\n",
       "      <td>Winner of the NAACP Image Award for Outstandin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321299</th>\n",
       "      <td>In the Moons of Borea</td>\n",
       "      <td>Brian Lumley</td>\n",
       "      <td>Fiction / World Literature</td>\n",
       "      <td>Titus Crow and his companions continue their b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321300</th>\n",
       "      <td>Speak My Name : Black Men on Masculinity and t...</td>\n",
       "      <td>Don Belton</td>\n",
       "      <td>Social Science / General</td>\n",
       "      <td>Through the voices of some of today's most pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71696 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title           authors  \\\n",
       "0                         Jane Eyre (Everyman's Classics)  Charlotte Brontë   \n",
       "216                  Pride & Prejudice (Penguin Classics)         Ibi Zoboi   \n",
       "524                                 To Kill a Mockingbird        Harper Lee   \n",
       "954                 Harry Potter and The Sorcerer's Stone     J. K. Rowling   \n",
       "1548                                           The Hobbit  J. R. R. Tolkien   \n",
       "...                                                   ...               ...   \n",
       "321296        Seeking His Mind (Voice from the Monastery)     Michael Casey   \n",
       "321297                           Jane's Marine Propulsion   Keith Henderson   \n",
       "321298                                  Across the Bridge        John Lewis   \n",
       "321299                              In the Moons of Borea      Brian Lumley   \n",
       "321300  Speak My Name : Black Men on Masculinity and t...        Don Belton   \n",
       "\n",
       "                                categories  \\\n",
       "0            Fiction / Mystery & Detective   \n",
       "216          Young Adult Fiction / General   \n",
       "524                        Drama / General   \n",
       "954     Juvenile Fiction / Fantasy & Magic   \n",
       "1548            Juvenile Fiction / General   \n",
       "...                                    ...   \n",
       "321296                  Religion / General   \n",
       "321297  Technology & Engineering / General   \n",
       "321298         Political Science / General   \n",
       "321299          Fiction / World Literature   \n",
       "321300            Social Science / General   \n",
       "\n",
       "                                              description  \n",
       "0       Jane Eyre (1847) has enjoyed huge popularity s...  \n",
       "216     In a timely update of Jane Austen's Pride and ...  \n",
       "524     Harper Lee's classic novel of a lawyer in the ...  \n",
       "954     Celebrate 20 years of Harry Potter magic! Harr...  \n",
       "1548    Celebrating 75 years of one of the world's mos...  \n",
       "...                                                   ...  \n",
       "321296  Michael Casey, a monk and scholar who has been...  \n",
       "321297  Jane's Marine propulsion is an exceptional one...  \n",
       "321298  Winner of the NAACP Image Award for Outstandin...  \n",
       "321299  Titus Crow and his companions continue their b...  \n",
       "321300  Through the voices of some of today's most pro...  \n",
       "\n",
       "[71696 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_books_df = books_df[['title', 'author', 'genre_consolidated', 'description']].copy()\n",
    "numerical_books_df.columns = ['title', 'authors', 'categories', 'description']\n",
    "\n",
    "numerical_books_df = numerical_books_df.drop_duplicates(subset=['title'], keep='first')\n",
    "\n",
    "\n",
    "numerical_books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3f883e-168b-41e6-af5e-fd746aa5f3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321296</th>\n",
       "      <td>71692</td>\n",
       "      <td>50078</td>\n",
       "      <td>16</td>\n",
       "      <td>69555</td>\n",
       "      <td>71691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321297</th>\n",
       "      <td>71693</td>\n",
       "      <td>50079</td>\n",
       "      <td>69</td>\n",
       "      <td>69556</td>\n",
       "      <td>71692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321298</th>\n",
       "      <td>71694</td>\n",
       "      <td>37457</td>\n",
       "      <td>8</td>\n",
       "      <td>69557</td>\n",
       "      <td>71693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321299</th>\n",
       "      <td>71695</td>\n",
       "      <td>14930</td>\n",
       "      <td>6</td>\n",
       "      <td>69558</td>\n",
       "      <td>71694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321300</th>\n",
       "      <td>71696</td>\n",
       "      <td>44320</td>\n",
       "      <td>53</td>\n",
       "      <td>69559</td>\n",
       "      <td>71695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71696 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title  authors  categories  description  book_id\n",
       "0           1        1           1            1        0\n",
       "216         2        2           2            2        1\n",
       "524         3        3           3            3        2\n",
       "954         4        4           4            4        3\n",
       "1548        5        5           5            5        4\n",
       "...       ...      ...         ...          ...      ...\n",
       "321296  71692    50078          16        69555    71691\n",
       "321297  71693    50079          69        69556    71692\n",
       "321298  71694    37457           8        69557    71693\n",
       "321299  71695    14930           6        69558    71694\n",
       "321300  71696    44320          53        69559    71695\n",
       "\n",
       "[71696 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the lookup transformation to each column\n",
    "\n",
    "numerical_books_df['title'] = book_title_vocab_layer(numerical_books_df['title']).numpy()\n",
    "numerical_books_df['authors'] = book_authors_vocab_layer(numerical_books_df['authors']).numpy()\n",
    "numerical_books_df['categories'] = book_genre_vocab_layer(numerical_books_df['categories']).numpy()\n",
    "numerical_books_df['description'] = book_description_vocab_layer(numerical_books_df['description']).numpy()\n",
    "\n",
    "# Also add in the 'book_id'\n",
    "numerical_books_df['book_id'] = [i for i in range(0, numerical_books_df.shape[0])]\n",
    "\n",
    "numerical_books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbc9bcdf-e407-4b05-abb4-5f5e4e1006ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'liked_books', 'disliked_books', 'liked_genres',\n",
       "       'disliked_genres', 'liked_authors', 'disliked_authors', 'liked_ratings',\n",
       "       'disliked_ratings', 'target_book', 'target_book_rating', 'authors',\n",
       "       'description', 'categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_sessionized_df = sessionized_df.copy()\n",
    "\n",
    "numerical_sessionized_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf9d3c1f-e985-4a58-b28d-df082be1937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15839"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numerical_sessionized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dfa69-84d2-493c-a5be-c537f3c2bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_sessionized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee81b1d-8161-402e-96cf-948f062a2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lookup layer in a vectorized manner\n",
    "def fast_lookup_list(values, lookup_layer):\n",
    "    values = values.apply(lambda x: x if isinstance(x, list) else [\"UNKNOWN\"])  # Ensure lists\n",
    "    tensor_input = tf.ragged.constant(values.tolist(), dtype=tf.string)  # Convert to ragged tensor\n",
    "    return lookup_layer(tensor_input).numpy().tolist()  # Apply lookup in batch and convert to list\n",
    "\n",
    "# Apply lookup in bulk for better performance\n",
    "numerical_sessionized_df['user_id'] = user_id_vocab_layer(numerical_sessionized_df['user_id'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['liked_books'] = fast_lookup_list(numerical_sessionized_df['liked_books'], book_title_vocab_layer)\n",
    "numerical_sessionized_df['disliked_books'] = fast_lookup_list(numerical_sessionized_df['disliked_books'], book_title_vocab_layer)\n",
    "\n",
    "numerical_sessionized_df['liked_genres'] = fast_lookup_list(numerical_sessionized_df['liked_genres'], book_genre_vocab_layer)\n",
    "numerical_sessionized_df['disliked_genres'] = fast_lookup_list(numerical_sessionized_df['disliked_genres'], book_genre_vocab_layer)\n",
    "\n",
    "numerical_sessionized_df['liked_authors'] = fast_lookup_list(numerical_sessionized_df['liked_authors'], book_authors_vocab_layer)\n",
    "numerical_sessionized_df['disliked_authors'] = fast_lookup_list(numerical_sessionized_df['disliked_authors'], book_authors_vocab_layer)\n",
    "\n",
    "numerical_sessionized_df['target_book'] = book_title_vocab_layer(numerical_sessionized_df['target_book'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['authors'] = book_authors_vocab_layer(numerical_sessionized_df['authors'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['description'] = book_description_vocab_layer(numerical_sessionized_df['description'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['categories'] = book_genre_vocab_layer(numerical_sessionized_df['categories'].astype(str)).numpy()\n",
    "\n",
    "# Increment the ratings up by 1 to leave 0 to be the padding\n",
    "\n",
    "def rating_shift(ratings):\n",
    "\n",
    "    if type(ratings) == list:\n",
    "        if len(ratings) == 0:\n",
    "            return ratings\n",
    "\n",
    "        return [entry + 1 for entry in ratings]\n",
    "\n",
    "    else:\n",
    "\n",
    "        return ratings + 1\n",
    "\n",
    "\n",
    "numerical_sessionized_df['liked_ratings'] = numerical_sessionized_df['liked_ratings'].apply(lambda x: rating_shift(x))\n",
    "numerical_sessionized_df['disliked_ratings'] = numerical_sessionized_df['disliked_ratings'].apply(lambda x: rating_shift(x))\n",
    "numerical_sessionized_df['target_book_rating'] = numerical_sessionized_df['target_book_rating'].apply(lambda x: rating_shift(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb55006-d5fe-44cc-a3d7-bc85a0dd24e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>liked_books</th>\n",
       "      <th>disliked_books</th>\n",
       "      <th>liked_genres</th>\n",
       "      <th>disliked_genres</th>\n",
       "      <th>liked_authors</th>\n",
       "      <th>disliked_authors</th>\n",
       "      <th>liked_ratings</th>\n",
       "      <th>disliked_ratings</th>\n",
       "      <th>target_book</th>\n",
       "      <th>target_book_rating</th>\n",
       "      <th>authors</th>\n",
       "      <th>description</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[6305, 21182, 20922, 658, 5764, 13248, 6148, 1...</td>\n",
       "      <td>[23325, 1007, 14131]</td>\n",
       "      <td>[60, 50, 27, 29, 34, 68, 10, 52, 6, 33, 53]</td>\n",
       "      <td>[36, 52, 85]</td>\n",
       "      <td>[9454, 12846, 4340, 4065, 4285, 458, 14942, 15...</td>\n",
       "      <td>[692, 10080, 16638]</td>\n",
       "      <td>[6.0, 6.0, 5.0, 5.0, 6.0, 6.0, 6.0, 5.0, 4.0, ...</td>\n",
       "      <td>[3.0, 3.0, 3.0]</td>\n",
       "      <td>56223</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39401</td>\n",
       "      <td>54887</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[491, 444, 1432, 48930, 11, 10097, 197, 33041,...</td>\n",
       "      <td>[48343]</td>\n",
       "      <td>[5, 41, 19, 6, 100, 13]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>[343, 69, 9, 7160, 34411, 183, 194, 147]</td>\n",
       "      <td>[34000]</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>607</td>\n",
       "      <td>6.0</td>\n",
       "      <td>420</td>\n",
       "      <td>583</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[859, 11445, 16211, 11151, 3102, 8855, 55148, ...</td>\n",
       "      <td>[41712, 16775, 21684, 15588, 27706, 33754]</td>\n",
       "      <td>[71, 23, 26, 40, 60, 29, 1, 98, 6, 53, 65]</td>\n",
       "      <td>[14, 73, 1, 98, 57, 90]</td>\n",
       "      <td>[7930, 593, 3621, 11559, 8144, 6283, 29217, 17...</td>\n",
       "      <td>[19662, 23940, 11967, 1245, 1767, 11111]</td>\n",
       "      <td>[4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, ...</td>\n",
       "      <td>[3.0, 3.0, 3.0, 3.0, 3.0, 3.0]</td>\n",
       "      <td>45074</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31736</td>\n",
       "      <td>44138</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[20216, 61316, 61811, 10629, 1880, 4407, 4964]</td>\n",
       "      <td>[8057]</td>\n",
       "      <td>[79, 80, 17, 5, 1]</td>\n",
       "      <td>[76]</td>\n",
       "      <td>[14407, 2560, 42849, 1317, 2284]</td>\n",
       "      <td>[1681]</td>\n",
       "      <td>[5.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>9679</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2284</td>\n",
       "      <td>9539</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[3505, 26398, 1380, 16599, 27875, 2200, 1101]</td>\n",
       "      <td>[31563]</td>\n",
       "      <td>[70, 29, 41, 38, 82, 33]</td>\n",
       "      <td>[58]</td>\n",
       "      <td>[18764, 967, 761, 1547, 2170, 2447, 19781]</td>\n",
       "      <td>[10506]</td>\n",
       "      <td>[6.0, 4.0, 6.0, 4.0, 6.0, 6.0, 6.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>27369</td>\n",
       "      <td>6.0</td>\n",
       "      <td>761</td>\n",
       "      <td>26892</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                        liked_books  \\\n",
       "0        1  [6305, 21182, 20922, 658, 5764, 13248, 6148, 1...   \n",
       "1        2  [491, 444, 1432, 48930, 11, 10097, 197, 33041,...   \n",
       "2        3  [859, 11445, 16211, 11151, 3102, 8855, 55148, ...   \n",
       "3        4     [20216, 61316, 61811, 10629, 1880, 4407, 4964]   \n",
       "4        5      [3505, 26398, 1380, 16599, 27875, 2200, 1101]   \n",
       "\n",
       "                               disliked_books  \\\n",
       "0                        [23325, 1007, 14131]   \n",
       "1                                     [48343]   \n",
       "2  [41712, 16775, 21684, 15588, 27706, 33754]   \n",
       "3                                      [8057]   \n",
       "4                                     [31563]   \n",
       "\n",
       "                                  liked_genres          disliked_genres  \\\n",
       "0  [60, 50, 27, 29, 34, 68, 10, 52, 6, 33, 53]             [36, 52, 85]   \n",
       "1                      [5, 41, 19, 6, 100, 13]                      [9]   \n",
       "2   [71, 23, 26, 40, 60, 29, 1, 98, 6, 53, 65]  [14, 73, 1, 98, 57, 90]   \n",
       "3                           [79, 80, 17, 5, 1]                     [76]   \n",
       "4                     [70, 29, 41, 38, 82, 33]                     [58]   \n",
       "\n",
       "                                       liked_authors  \\\n",
       "0  [9454, 12846, 4340, 4065, 4285, 458, 14942, 15...   \n",
       "1           [343, 69, 9, 7160, 34411, 183, 194, 147]   \n",
       "2  [7930, 593, 3621, 11559, 8144, 6283, 29217, 17...   \n",
       "3                   [14407, 2560, 42849, 1317, 2284]   \n",
       "4         [18764, 967, 761, 1547, 2170, 2447, 19781]   \n",
       "\n",
       "                           disliked_authors  \\\n",
       "0                       [692, 10080, 16638]   \n",
       "1                                   [34000]   \n",
       "2  [19662, 23940, 11967, 1245, 1767, 11111]   \n",
       "3                                    [1681]   \n",
       "4                                   [10506]   \n",
       "\n",
       "                                       liked_ratings  \\\n",
       "0  [6.0, 6.0, 5.0, 5.0, 6.0, 6.0, 6.0, 5.0, 4.0, ...   \n",
       "1      [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0]   \n",
       "2  [4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, ...   \n",
       "3                [5.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0]   \n",
       "4                [6.0, 4.0, 6.0, 4.0, 6.0, 6.0, 6.0]   \n",
       "\n",
       "                 disliked_ratings  target_book  target_book_rating  authors  \\\n",
       "0                 [3.0, 3.0, 3.0]        56223                 5.0    39401   \n",
       "1                           [2.0]          607                 6.0      420   \n",
       "2  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0]        45074                 5.0    31736   \n",
       "3                           [3.0]         9679                 5.0     2284   \n",
       "4                           [2.0]        27369                 6.0      761   \n",
       "\n",
       "   description  categories  \n",
       "0        54887          82  \n",
       "1          583           6  \n",
       "2        44138          29  \n",
       "3         9539          37  \n",
       "4        26892          70  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_sessionized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ba55ccd-56be-4900-8795-98db1f3ec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "HT = {}\n",
    "\n",
    "for col in numerical_sessionized_df.columns:\n",
    "    try:\n",
    "        for entry in numerical_sessionized_df[col]:\n",
    "            \n",
    "            if col not in HT:\n",
    "                HT[col] = [len(entry)]\n",
    "            else:\n",
    "                HT[col].append(len(entry))\n",
    "    except:\n",
    "        HT[col] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf051ce-5013-4211-bd43-2d2eada9f7c3",
   "metadata": {},
   "source": [
    "So later down the line I noticed that we can't call our mode for inferences with ragged tensors since their sizes can vary and the model needs fixed sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ddfb45d-2105-4266-aa45-9f85a10bf115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id 1\n",
      "liked_books 34\n",
      "disliked_books 4\n",
      "liked_genres 18\n",
      "disliked_genres 3\n",
      "liked_authors 30\n",
      "disliked_authors 4\n",
      "liked_ratings 34\n",
      "disliked_ratings 4\n",
      "target_book 1\n",
      "target_book_rating 1\n",
      "authors 1\n",
      "description 1\n",
      "categories 1\n"
     ]
    }
   ],
   "source": [
    "for col in HT.keys():\n",
    "    print(col, int(np.percentile(HT[col], 90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d039e9-3e3e-47c6-8155-d387cd37c026",
   "metadata": {},
   "source": [
    "We need to consider what *most* of our samples' lengths are and add in some extra space for future inferences with more information. Since in the future I plan on having feed back on every 10 books or so I'll allocate ~20 slots for each list just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca72b190-9e53-48b9-8c90-92738432782a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 1,\n",
       " 'liked_books': 20,\n",
       " 'disliked_books': 20,\n",
       " 'liked_genres': 20,\n",
       " 'disliked_genres': 20,\n",
       " 'liked_authors': 20,\n",
       " 'disliked_authors': 20,\n",
       " 'liked_ratings': 20,\n",
       " 'disliked_ratings': 20,\n",
       " 'target_book': 1,\n",
       " 'target_book_rating': 1,\n",
       " 'authors': 1,\n",
       " 'description': 1,\n",
       " 'categories': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HT['liked_books'] = 20\n",
    "HT['disliked_books'] = 20\n",
    "\n",
    "HT['liked_genres'] = 20\n",
    "HT['disliked_genres'] = 20\n",
    "\n",
    "HT['liked_authors'] = 20\n",
    "HT['disliked_authors'] = 20\n",
    "\n",
    "HT['liked_ratings'] = 20\n",
    "HT['disliked_ratings'] = 20\n",
    "\n",
    "HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "575014a4-3c86-4ab5-b23e-5190c7759170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_numpy_array(column, max_len, padding_value=0):\n",
    "    return column.apply(lambda x: np.pad(x[:max_len], (0, max_len - min(len(x), max_len)), constant_values=padding_value))\n",
    "\n",
    "def pad_column(column, max_len, padding_value=0):\n",
    "    return column.apply(lambda x: (x[:max_len] if len(x) > max_len else x + [padding_value] * (max_len - len(x))))\n",
    "\n",
    "for column, max_len in HT.items():\n",
    "    first_entry = numerical_sessionized_df[column].iloc[0]\n",
    "    \n",
    "    if isinstance(first_entry, np.ndarray):\n",
    "        numerical_sessionized_df[column] = pad_numpy_array(numerical_sessionized_df[column], max_len)\n",
    "        \n",
    "    elif isinstance(first_entry, list):\n",
    "        numerical_sessionized_df[column] = pad_column(numerical_sessionized_df[column], max_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b17ad4ed-4379-497f-bf15-f14d3473b302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>liked_books</th>\n",
       "      <th>disliked_books</th>\n",
       "      <th>liked_genres</th>\n",
       "      <th>disliked_genres</th>\n",
       "      <th>liked_authors</th>\n",
       "      <th>disliked_authors</th>\n",
       "      <th>liked_ratings</th>\n",
       "      <th>disliked_ratings</th>\n",
       "      <th>target_book</th>\n",
       "      <th>target_book_rating</th>\n",
       "      <th>authors</th>\n",
       "      <th>description</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[6305, 21182, 20922, 658, 5764, 13248, 6148, 1...</td>\n",
       "      <td>[23325, 1007, 14131, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[60, 50, 27, 29, 34, 68, 10, 52, 6, 33, 53, 0,...</td>\n",
       "      <td>[36, 52, 85, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9454, 12846, 4340, 4065, 4285, 458, 14942, 15...</td>\n",
       "      <td>[692, 10080, 16638, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[6.0, 6.0, 5.0, 5.0, 6.0, 6.0, 6.0, 5.0, 4.0, ...</td>\n",
       "      <td>[3.0, 3.0, 3.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>56223</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39401</td>\n",
       "      <td>54887</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[491, 444, 1432, 48930, 11, 10097, 197, 33041,...</td>\n",
       "      <td>[48343, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[5, 41, 19, 6, 100, 13, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[343, 69, 9, 7160, 34411, 183, 194, 147, 0, 0,...</td>\n",
       "      <td>[34000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, ...</td>\n",
       "      <td>[2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>607</td>\n",
       "      <td>6.0</td>\n",
       "      <td>420</td>\n",
       "      <td>583</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[859, 11445, 16211, 11151, 3102, 8855, 55148, ...</td>\n",
       "      <td>[41712, 16775, 21684, 15588, 27706, 33754, 0, ...</td>\n",
       "      <td>[71, 23, 26, 40, 60, 29, 1, 98, 6, 53, 65, 0, ...</td>\n",
       "      <td>[14, 73, 1, 98, 57, 90, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[7930, 593, 3621, 11559, 8144, 6283, 29217, 17...</td>\n",
       "      <td>[19662, 23940, 11967, 1245, 1767, 11111, 0, 0,...</td>\n",
       "      <td>[4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, ...</td>\n",
       "      <td>[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>45074</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31736</td>\n",
       "      <td>44138</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[20216, 61316, 61811, 10629, 1880, 4407, 4964,...</td>\n",
       "      <td>[8057, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[79, 80, 17, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[14407, 2560, 42849, 1317, 2284, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1681, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0, 0, 0, 0, 0...</td>\n",
       "      <td>[3.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>9679</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2284</td>\n",
       "      <td>9539</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[3505, 26398, 1380, 16599, 27875, 2200, 1101, ...</td>\n",
       "      <td>[31563, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[70, 29, 41, 38, 82, 33, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[18764, 967, 761, 1547, 2170, 2447, 19781, 0, ...</td>\n",
       "      <td>[10506, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[6.0, 4.0, 6.0, 4.0, 6.0, 6.0, 6.0, 0, 0, 0, 0...</td>\n",
       "      <td>[2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>27369</td>\n",
       "      <td>6.0</td>\n",
       "      <td>761</td>\n",
       "      <td>26892</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                        liked_books  \\\n",
       "0        1  [6305, 21182, 20922, 658, 5764, 13248, 6148, 1...   \n",
       "1        2  [491, 444, 1432, 48930, 11, 10097, 197, 33041,...   \n",
       "2        3  [859, 11445, 16211, 11151, 3102, 8855, 55148, ...   \n",
       "3        4  [20216, 61316, 61811, 10629, 1880, 4407, 4964,...   \n",
       "4        5  [3505, 26398, 1380, 16599, 27875, 2200, 1101, ...   \n",
       "\n",
       "                                      disliked_books  \\\n",
       "0  [23325, 1007, 14131, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [48343, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [41712, 16775, 21684, 15588, 27706, 33754, 0, ...   \n",
       "3  [8057, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [31563, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                        liked_genres  \\\n",
       "0  [60, 50, 27, 29, 34, 68, 10, 52, 6, 33, 53, 0,...   \n",
       "1  [5, 41, 19, 6, 100, 13, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [71, 23, 26, 40, 60, 29, 1, 98, 6, 53, 65, 0, ...   \n",
       "3  [79, 80, 17, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [70, 29, 41, 38, 82, 33, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                     disliked_genres  \\\n",
       "0  [36, 52, 85, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [14, 73, 1, 98, 57, 90, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  [58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                       liked_authors  \\\n",
       "0  [9454, 12846, 4340, 4065, 4285, 458, 14942, 15...   \n",
       "1  [343, 69, 9, 7160, 34411, 183, 194, 147, 0, 0,...   \n",
       "2  [7930, 593, 3621, 11559, 8144, 6283, 29217, 17...   \n",
       "3  [14407, 2560, 42849, 1317, 2284, 0, 0, 0, 0, 0...   \n",
       "4  [18764, 967, 761, 1547, 2170, 2447, 19781, 0, ...   \n",
       "\n",
       "                                    disliked_authors  \\\n",
       "0  [692, 10080, 16638, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [34000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [19662, 23940, 11967, 1245, 1767, 11111, 0, 0,...   \n",
       "3  [1681, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [10506, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                       liked_ratings  \\\n",
       "0  [6.0, 6.0, 5.0, 5.0, 6.0, 6.0, 6.0, 5.0, 4.0, ...   \n",
       "1  [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, ...   \n",
       "2  [4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, ...   \n",
       "3  [5.0, 6.0, 6.0, 4.0, 6.0, 6.0, 6.0, 0, 0, 0, 0...   \n",
       "4  [6.0, 4.0, 6.0, 4.0, 6.0, 6.0, 6.0, 0, 0, 0, 0...   \n",
       "\n",
       "                                    disliked_ratings  target_book  \\\n",
       "0  [3.0, 3.0, 3.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        56223   \n",
       "1  [2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...          607   \n",
       "2  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 0, 0, 0, 0, 0, ...        45074   \n",
       "3  [3.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...         9679   \n",
       "4  [2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...        27369   \n",
       "\n",
       "   target_book_rating  authors  description  categories  \n",
       "0                 5.0    39401        54887          82  \n",
       "1                 6.0      420          583           6  \n",
       "2                 5.0    31736        44138          29  \n",
       "3                 5.0     2284         9539          37  \n",
       "4                 6.0      761        26892          70  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_sessionized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37c4950e-e806-4b67-95ab-8930b1f7674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_df, num_test_df = train_test_split(numerical_sessionized_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f241909-3e1a-4db9-b201-3e63a5a7d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>liked_books</th>\n",
       "      <th>disliked_books</th>\n",
       "      <th>liked_genres</th>\n",
       "      <th>disliked_genres</th>\n",
       "      <th>liked_authors</th>\n",
       "      <th>disliked_authors</th>\n",
       "      <th>liked_ratings</th>\n",
       "      <th>disliked_ratings</th>\n",
       "      <th>target_book</th>\n",
       "      <th>target_book_rating</th>\n",
       "      <th>authors</th>\n",
       "      <th>description</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12217</th>\n",
       "      <td>12218</td>\n",
       "      <td>[31971, 25199, 33441, 6756, 5111, 7647, 45010,...</td>\n",
       "      <td>[22758, 41342, 35391, 5333, 14016, 35968, 4504...</td>\n",
       "      <td>[45, 81, 57, 34, 56, 30, 62, 26, 17, 39, 42, 1...</td>\n",
       "      <td>[30, 77, 14, 29, 1, 41, 84, 39, 6, 44, 9, 13, ...</td>\n",
       "      <td>[11605, 31693, 203, 3480, 10202, 14724, 8219, ...</td>\n",
       "      <td>[26439, 464, 8691, 39723, 7036, 13069, 2809, 2...</td>\n",
       "      <td>[4.0, 4.0, 4.0, 6.0, 6.0, 6.0, 4.0, 5.0, 5.0, ...</td>\n",
       "      <td>[3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...</td>\n",
       "      <td>12991</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9261</td>\n",
       "      <td>12793</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>170</td>\n",
       "      <td>[22411, 700, 5840, 63, 26701, 16590, 12878, 74...</td>\n",
       "      <td>[7074, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[48, 5, 70, 12, 66, 10, 6, 100, 78, 28, 0, 0, ...</td>\n",
       "      <td>[10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[16000, 2286, 4121, 475, 44, 9185, 517, 5373, ...</td>\n",
       "      <td>[5014, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, 6.0, 5.0, ...</td>\n",
       "      <td>[2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>22052</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10690</td>\n",
       "      <td>21701</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>2100</td>\n",
       "      <td>[443, 442, 354, 1191, 2908, 4488, 3, 0, 0, 0, ...</td>\n",
       "      <td>[285, 2897, 558, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[30, 3, 2, 48, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[2, 9, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1211, 313, 3, 56, 206, 82, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[388, 206, 2020, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[6.0, 6.0, 5.0, 6.0, 5.0, 5.0, 6.0, 0, 0, 0, 0...</td>\n",
       "      <td>[3.0, 3.0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>105</td>\n",
       "      <td>5.0</td>\n",
       "      <td>82</td>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>4949</td>\n",
       "      <td>[682, 1061, 20025, 40, 253, 408, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6, 20, 9, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[32, 187, 733, 14267, 289, 470, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5.0, 4.0, 4.0, 6.0, 6.0, 6.0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>470</td>\n",
       "      <td>6.0</td>\n",
       "      <td>333</td>\n",
       "      <td>449</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>5772</td>\n",
       "      <td>[380, 37443, 858, 8254, 16387, 927, 8476, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[16, 20, 4, 68, 77, 55, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[11681, 636, 592, 267, 3826, 5841, 6011, 0, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6.0, 6.0, 6.0, 6.0, 4.0, 6.0, 6.0, 0, 0, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>43443</td>\n",
       "      <td>6.0</td>\n",
       "      <td>267</td>\n",
       "      <td>42562</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                                        liked_books  \\\n",
       "12217    12218  [31971, 25199, 33441, 6756, 5111, 7647, 45010,...   \n",
       "169        170  [22411, 700, 5840, 63, 26701, 16590, 12878, 74...   \n",
       "2099      2100  [443, 442, 354, 1191, 2908, 4488, 3, 0, 0, 0, ...   \n",
       "4948      4949  [682, 1061, 20025, 40, 253, 408, 0, 0, 0, 0, 0...   \n",
       "5771      5772  [380, 37443, 858, 8254, 16387, 927, 8476, 0, 0...   \n",
       "\n",
       "                                          disliked_books  \\\n",
       "12217  [22758, 41342, 35391, 5333, 14016, 35968, 4504...   \n",
       "169    [7074, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2099   [285, 2897, 558, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4948   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5771   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            liked_genres  \\\n",
       "12217  [45, 81, 57, 34, 56, 30, 62, 26, 17, 39, 42, 1...   \n",
       "169    [48, 5, 70, 12, 66, 10, 6, 100, 78, 28, 0, 0, ...   \n",
       "2099   [30, 3, 2, 48, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4948   [6, 20, 9, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "5771   [16, 20, 4, 68, 77, 55, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                         disliked_genres  \\\n",
       "12217  [30, 77, 14, 29, 1, 41, 84, 39, 6, 44, 9, 13, ...   \n",
       "169    [10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2099   [2, 9, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4948   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5771   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           liked_authors  \\\n",
       "12217  [11605, 31693, 203, 3480, 10202, 14724, 8219, ...   \n",
       "169    [16000, 2286, 4121, 475, 44, 9185, 517, 5373, ...   \n",
       "2099   [1211, 313, 3, 56, 206, 82, 0, 0, 0, 0, 0, 0, ...   \n",
       "4948   [32, 187, 733, 14267, 289, 470, 0, 0, 0, 0, 0,...   \n",
       "5771   [11681, 636, 592, 267, 3826, 5841, 6011, 0, 0,...   \n",
       "\n",
       "                                        disliked_authors  \\\n",
       "12217  [26439, 464, 8691, 39723, 7036, 13069, 2809, 2...   \n",
       "169    [5014, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2099   [388, 206, 2020, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4948   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5771   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           liked_ratings  \\\n",
       "12217  [4.0, 4.0, 4.0, 6.0, 6.0, 6.0, 4.0, 5.0, 5.0, ...   \n",
       "169    [6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, 6.0, 5.0, ...   \n",
       "2099   [6.0, 6.0, 5.0, 6.0, 5.0, 5.0, 6.0, 0, 0, 0, 0...   \n",
       "4948   [5.0, 4.0, 4.0, 6.0, 6.0, 6.0, 0, 0, 0, 0, 0, ...   \n",
       "5771   [6.0, 6.0, 6.0, 6.0, 4.0, 6.0, 6.0, 0, 0, 0, 0...   \n",
       "\n",
       "                                        disliked_ratings  target_book  \\\n",
       "12217  [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...        12991   \n",
       "169    [2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...        22052   \n",
       "2099   [3.0, 3.0, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          105   \n",
       "4948   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          470   \n",
       "5771   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        43443   \n",
       "\n",
       "       target_book_rating  authors  description  categories  \n",
       "12217                 4.0     9261        12793           7  \n",
       "169                   6.0    10690        21701          48  \n",
       "2099                  5.0       82          101           6  \n",
       "4948                  6.0      333          449          30  \n",
       "5771                  6.0      267        42562          55  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfe142-9a3d-4783-a05d-71d885387c6d",
   "metadata": {},
   "source": [
    "## Save need data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d36b93b-4734-4e33-b764-c0ed19ad4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Starting dataset creation...\n",
      "✅ Datasets successfully created!\n"
     ]
    }
   ],
   "source": [
    "print(\"🚨 Starting dataset creation...\")\n",
    "\n",
    "num_train_ds = tf.data.Dataset.from_tensor_slices({\n",
    "    'user_id': tf.constant(num_train_df['user_id'].tolist(), dtype=tf.int64),\n",
    "    'liked_books': tf.constant(num_train_df['liked_books'].tolist(), dtype=tf.int64),\n",
    "    'disliked_books': tf.constant(num_train_df['disliked_books'].tolist(), dtype=tf.int64),\n",
    "    'liked_genres': tf.constant(num_train_df['liked_genres'].tolist(), dtype=tf.int64),\n",
    "    'disliked_genres': tf.constant(num_train_df['disliked_genres'].tolist(), dtype=tf.int64),\n",
    "    'liked_authors': tf.constant(num_train_df['liked_authors'].tolist(), dtype=tf.int64),\n",
    "    'disliked_authors': tf.constant(num_train_df['disliked_authors'].tolist(), dtype=tf.int64),\n",
    "    'liked_ratings': tf.constant(num_train_df['liked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'disliked_ratings': tf.constant(num_train_df['disliked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'target_book': tf.constant(num_train_df['target_book'], dtype=tf.int64),\n",
    "    'authors': tf.constant(num_train_df['authors'], dtype=tf.int64),\n",
    "    'description': tf.constant(num_train_df['description'], dtype=tf.int64),\n",
    "    'categories': tf.constant(num_train_df['categories'], dtype=tf.int64),\n",
    "    'target_book_rating': tf.constant(num_train_df['target_book_rating'], dtype=tf.float32),\n",
    "})\n",
    "\n",
    "num_test_ds = tf.data.Dataset.from_tensor_slices({\n",
    "    'user_id': tf.constant(num_test_df['user_id'].tolist(), dtype=tf.int64),\n",
    "    'liked_books': tf.constant(num_test_df['liked_books'].tolist(), dtype=tf.int64),\n",
    "    'disliked_books': tf.constant(num_test_df['disliked_books'].tolist(), dtype=tf.int64),\n",
    "    'liked_genres': tf.constant(num_test_df['liked_genres'].tolist(), dtype=tf.int64),\n",
    "    'disliked_genres': tf.constant(num_test_df['disliked_genres'].tolist(), dtype=tf.int64),\n",
    "    'liked_authors': tf.constant(num_test_df['liked_authors'].tolist(), dtype=tf.int64),\n",
    "    'disliked_authors': tf.constant(num_test_df['disliked_authors'].tolist(), dtype=tf.int64),\n",
    "    'liked_ratings': tf.constant(num_test_df['liked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'disliked_ratings': tf.constant(num_test_df['disliked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'target_book': tf.constant(num_test_df['target_book'], dtype=tf.int64),\n",
    "    'authors': tf.constant(num_test_df['authors'], dtype=tf.int64),\n",
    "    'description': tf.constant(num_test_df['description'], dtype=tf.int64),\n",
    "    'categories': tf.constant(num_test_df['categories'], dtype=tf.int64),\n",
    "    'target_book_rating': tf.constant(num_test_df['target_book_rating'], dtype=tf.float32),\n",
    "})\n",
    "\n",
    "print(\"✅ Datasets successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aef2b6c5-c94f-4d83-8cc7-7fa57469676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': <tf.Tensor: shape=(), dtype=int64, numpy=3131>, 'liked_books': <tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
      "array([  423,  2903,   526, 30042, 21932,  9074,   209,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'disliked_books': <tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
      "array([29071, 34391,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'liked_genres': <tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
      "array([26, 60, 81, 58, 52,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0])>, 'disliked_genres': <tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
      "array([36, 53,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0])>, 'liked_authors': <tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
      "array([  366, 21297,  6445,   188, 15652,   155,  2022,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'disliked_authors': <tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
      "array([24398, 20615,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])>, 'liked_ratings': <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([6., 6., 6., 6., 6., 4., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>, 'disliked_ratings': <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([3., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.], dtype=float32)>, 'target_book': <tf.Tensor: shape=(), dtype=int64, numpy=937>, 'authors': <tf.Tensor: shape=(), dtype=int64, numpy=643>, 'description': <tf.Tensor: shape=(), dtype=int64, numpy=909>, 'categories': <tf.Tensor: shape=(), dtype=int64, numpy=40>, 'target_book_rating': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>}\n"
     ]
    }
   ],
   "source": [
    "for example in num_train_ds.take(1):\n",
    "\n",
    "    print(example)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da4ee82b-3eb1-470a-a128-1d113be7ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_ds_limited = num_train_ds#.take(500)  # Limit to 1000 samples\n",
    "num_test_ds_limited = num_test_ds#.take(500)  # Limit to 500 samples\n",
    "\n",
    "num_train_ds_cached = num_train_ds_limited.batch(128).cache()#.batch(128).cache()\n",
    "num_test_ds_cached = num_test_ds_limited.batch(128).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041accc-474c-4cb2-9aad-b630ac3ebfd2",
   "metadata": {},
   "source": [
    "## Build Twin-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9c491-3f1a-46c7-871f-76430344d639",
   "metadata": {},
   "source": [
    "Compared to the original version of our twin tower model, I want to simplify the input scheme to the user and book towers and incorporate some additional features for user embedding. Furthermore, we want to pass in sessionized user data separately from book data to each tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb4d81d4-a65f-474a-bf15-ed231618d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow.keras.layers as layers\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "class BooksTwoTowersModel(tfrs.Model):\n",
    "    def __init__(self, user_data: pd.DataFrame, book_metadata: pd.DataFrame, embedding_dimensions=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_projection_user = tf.keras.layers.Dense(embedding_dimensions, name='user_dense_projection')\n",
    "        \n",
    "        self.dense_projection_book = tf.keras.layers.Dense(embedding_dimensions, name='book_dense_projection')\n",
    "\n",
    "        self.user_model = UserModel(user_data, book_metadata, self.dense_projection_user, embedding_dimensions)\n",
    "\n",
    "        self.book_model = BookModel(book_metadata, embedding_dimensions, 10000, self.dense_projection_book)\n",
    "\n",
    "        self.candidate_ds = tf.data.Dataset.from_tensor_slices({\n",
    "            'title': tf.convert_to_tensor(book_metadata['title'].values, dtype=tf.int64),\n",
    "            'authors': tf.convert_to_tensor(book_metadata['authors'].values, dtype=tf.int64),\n",
    "            'description': tf.convert_to_tensor(book_metadata['description'].values, dtype=tf.int64),\n",
    "            'categories': tf.convert_to_tensor(book_metadata['categories'].values, dtype=tf.int64)\n",
    "        })\n",
    "\n",
    "        candidates = self.candidate_ds.batch(1).map(\n",
    "            lambda x: self.book_model(x), num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).map(\n",
    "            lambda x: tf.squeeze(x, axis=0)\n",
    "        )\n",
    "\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=candidates.batch(1),\n",
    "                ks=(10, 20, 50)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.full_book_embeddings = None\n",
    "        self.full_book_embeddings_copy = None\n",
    "\n",
    "    def compute_loss(self, features: Dict[str, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        user_embeddings = self.user_model(features)\n",
    "\n",
    "        target_book_embeddings = self.book_model(features)\n",
    "\n",
    "        retrieval_loss = self.task(user_embeddings, target_book_embeddings, compute_metrics=not training)\n",
    "\n",
    "        return retrieval_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd74ab6-94e8-41e9-bc52-694ce9a143d5",
   "metadata": {},
   "source": [
    "### Book and User model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f294221-fe13-44a3-9b48-7b271f8e24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(tf.keras.Model):\n",
    "    '''\n",
    "    The book(query) tower that processes book data.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, book_data: pd.DataFrame, embedding_dimensions: int, text_vectorization_max_tokens: int, dense_projection_book): #, book_title_weight_layer, book_author_weight_layer, book_genre_weight_layer):\n",
    "        '''\n",
    "        :param book_data: DataFrame containing book information.\n",
    "        :param embedding_dimensions: Number of dimensions in embedding layer.\n",
    "        :param text_vectorization_max_tokens: Maximum number of tokens to vector.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Extract unique values for embeddings\n",
    "        self.feature_book_title_name = \"title\"\n",
    "        self.feature_author_name = \"authors\"\n",
    "        self.feature_genre_name = \"categories\"\n",
    "\n",
    "        unique_titles = book_data[self.feature_book_title_name].astype(str).unique()\n",
    "        unique_authors = book_data[self.feature_author_name].astype(str).unique()\n",
    "        unique_genres = book_data[self.feature_genre_name ].astype(str).unique()\n",
    "\n",
    "        self.dense_projection_book = dense_projection_book\n",
    "        \n",
    "        # Book Title embedding\n",
    "        self.book_title_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_titles) + 1, output_dim=embedding_dimensions, name='book_title_embedding')\n",
    "\n",
    "        # Book Author embedding\n",
    "        self.book_author_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_authors) + 1, output_dim=embedding_dimensions, name='book_author_embedding')\n",
    "\n",
    "        # Book Genere embedding\n",
    "        self.book_genre_emdedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_genres) + 1, output_dim=embedding_dimensions, name='book_genre_embedding')\n",
    "    \n",
    "        # print(\"Finsihed setting up book tower\\n\")\n",
    "\n",
    "    def call(self, book_data: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \n",
    "        # Handle case where 'target_book' might not exist\n",
    "        try:\n",
    "            if len(book_data['target_book'].shape) == 0:\n",
    "                book_data['target_book'] = tf.expand_dims(book_data['target_book'], axis=0)\n",
    "            \n",
    "            book_title_embed = self.book_title_embedding_layers(book_data['target_book'])\n",
    "        except KeyError:\n",
    "            if len(book_data['title'].shape) == 0:\n",
    "                book_data['title'] = tf.expand_dims(book_data['title'], axis=0)\n",
    "                \n",
    "            book_title_embed = self.book_title_embedding_layers(book_data['title'])\n",
    "        \n",
    "        if len(book_data['authors'].shape) == 0:\n",
    "            book_data['authors'] = tf.expand_dims(book_data['authors'], axis=0)\n",
    "            book_data['categories'] = tf.expand_dims(book_data['categories'], axis=0)\n",
    "            \n",
    "        book_author_embed = self.book_author_embedding_layers(book_data['authors'])\n",
    "        book_genre_embed = self.book_genre_emdedding_layers(book_data['categories'])\n",
    "        \n",
    "        # Concatenation without expand_dims\n",
    "        concatenated_embeddings = tf.concat([\n",
    "            book_title_embed,\n",
    "            book_author_embed,\n",
    "            book_genre_embed\n",
    "            ], axis=-1)  # Use last axis for feature concat\n",
    "    \n",
    "        # Apply projection to 64D embedding\n",
    "        projected_embeddings = self.dense_projection_book(concatenated_embeddings)\n",
    "    \n",
    "        return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81053df9-b155-4c12-be1f-f9755fc87992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, user_data: pd.DataFrame, book_metadata: pd.DataFrame, dense_projection_user, embedding_dimensions=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Extract unique values from user and book metadata\n",
    "        unique_user_ids = user_data['user_id'].astype(str).unique().tolist()\n",
    "        unique_book_titles = book_metadata['title'].astype(str).unique().tolist()\n",
    "        unique_genres = book_metadata['categories'].astype(str).unique().tolist()\n",
    "        unique_authors = book_metadata['authors'].astype(str).unique().tolist()\n",
    "\n",
    "        self.dense_projection_user = dense_projection_user\n",
    "        \n",
    "        # User embedding\n",
    "        self.user_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_user_ids) + 1, output_dim=embedding_dimensions, name='user_id_embedding')\n",
    "\n",
    "        # Book embedding\n",
    "        self.book_title_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_book_titles) + 1, output_dim=embedding_dimensions, name='book_embedding')\n",
    "\n",
    "        # Genre embedding\n",
    "        self.genre_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_genres) + 1, output_dim=embedding_dimensions, name='genre_embedding')\n",
    "\n",
    "        # Author embedding\n",
    "        self.author_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_authors) + 1, output_dim=embedding_dimensions, name='author_embedding')\n",
    "        \n",
    "        # print(\"Finsihed setting up user tower\\n\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if len(inputs['user_id']) > 1:\n",
    "                # tf.print(\"Entered regular path\")\n",
    "                pass\n",
    "            else:\n",
    "                x = 1/0\n",
    "            \n",
    "        except:\n",
    "\n",
    "            temp = inputs\n",
    "            \n",
    "            try:\n",
    "                inputs = {\n",
    "                    'user_id': tf.expand_dims(temp[0][0], axis = 0),\n",
    "                    'liked_books': tf.expand_dims(temp[1], axis = 0),\n",
    "                    'disliked_books': tf.expand_dims(temp[2], axis = 0),\n",
    "                    'liked_genres': tf.expand_dims(temp[3], axis = 0),\n",
    "                    'disliked_genres': tf.expand_dims(temp[4], axis = 0),\n",
    "                    'liked_authors': tf.expand_dims(temp[5], axis = 0),\n",
    "                    'disliked_authors': tf.expand_dims(temp[6], axis = 0),\n",
    "                    'liked_ratings': tf.expand_dims(temp[7], axis = 0),\n",
    "                    'disliked_ratings': tf.expand_dims(temp[8], axis = 0)\n",
    "                }\n",
    "            except:\n",
    "                temp = [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
    "\n",
    "                inputs = {\n",
    "                    'user_id': tf.expand_dims(temp[0][0], axis = 0),\n",
    "                    'liked_books': tf.expand_dims(temp[1], axis = 0),\n",
    "                    'disliked_books': tf.expand_dims(temp[2], axis = 0),\n",
    "                    'liked_genres': tf.expand_dims(temp[3], axis = 0),\n",
    "                    'disliked_genres': tf.expand_dims(temp[4], axis = 0),\n",
    "                    'liked_authors': tf.expand_dims(temp[5], axis = 0),\n",
    "                    'disliked_authors': tf.expand_dims(temp[6], axis = 0),\n",
    "                    'liked_ratings': tf.expand_dims(temp[7], axis = 0),\n",
    "                    'disliked_ratings': tf.expand_dims(temp[8], axis = 0)\n",
    "                }\n",
    "        \n",
    "        user_embed = self.user_embedding_layers(inputs['user_id'])\n",
    "\n",
    "        def pool_embeddings(embedding_layer, input_list, weights, embedding_dim=64, pad_value=0):\n",
    "            # Get embeddings\n",
    "            embeddings = embedding_layer(input_list)\n",
    "        \n",
    "            # Create mask\n",
    "            mask = tf.not_equal(input_list, pad_value)\n",
    "            mask = tf.expand_dims(mask, axis=-1)\n",
    "        \n",
    "            # Zero out padded embeddings\n",
    "            embeddings = tf.where(mask, embeddings, tf.zeros_like(embeddings))\n",
    "        \n",
    "            # Normalize weights (zero-safe)\n",
    "            weight_sum = tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
    "            weight_sum = tf.where(weight_sum == 0, tf.ones_like(weight_sum), weight_sum)\n",
    "            weights = weights / weight_sum\n",
    "            \n",
    "            # Expand weights dims\n",
    "            expanded_weights = tf.expand_dims(weights, axis=-1)\n",
    "            \n",
    "            # Weighted Embeddings\n",
    "            weighted_embeddings = embeddings * expanded_weights\n",
    "        \n",
    "            # Sum + Pool\n",
    "            summed_embeddings = tf.reduce_sum(weighted_embeddings, axis=1)\n",
    "            valid_counts = tf.reduce_sum(tf.cast(mask, tf.float32), axis=1)\n",
    "            valid_counts = tf.where(valid_counts == 0, tf.ones_like(valid_counts), valid_counts)\n",
    "            pooled_embeddings = summed_embeddings / valid_counts\n",
    "        \n",
    "            # Fix NaNs\n",
    "            pooled_embeddings = tf.where(tf.math.is_nan(pooled_embeddings), tf.zeros_like(pooled_embeddings), pooled_embeddings)\n",
    "        \n",
    "            return pooled_embeddings\n",
    "\n",
    "\n",
    "        \n",
    "        # Process liked books\n",
    "        liked_books_embed = pool_embeddings(self.book_title_embedding_layers, inputs['liked_books'], inputs['liked_ratings'])\n",
    "\n",
    "        # Process disliked books\n",
    "        # disliked_books_embed = pool_embeddings(self.book_title_embedding_layers, inputs['disliked_books'], inputs['disliked_ratings'])\n",
    "\n",
    "        # Process liked genres\n",
    "        liked_genres_embed = pool_embeddings(self.genre_embedding_layers, inputs['liked_genres'], inputs['liked_ratings'])\n",
    "\n",
    "        # Process disliked genres\n",
    "        # disliked_genres_embed = pool_embeddings(self.genre_embedding_layers, inputs['disliked_genres'], inputs['disliked_ratings'])\n",
    "\n",
    "        # Process liked authors\n",
    "        liked_authors_embed = pool_embeddings(self.author_embedding_layers, inputs['liked_authors'], inputs['liked_ratings'])\n",
    "\n",
    "        # Process disliked authors\n",
    "        # disliked_authors_embed = pool_embeddings(self.author_embedding_layers, inputs['disliked_authors'], inputs['disliked_ratings'])\n",
    "\n",
    "        # Concatenate everything into a single user representation\n",
    "        try:\n",
    "            concatenated_embeddings = tf.concat([\n",
    "                user_embed,\n",
    "                liked_books_embed,\n",
    "                # disliked_books_embed,\n",
    "                liked_genres_embed,\n",
    "                # disliked_genres_embed,\n",
    "                liked_authors_embed,\n",
    "                # disliked_authors_embed\n",
    "            ], axis=1)\n",
    "        except:\n",
    "            return inputs\n",
    "\n",
    "        projected_embeddings = self.dense_projection_user(concatenated_embeddings)\n",
    "\n",
    "        # print(f\"projected_embeddings.shape: {projected_embeddings.shape}\\n\")\n",
    "        \n",
    "        return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7487d3b-99d7-4e72-8121-8950452ff390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of GPUs being used: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    }
   ],
   "source": [
    "# Compile Model\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of GPUs being used: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = BooksTwoTowersModel(\n",
    "        user_data=numerical_sessionized_df,\n",
    "        book_metadata=numerical_books_df,\n",
    "        embedding_dimensions=64, # Change this for embedding sizes to change (64 default val)\n",
    "    )\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b14609b5-066f-4aa5-a65b-e9e5cce76452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 00:39:52.879084: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/87 [..............................] - ETA: 19s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 621.1970 - regularization_loss: 0.0000e+00 - total_loss: 621.1970WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      " 2/87 [..............................] - ETA: 7s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 621.1319 - regularization_loss: 0.0000e+00 - total_loss: 621.1319 WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      " 3/87 [>.............................] - ETA: 6s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 621.1142 - regularization_loss: 0.0000e+00 - total_loss: 621.1142WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      " 4/87 [>.............................] - ETA: 5s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 621.1246 - regularization_loss: 0.0000e+00 - total_loss: 621.1246WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "87/87 [==============================] - 6s 65ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 612.8532 - regularization_loss: 0.0000e+00 - total_loss: 612.8532\n",
      "Epoch 2/7\n",
      " 1/87 [..............................] - ETA: 5s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 606.6939 - regularization_loss: 0.0000e+00 - total_loss: 606.6939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 00:39:58.705267: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 6s 64ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 431.7124 - regularization_loss: 0.0000e+00 - total_loss: 431.7124\n",
      "Epoch 3/7\n",
      " 1/87 [..............................] - ETA: 5s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 343.1006 - regularization_loss: 0.0000e+00 - total_loss: 343.1006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 00:40:04.309310: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 6s 64ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 325.1027 - regularization_loss: 0.0000e+00 - total_loss: 325.1027\n",
      "Epoch 4/7\n",
      "87/87 [==============================] - 6s 65ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 110.5035 - regularization_loss: 0.0000e+00 - total_loss: 110.5035\n",
      "Epoch 5/7\n",
      " 1/87 [..............................] - ETA: 5s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 28.5067 - regularization_loss: 0.0000e+00 - total_loss: 28.5067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 00:40:15.661676: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 6s 64ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 23.4362 - regularization_loss: 0.0000e+00 - total_loss: 23.4362\n",
      "Epoch 6/7\n",
      "87/87 [==============================] - 6s 65ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 6.2603 - regularization_loss: 0.0000e+00 - total_loss: 6.2603\n",
      "Epoch 7/7\n",
      " 2/87 [..............................] - ETA: 5s - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 2.0063 - regularization_loss: 0.0000e+00 - total_loss: 2.0063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 00:40:27.152387: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 6s 68ms/step - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_20_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - loss: 2.6035 - regularization_loss: 0.0000e+00 - total_loss: 2.6035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7fc1a2e8ca90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(num_train_ds_cached, epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00d063-ed13-4a26-9320-80f23fbd8103",
   "metadata": {},
   "source": [
    "See if you can get recommendations directly with filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2485ef7-3ba9-48c7-9f49-6a46aaf40a55",
   "metadata": {},
   "source": [
    "## Save model.user_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5855071-f0cc-4b38-9aea-8794e0694221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import sys\n",
    "\n",
    "# # Create a function to explicitly route tf.print to stdout\n",
    "# # def tf_print_to_stdout(msg):\n",
    "# #     tf.print(msg, output_stream=sys.stdout)\n",
    "\n",
    "# #tf_print_to_stdout(\"Saving model now...\")\n",
    "# model.user_model.save('model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae23f78-fe51-4148-9c6b-6db7ab988aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert model.user_model to .tar.gz type file. \n",
    "# Pack model and artifacts into tar.gz\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "#tarfile_name = \"model.tar.gz\"\n",
    "model.user_model.save(\"export/Servo/1\")\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fa03e-56c7-4fad-9c22-566516134bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "import boto3\n",
    "import subprocess\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86da6dc-ec2c-4621-a63c-615f9b76e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.tar.gz to required S3 bucket\n",
    "# s3://w210recsys/testModels/model.tar.gz\n",
    "bucket_name=\"w210recsys\"\n",
    "\n",
    "## Real Model\n",
    "# key_prefix=\"model/recModel\"\n",
    "\n",
    "# Test Model\n",
    "key_prefix = \"testModels\"\n",
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a69da-b177-4112-83f6-447a1901e8c5",
   "metadata": {},
   "source": [
    "### Model Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f053147-c2be-4bbf-8a1e-9b72f6145748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Fiction / Mystery & Detective', 'Young Adult Fiction / General',\n",
    "#        'Drama / General', 'Juvenile Fiction / Fantasy & Magic',\n",
    "#        'Juvenile Fiction / General', 'Fiction / World Literature',\n",
    "#        'Fiction / Romance', 'Political Science / General',\n",
    "#        'Fiction / Literary', 'Business & Economics / General',\n",
    "#        'Juvenile Fiction / Legends, Myths, Fables',\n",
    "#        'Juvenile Fiction / Science Fiction',\n",
    "#        'Fiction / Fairy Tales, Folk Tales, Legends & Mythology',\n",
    "#        'Fiction / Science Fiction', 'Fiction / Classics',\n",
    "#        'Religion / General', 'Fiction / General', 'Fiction / Ghost',\n",
    "#        'Fiction / Action & Adventure', 'Juvenile Nonfiction / General',\n",
    "#        'Juvenile Fiction / Fairy Tales & Folklore',\n",
    "#        'Fiction / War & Military', 'History / Maritime History & Piracy',\n",
    "#        'Juvenile Fiction / Thrillers & Suspense',\n",
    "#        'Comics & Graphic Novels / Superheroes',\n",
    "#        'Literary Criticism / General', 'Science / General',\n",
    "#        'Reference / General', 'History / General',\n",
    "#        'Fiction / Occult & Supernatural', 'Philosophy / General',\n",
    "#        'Computers / General',\n",
    "#        'Biography & Autobiography / Personal Memoirs', 'Art / General',\n",
    "#        'Fiction / Visionary & Metaphysical',\n",
    "#        'Family & Relationships / General', 'Fiction / Thrillers',\n",
    "#        'Health & Fitness / General', 'Fiction / Anthologies',\n",
    "#        'Biography & Autobiography / General', 'Fiction / Sea Stories',\n",
    "#        'Fiction / Erotica', 'Fiction / Sagas',\n",
    "#        'Fiction / Magical Realism', 'Fiction / Biographical',\n",
    "#        'History / Expeditions & Discoveries', 'Education / General',\n",
    "#        'Juvenile Fiction / Nursery Rhymes', 'Humor / Topic',\n",
    "#        'Nature / General', 'True Crime / Murder', 'Psychology / General',\n",
    "#        'Social Science / General', 'Photography / General',\n",
    "#        'Religion / Theology', 'Fiction / Dystopian',\n",
    "#        'History / Wars & Conflicts', 'Body, Mind & Spirit / General',\n",
    "#        'Fiction / Short Stories', 'History / Social History',\n",
    "#        'Games & Activities / General', 'Fiction / Family Life',\n",
    "#        'Comics & Graphic Novels / General', 'Fiction / City Life',\n",
    "#        'Biography & Autobiography / Literary Figures',\n",
    "#        'Juvenile Fiction / Short Stories', 'Fiction / Crime',\n",
    "#        'Travel / Essays & Travelogues',\n",
    "#        'Technology & Engineering / General', 'Drama / Shakespeare',\n",
    "#        'History / Historiography', 'Bibles / General',\n",
    "#        'History / Indigenous Peoples of the Americas',\n",
    "#        'Cooking / Individual Chefs & Restaurants',\n",
    "#        'Performing Arts / General', 'Fiction / Noir', 'Poetry / General',\n",
    "#        'History / Military', 'Cooking / General', 'Travel / General',\n",
    "#        'Music / General', 'Sports & Recreation / General',\n",
    "#        'True Crime / General', 'Religion / Christian Theology',\n",
    "#        'Language Arts & Disciplines / General',\n",
    "#        'Crafts & Hobbies / General', 'Pets / General',\n",
    "#        'Young Adult Nonfiction / General', 'House & Home / General',\n",
    "#        'Literary Collections / General', 'Humor / General',\n",
    "#        'Antiques & Collectibles / General', 'Study Aids / General',\n",
    "#        'Foreign Language Study / General', 'Medical / General',\n",
    "#        'Law / General', 'Mathematics / General',\n",
    "#        'History / Historical Geography', 'Architecture / General',\n",
    "#        'Transportation / General', 'Gardening / General',\n",
    "#        'Design / General']\n",
    "\n",
    "# Profile one\n",
    "liked_genres = ['Mathematics / General', 'Computers / General']\n",
    "disliked_genres = ['History / Military']\n",
    "\n",
    "# Profile two\n",
    "# liked_genres = ['Religion / General' [ 'Medical / General']\n",
    "# disliked_genres = [] # base has some 'Family & Relationships / General' books\n",
    "# disliked_genres = ['Family & Relationships / General'] # Commenting this in actually gave more of these as recs????\n",
    "\n",
    "# Profile three\n",
    "# liked_genres = ['Law / General', 'True Crime / General']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile four\n",
    "# liked_genres = ['Performing Arts / General', 'Humor / Topic']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile five (Popular)\n",
    "# liked_genres = ['Religion / General'] # ['Fiction / World Literature', \n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile five (Un-Popular)\n",
    "# liked_genres = ['Study Aids / General', 'Young Adult Nonfiction / General']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile six (Un-Popular)\n",
    "# liked_genres = ['Nature / General', ' Health & Fitness / General']\n",
    "# disliked_genres = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1755a8de-6a01-4c0a-ad1b-7b5c108ddc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publish_year</th>\n",
       "      <th>description</th>\n",
       "      <th>preview_link</th>\n",
       "      <th>normalized_popularity</th>\n",
       "      <th>genre_general</th>\n",
       "      <th>genre_specific</th>\n",
       "      <th>genre_combined</th>\n",
       "      <th>genre_consolidated</th>\n",
       "      <th>review_helpfulness</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14568</th>\n",
       "      <td>A1RAFRNWOWMC3D</td>\n",
       "      <td>83</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Jean Andrews</td>\n",
       "      <td>2013</td>\n",
       "      <td>This step-by-step, highly visual text provides...</td>\n",
       "      <td>http://books.google.nl/books?id=bUILAAAAQBAJ&amp;p...</td>\n",
       "      <td>0.787122</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>Computers / Hardware</td>\n",
       "      <td>Computers / General</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-10-28</td>\n",
       "      <td>The Power of Love</td>\n",
       "      <td>The Notebook is the love story of Noah and All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274404</th>\n",
       "      <td>A13586UCKKITJZ</td>\n",
       "      <td>35697</td>\n",
       "      <td>Learning SQL: A Step-By-Step Guide Using Oracle</td>\n",
       "      <td>Richard W. Earp</td>\n",
       "      <td>2002</td>\n",
       "      <td>Step-by-step examples and exercises help reade...</td>\n",
       "      <td>http://books.google.nl/books?id=706LQAAACAAJ&amp;d...</td>\n",
       "      <td>0.157150</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Database Administration &amp; Management</td>\n",
       "      <td>Computers / Database Administration &amp; Management</td>\n",
       "      <td>Computers / General</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-07-02</td>\n",
       "      <td>A Wonderful Book That Clearly Explains Unclear...</td>\n",
       "      <td>Despite the pile Oracle Certified Professional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186922</th>\n",
       "      <td>A1V0MM7VDPMPU8</td>\n",
       "      <td>9039</td>\n",
       "      <td>Introductory Real Analysis (Dover Books on Mat...</td>\n",
       "      <td>A. N. Kolmogorov</td>\n",
       "      <td>1975</td>\n",
       "      <td>Comprehensive, elementary introduction to real...</td>\n",
       "      <td>http://books.google.com/books?id=U_FIAwAAQBAJ&amp;...</td>\n",
       "      <td>0.331050</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Functional Analysis</td>\n",
       "      <td>Mathematics / Functional Analysis</td>\n",
       "      <td>Mathematics / General</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>DON'T buy this book!</td>\n",
       "      <td>This is not the original work. Just like Emily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141138</th>\n",
       "      <td>A1PRZY9391ES5F</td>\n",
       "      <td>3985</td>\n",
       "      <td>Learning Java (The Java Series)</td>\n",
       "      <td>Jamie Chan</td>\n",
       "      <td>2016</td>\n",
       "      <td>(2018 Edition, Updated for Netbeans 9.0) Learn...</td>\n",
       "      <td>http://books.google.nl/books?id=GohfvwEACAAJ&amp;d...</td>\n",
       "      <td>0.424547</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Programming</td>\n",
       "      <td>Computers / Programming</td>\n",
       "      <td>Computers / General</td>\n",
       "      <td>79</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2002-10-19</td>\n",
       "      <td>Not a tutorial and not for new programmers</td>\n",
       "      <td>I had purchased \"Learning Java\" out of the con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207113</th>\n",
       "      <td>A2K0JZN74WFCYO</td>\n",
       "      <td>12575</td>\n",
       "      <td>SQL Server 2005 T-SQL Recipes: A Problem-Solut...</td>\n",
       "      <td>Joseph Sack</td>\n",
       "      <td>2006</td>\n",
       "      <td>* Comprehensive T-SQL Coverage, including all ...</td>\n",
       "      <td>http://books.google.com/books?id=5_AEiJXbyiEC&amp;...</td>\n",
       "      <td>0.294963</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Database Administration &amp; Management</td>\n",
       "      <td>Computers / Database Administration &amp; Management</td>\n",
       "      <td>Computers / General</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2006-05-05</td>\n",
       "      <td>No BS type of book</td>\n",
       "      <td>This book is a very good book for updating ski...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user_id  book_id  \\\n",
       "14568   A1RAFRNWOWMC3D       83   \n",
       "274404  A13586UCKKITJZ    35697   \n",
       "186922  A1V0MM7VDPMPU8     9039   \n",
       "141138  A1PRZY9391ES5F     3985   \n",
       "207113  A2K0JZN74WFCYO    12575   \n",
       "\n",
       "                                                    title            author  \\\n",
       "14568                                        The Notebook      Jean Andrews   \n",
       "274404    Learning SQL: A Step-By-Step Guide Using Oracle   Richard W. Earp   \n",
       "186922  Introductory Real Analysis (Dover Books on Mat...  A. N. Kolmogorov   \n",
       "141138                    Learning Java (The Java Series)        Jamie Chan   \n",
       "207113  SQL Server 2005 T-SQL Recipes: A Problem-Solut...       Joseph Sack   \n",
       "\n",
       "        publish_year                                        description  \\\n",
       "14568           2013  This step-by-step, highly visual text provides...   \n",
       "274404          2002  Step-by-step examples and exercises help reade...   \n",
       "186922          1975  Comprehensive, elementary introduction to real...   \n",
       "141138          2016  (2018 Edition, Updated for Netbeans 9.0) Learn...   \n",
       "207113          2006  * Comprehensive T-SQL Coverage, including all ...   \n",
       "\n",
       "                                             preview_link  \\\n",
       "14568   http://books.google.nl/books?id=bUILAAAAQBAJ&p...   \n",
       "274404  http://books.google.nl/books?id=706LQAAACAAJ&d...   \n",
       "186922  http://books.google.com/books?id=U_FIAwAAQBAJ&...   \n",
       "141138  http://books.google.nl/books?id=GohfvwEACAAJ&d...   \n",
       "207113  http://books.google.com/books?id=5_AEiJXbyiEC&...   \n",
       "\n",
       "        normalized_popularity genre_general  \\\n",
       "14568                0.787122     Computers   \n",
       "274404               0.157150     Computers   \n",
       "186922               0.331050   Mathematics   \n",
       "141138               0.424547     Computers   \n",
       "207113               0.294963     Computers   \n",
       "\n",
       "                              genre_specific  \\\n",
       "14568                               Hardware   \n",
       "274404  Database Administration & Management   \n",
       "186922                   Functional Analysis   \n",
       "141138                           Programming   \n",
       "207113  Database Administration & Management   \n",
       "\n",
       "                                          genre_combined  \\\n",
       "14568                               Computers / Hardware   \n",
       "274404  Computers / Database Administration & Management   \n",
       "186922                 Mathematics / Functional Analysis   \n",
       "141138                           Computers / Programming   \n",
       "207113  Computers / Database Administration & Management   \n",
       "\n",
       "           genre_consolidated review_helpfulness  review_score review_time  \\\n",
       "14568     Computers / General                  0           5.0  2002-10-28   \n",
       "274404    Computers / General                 12           5.0  2002-07-02   \n",
       "186922  Mathematics / General                  8           1.0  2008-07-09   \n",
       "141138    Computers / General                 79           2.0  2002-10-19   \n",
       "207113    Computers / General                  5           5.0  2006-05-05   \n",
       "\n",
       "                                           review_summary  \\\n",
       "14568                                   The Power of Love   \n",
       "274404  A Wonderful Book That Clearly Explains Unclear...   \n",
       "186922                               DON'T buy this book!   \n",
       "141138         Not a tutorial and not for new programmers   \n",
       "207113                                 No BS type of book   \n",
       "\n",
       "                                              review_text  \n",
       "14568   The Notebook is the love story of Noah and All...  \n",
       "274404  Despite the pile Oracle Certified Professional...  \n",
       "186922  This is not the original work. Just like Emily...  \n",
       "141138  I had purchased \"Learning Java\" out of the con...  \n",
       "207113  This book is a very good book for updating ski...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_books = books_df[books_df['genre_consolidated'].isin(liked_genres)].sample(5)\n",
    "\n",
    "sampled_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c175ab86-f142-4bba-8b03-03752b7821a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each book\n",
    "\n",
    "sampled_encoded_liked_title = book_title_vocab_layer(sampled_books['title']).numpy()\n",
    "sampled_encoded_liked_authors = book_authors_vocab_layer(sampled_books['author']).numpy()\n",
    "sampled_encoded_liked_genres = book_genre_vocab_layer(sampled_books['genre_consolidated']).numpy()\n",
    "\n",
    "sampled_encoded_disliked_title = np.array([], dtype='int64')\n",
    "sampled_encoded_disliked_authors = np.array([], dtype='int64')\n",
    "sampled_encoded_disliked_genres = np.array([], dtype='int64')\n",
    "\n",
    "sampled_liked_rating = [6 for title in sampled_encoded_liked_title]\n",
    "\n",
    "encoded_disliked_genres = []\n",
    "\n",
    "for genre in disliked_genres:\n",
    "    encoded_disliked_genres.append(book_genre_vocab_layer(genre).numpy())\n",
    "    \n",
    "disliked_ratings = [1 for title in encoded_disliked_genres]\n",
    "\n",
    "# sampled_encoded_liked_title, sampled_encoded_liked_authors, sampled_encoded_liked_genres, sampled_liked_rating, encoded_disliked_genres, disliked_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e05713d-802e-42c1-a10b-5b05a5842a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   84, 35698,  9040,  3986, 12576])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_encoded_liked_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b82cfb84-769e-4cc1-a618-39590f8d0fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_encoded_disliked_title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "be7b2e94-f97d-4445-b333-2e52c8ffa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_info = ({\n",
    "    'user_id': [0], # Doesn't matter\n",
    "    'liked_books': sampled_encoded_liked_title.tolist(),\n",
    "    'disliked_books': [],\n",
    "\n",
    "    'liked_genres': sampled_encoded_liked_genres.tolist(),\n",
    "    'disliked_genres': encoded_disliked_genres,\n",
    "    \n",
    "    'liked_authors': sampled_encoded_liked_authors.tolist(),\n",
    "    'disliked_authors':[],\n",
    "    \n",
    "    'liked_ratings': sampled_liked_rating,\n",
    "    'disliked_ratings': disliked_ratings,\n",
    "})\n",
    "\n",
    "sample_user = []\n",
    "\n",
    "for col in sample_user_info:\n",
    "\n",
    "    # print(col, sample_user_info[col])\n",
    "    \n",
    "    sample_user_info[col].extend([0]*(20 - len(sample_user_info[col])))\n",
    "\n",
    "    # print(sample_user_info[col])\n",
    "    \n",
    "    sample_user.append(sample_user_info[col])\n",
    "\n",
    "sample_user = tf.cast(sample_user, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c703486c-72df-45bd-b982-88be199ad8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "2025-04-14 01:15:32.503136: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2025-04-14 01:15:32.522531: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n"
     ]
    }
   ],
   "source": [
    "# model.user_model.predict([[i for i in range(20)], [i for i in range(20)], [i for i in range(20)]])\n",
    "\n",
    "user_embedding = model.user_model.predict(sample_user)\n",
    "\n",
    "# user_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44aca1-c7fe-4f05-b434-2f2e51e3d88f",
   "metadata": {},
   "source": [
    "Extract the books' embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ceefb14-d12c-4e9d-bb23-a13bf10ebf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Embeddings Shape: (71696, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255/2440940779.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71696, 18)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_tower = model.book_model\n",
    "\n",
    "book_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'target_book': tf.constant(numerical_books_df['title'].tolist(), dtype=tf.int64),\n",
    "    'authors': tf.constant(numerical_books_df['authors'].tolist(), dtype=tf.int64),\n",
    "    'categories': tf.constant(numerical_books_df['categories'].tolist(), dtype=tf.int64),\n",
    "    'description': tf.constant(numerical_books_df['description'].tolist(), dtype=tf.int64),\n",
    "}).batch(128)  # Optional batching\n",
    "\n",
    "book_embeddings = []\n",
    "\n",
    "for batch in book_dataset:\n",
    "    batch_embeddings = book_tower(batch)\n",
    "    book_embeddings.append(batch_embeddings)\n",
    "\n",
    "book_embeddings = tf.concat(book_embeddings, axis=0)\n",
    "print(\"Book Embeddings Shape:\", book_embeddings.shape)\n",
    "\n",
    "books_df_unique['Embeddings'] = [embed.numpy() for embed in book_embeddings]\n",
    "\n",
    "books_df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb0a5817-ac30-49c9-b067-8306b324021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'book_id', 'title', 'author', 'publish_year', 'description',\n",
       "       'preview_link', 'normalized_popularity', 'genre_general',\n",
       "       'genre_specific', 'genre_combined', 'genre_consolidated',\n",
       "       'review_helpfulness', 'review_score', 'review_time', 'review_summary',\n",
       "       'review_text', 'Embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_unique.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b865893-c577-41fa-aebf-2c0c8e7a1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murach's ASP.NET 2.0 Web Programming with VB 2005 | Computers / General\n",
      "CFS traced to childhood trauma, emotional instability, stress.(Across Specialties)(chronic fatigue syndrome): An article from: Clinical Psychiatry News | Medical / General\n",
      "Java Design Patterns: A Tutorial | Computers / General\n",
      "Enterprise Messaging Using JMS and IBM WebSphere | Computers / General\n",
      "Blues Harmonica Collection | Music / General\n",
      "GMAT for Dummies | Study Aids / General\n",
      "Huntington Beach (CA) (Images of America) | History / Expeditions & Discoveries\n",
      "Photoshop for Right-Brainers: The Art of Photo Manipulation | Computers / General\n",
      "Sams Teach Yourself CSS in 24 Hours (Sams Teach Yourself...in 24 Hours) | Computers / General\n",
      "The Invisible Computer: Why Good Products Can Fail, the Personal Computer Is So Complex, and Information Appliances Are the Solution | Computers / General\n",
      "Little Digital Video Book | Computers / General\n",
      "What Just Happened: A Chronicle from the Information Frontier | Computers / General\n",
      "Student Study Guide to Accompany Physics 6th Edition | Science / General\n",
      "Welding Fabrication and Repair: Questions & Answers | Technology & Engineering / General\n",
      "The forest ranger: A study in administrative behavior | Political Science / General\n",
      "Applied Microsoft&reg; .NET Framework Programming in Microsoft&reg; Visual Basic&reg; .NET | Computers / General\n",
      "Battle for the Hague 1940: The First Great Airborne Operation in History | History / Wars & Conflicts\n",
      "Handbook of Mathematical Functions | Mathematics / General\n",
      "As Jesus Cared for Women: Restoring Women Then and Now | History / General\n",
      "Taming the Nueces Strip: The Story of McNelly's Rangers | History / Maritime History & Piracy\n",
      "The Story of Jazz | Music / General\n",
      "Pthreads Programming: A POSIX Standard for Better Multiprocessing (O'Reilly Nutshell) | Computers / General\n",
      "802.11 Wireless Networks: The Definitive Guide (O'Reilly Networking) | Technology & Engineering / General\n",
      "Design for the Real World: Human Ecology and Social Change | Design / General\n",
      "Macromedia Dreamweaver MX Dynamic Applications: Advanced Training from the Source | Computers / General\n",
      "Pasta: Every Way for Every Day | Cooking / Individual Chefs & Restaurants\n",
      "Faster | Biography & Autobiography / Personal Memoirs\n",
      "Home Comforts : The Art and Science of Keeping House | House & Home / General\n",
      "Zen Mind, Beginner's Mind | Religion / General\n",
      "Abide in My Word - 2007: Mass Readings at Your Fingertips | Religion / General\n",
      "And You Invited Me In: A Story of Redemption | Family & Relationships / General\n",
      "The Postmodern Bible | Religion / General\n",
      "St. Augustine Confessions (Oxford World's Classics) | Literary Collections / General\n",
      "Divided by God: America's Church-State Problem--and What We Should Do About It | Political Science / General\n",
      "Surface Science: An Introduction (Advanced Texts in Physics) | Science / General\n",
      "The Official Guide for GMAT Review, 11th Edition | Study Aids / General\n",
      "Choral Arranging | Music / General\n",
      "Randomized Algorithms | Computers / General\n",
      "MCTS Self-Paced Training Kit (Exam 70-526): Microsoft .NET Framework 2.0 Windows-Based Client Development | Computers / General\n",
      "Gruber's Complete Preparation for the New Sat (Gruber's Complete SAT Guide) | Study Aids / General\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "filtered_books_df_unique = books_df_unique\n",
    "\n",
    "book_embeddings = np.array(filtered_books_df_unique['Embeddings'].tolist())\n",
    "\n",
    "# Normalize user embedding to unit vector (L2 normalization)\n",
    "# user_embedding /= np.linalg.norm(user_embedding)\n",
    "\n",
    "# Normalize all book embeddings to unit vectors (L2 normalization)\n",
    "# book_embeddings /= np.linalg.norm(book_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Calculate the cosine similarity between user_embedding and all book embeddings\n",
    "cos_similarities = cosine_similarity(user_embedding.reshape(1, -1), book_embeddings)\n",
    "\n",
    "# Get the indices of the top 6 closest books\n",
    "top_k_indices = np.argsort(cos_similarities[0])[-40:][::-1]  # Top 6 indices with highest similarity\n",
    "\n",
    "# Print the results (book indices and cosine similarity scores)\n",
    "for i, idx in enumerate(top_k_indices):\n",
    "\n",
    "    print(f\"{filtered_books_df_unique.iloc[idx]['title']} | {filtered_books_df_unique.iloc[idx]['genre_consolidated']}\")\n",
    "    # print(f\"Recommendation {i+1}: Book Index {idx} (Cosine Similarity: {cos_similarities[0][idx]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fc091-8723-4a8c-a2ef-68f1e5dfc609",
   "metadata": {},
   "source": [
    "Mimic subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e167112a-072c-4bf3-a20a-2db2047df630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map recs to each category\n",
    "\n",
    "# Murach's ASP.NET 2.0 Web Programming with VB 2005 | Computers / General\n",
    "# CFS traced to childhood trauma, emotional instability, stress.(Across Specialties)(chronic fatigue syndrome): An article from: Clinical Psychiatry News | Medical / General\n",
    "# Enterprise Messaging Using JMS and IBM WebSphere | Computers / General\n",
    "# GMAT for Dummies | Study Aids / General\n",
    "# Java Design Patterns: A Tutorial | Computers / General\n",
    "# Huntington Beach (CA) (Images of America) | History / Expeditions & Discoveries\n",
    "\n",
    "sub_liked_books = [\n",
    "    \"Enterprise Messaging Using JMS and IBM WebSphere\",\n",
    "    \"Murach's ASP.NET 2.0 Web Programming with VB 2005\"\n",
    "]\n",
    "sub_disliked_books = [\n",
    "    \"CFS traced to childhood trauma, emotional instability, stress.(Across Specialties)(chronic fatigue syndrome): An article from: Clinical Psychiatry News\"\n",
    "]\n",
    "\n",
    "# Next, extract the encoded metadata of each book\n",
    "\n",
    "sub_book_titles = book_title_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['title']).numpy()\n",
    "sampled_encoded_liked_title = np.concatenate([sampled_encoded_liked_title, sub_book_titles])\n",
    "\n",
    "sub_book_authors = book_authors_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['author']).numpy()\n",
    "sampled_encoded_liked_authors = np.concatenate([sampled_encoded_liked_authors, sub_book_authors])\n",
    "\n",
    "sub_book_genres = book_genre_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['genre_consolidated']).numpy()\n",
    "sampled_encoded_liked_genres = np.concatenate([sampled_encoded_liked_genres, sub_book_genres])\n",
    "\n",
    "#####\n",
    "\n",
    "sub_book_titles_dis = book_title_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_disliked_books)]['title']).numpy()\n",
    "sampled_encoded_disliked_title = np.concatenate([sampled_encoded_disliked_title, sub_book_titles_dis])\n",
    "\n",
    "sub_book_authors_dis = book_authors_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_disliked_books)]['author']).numpy()\n",
    "sampled_encoded_disliked_authors = np.concatenate([sampled_encoded_disliked_authors, sub_book_authors_dis])\n",
    "\n",
    "sub_book_genres_dis = book_genre_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_disliked_books)]['genre_consolidated']).numpy()\n",
    "sampled_encoded_disliked_genres = np.concatenate([sampled_encoded_disliked_genres, sub_book_genres_dis])\n",
    "\n",
    "####\n",
    "\n",
    "# sampled_encoded_disliked_title.extend(book_title_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['title']).numpy()\n",
    "# sampled_encoded_disliked_authors.extend(book_authors_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['author']).numpy()\n",
    "# sampled_encoded_disliked_genres.extend(book_genre_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['genre_consolidated']).numpy()\n",
    "\n",
    "sampled_liked_rating = [6 for title in sampled_encoded_liked_title]\n",
    "    \n",
    "sampled_disliked_ratings = [1 for title in sampled_encoded_disliked_title]\n",
    "\n",
    "# sampled_encoded_liked_title, sampled_encoded_liked_authors, sampled_encoded_liked_genres, sampled_encoded_disliked_title,sampled_encoded_disliked_authors, sampled_encoded_disliked_genres, sampled_liked_rating, sampled_disliked_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "63a1dc02-51f7-4ca8-a595-ebdea926ab30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   84, 35698,  9040,  3986, 12576,  5113, 25534])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_encoded_liked_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f44867e3-da55-4685-9f36-6567f29288d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([62629])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_encoded_disliked_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "edb4b876-9503-485c-a80a-c671dd5083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_info = ({\n",
    "    'user_id': [0], # Doesn't matter\n",
    "    'liked_books': sampled_encoded_liked_title.tolist(),\n",
    "    'disliked_books': sampled_encoded_disliked_title.tolist(),\n",
    "\n",
    "    'liked_genres': sampled_encoded_liked_genres.tolist(),\n",
    "    'disliked_genres': sampled_encoded_disliked_genres.tolist(),\n",
    "    \n",
    "    'liked_authors': sampled_encoded_liked_authors.tolist(),\n",
    "    'disliked_authors': sampled_encoded_disliked_authors.tolist(),\n",
    "    \n",
    "    'liked_ratings': sampled_liked_rating,\n",
    "    'disliked_ratings': sampled_disliked_ratings\n",
    "})\n",
    "\n",
    "sample_user = []\n",
    "\n",
    "for col in sample_user_info:\n",
    "\n",
    "    # print(col, sample_user_info[col])\n",
    "    \n",
    "    sample_user_info[col].extend([0]*(20 - len(sample_user_info[col])))\n",
    "\n",
    "    # print(sample_user_info[col])\n",
    "    \n",
    "    sample_user.append(sample_user_info[col])\n",
    "\n",
    "sample_user = tf.cast(sample_user, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "754ddbb8-4eb5-45a9-8ece-a9263b6a6eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 76ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "2025-04-14 01:17:13.748405: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "# model.user_model.predict([[i for i in range(20)], [i for i in range(20)], [i for i in range(20)]])\n",
    "\n",
    "user_embedding = model.user_model.predict(sample_user)\n",
    "\n",
    "# user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d343cbeb-ecdb-4e61-b84d-1953690ee301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((71696, 18), (71693, 18))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_unique.shape, filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6328a853-bea8-4018-86af-27b9f89b10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMAT for Dummies | Study Aids / General\n",
      "CFS traced to childhood trauma, emotional instability, stress.(Across Specialties)(chronic fatigue syndrome): An article from: Clinical Psychiatry News | Medical / General\n",
      "Murach's ASP.NET 2.0 Web Programming with VB 2005 | Computers / General\n",
      "Huntington Beach (CA) (Images of America) | History / Expeditions & Discoveries\n",
      "Blues Harmonica Collection | Music / General\n",
      "Enterprise Messaging Using JMS and IBM WebSphere | Computers / General\n",
      "Java Design Patterns: A Tutorial | Computers / General\n",
      "Welding Fabrication and Repair: Questions & Answers | Technology & Engineering / General\n",
      "802.11 Wireless Networks: The Definitive Guide (O'Reilly Networking) | Technology & Engineering / General\n",
      "The forest ranger: A study in administrative behavior | Political Science / General\n",
      "Photoshop for Right-Brainers: The Art of Photo Manipulation | Computers / General\n",
      "Pasta: Every Way for Every Day | Cooking / Individual Chefs & Restaurants\n",
      "Battle for the Hague 1940: The First Great Airborne Operation in History | History / Wars & Conflicts\n",
      "As Jesus Cared for Women: Restoring Women Then and Now | History / General\n",
      "The Catholic Study Bible: New American Bible | Bibles / General\n",
      "Gruber's Complete Preparation for the New Sat (Gruber's Complete SAT Guide) | Study Aids / General\n",
      "Design for the Real World: Human Ecology and Social Change | Design / General\n",
      "Applied Microsoft&reg; .NET Framework Programming in Microsoft&reg; Visual Basic&reg; .NET | Computers / General\n",
      "Sams Teach Yourself CSS in 24 Hours (Sams Teach Yourself...in 24 Hours) | Computers / General\n",
      "Student Study Guide to Accompany Physics 6th Edition | Science / General\n"
     ]
    }
   ],
   "source": [
    "excluded_books = sub_liked_books\n",
    "excluded_books.extend(sub_disliked_books)\n",
    "\n",
    "# Assuming your DataFrame is called df and the column containing books is 'book_id'\n",
    "filtered_df = books_df_unique[~books_df_unique['title'].isin(excluded_books)]\n",
    "\n",
    "book_embeddings = np.array(filtered_books_df_unique['Embeddings'].tolist()) \n",
    "\n",
    "# Normalize user embedding to unit vector (L2 normalization)\n",
    "# user_embedding /= np.linalg.norm(user_embedding)\n",
    "\n",
    "# Normalize all book embeddings to unit vectors (L2 normalization)\n",
    "# book_embeddings /= np.linalg.norm(book_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Calculate the cosine similarity between user_embedding and all book embeddings\n",
    "cos_similarities = cosine_similarity(user_embedding.reshape(1, -1), book_embeddings)\n",
    "\n",
    "# Get the indices of the top 6 closest books\n",
    "top_k_indices = np.argsort(cos_similarities[0])[-20:][::-1]  # Top 6 indices with highest similarity\n",
    "\n",
    "# Print the results (book indices and cosine similarity scores)\n",
    "for i, idx in enumerate(top_k_indices):\n",
    "\n",
    "    print(f\"{filtered_books_df_unique.iloc[idx]['title']} | {filtered_books_df_unique.iloc[idx]['genre_consolidated']}\")\n",
    "    # print(f\"Recommendation {i+1}: Book Index {idx} (Cosine Similarity: {cos_similarities[0][idx]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b376c-9ed7-4107-ba15-43b39128afbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7bf89-8cf0-4b73-bc15-c29d7043691a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e26255d6-ca62-4c33-8556-75b23055c7f9",
   "metadata": {},
   "source": [
    "### Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573f9b4-af4f-45fe-961e-999e6d520245",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tower = model.book_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b5101-b275-4283-98fb-16b71edc122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'target_book': tf.constant(numerical_books_df['title'].tolist(), dtype=tf.int64),\n",
    "    'authors': tf.constant(numerical_books_df['authors'].tolist(), dtype=tf.int64),\n",
    "    'categories': tf.constant(numerical_books_df['categories'].tolist(), dtype=tf.int64),\n",
    "    'description': tf.constant(numerical_books_df['description'].tolist(), dtype=tf.int64),\n",
    "}).batch(128)  # Optional batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc4334-bcc9-41ce-b1d7-5072414bd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_embeddings = []\n",
    "\n",
    "for batch in book_dataset:\n",
    "    batch_embeddings = book_tower(batch)\n",
    "    book_embeddings.append(batch_embeddings)\n",
    "\n",
    "book_embeddings = tf.concat(book_embeddings, axis=0)\n",
    "print(\"Book Embeddings Shape:\", book_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef9be4-fc3c-4b6f-94d5-fa99bf73963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.distinct('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9302e-f7a4-4d4a-9e6a-689b22d24cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"book_embeddings.npy\", book_embeddings.numpy())\n",
    "books_df.to_csv('books_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4dcfbb-c5ea-4fa1-bcf4-844e19e5c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.tar.gz to required S3 bucket\n",
    "#s3://w210recsys/model/recModel/modelFiles/\n",
    "bucket_name=\"w210recsys\"\n",
    "key_prefix=\"model/recModel/modelFiles\"\n",
    "s3_response = sm_session.upload_data(\"book_embeddings.npy\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cd476-2b9a-4fe8-8863-f78c65d6e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.tar.gz to required S3 bucket\n",
    "#s3://w210recsys/model/recModel/modelFiles/\n",
    "bucket_name=\"w210recsys\"\n",
    "key_prefix=\"model/recModel/modelFiles\"\n",
    "s3_response = sm_session.upload_data(\"books_df.csv\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da6a9b-3932-4901-add1-ab87d0c1d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = tarfile.open('model.tar.gz')\n",
    "\n",
    "file.extractall('./extractTar')\n",
    "\n",
    "file.close()\n",
    "\n",
    "# def list_tar_gz_contents(file_path):\n",
    "#     try:\n",
    "#         with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "#             for member in tar.getmembers():\n",
    "#                 print(member.name)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306302d-f2c9-48af-9c0d-5776e9573dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_book_embeddings(self, path, books_df):\n",
    "\n",
    "#     # We want to load in books' embeddings to make sure our model has them on hand to give direct recommendations\n",
    "#     # Load in via boto3 and sagemaker\n",
    "\n",
    "#     role = sagemaker.get_execution_role()\n",
    "#     sm_session = sagemaker.Session()\n",
    "#     bucket_name = sm_session.default_bucket()\n",
    "#     s3 = boto3.client('s3')\n",
    "\n",
    "#     # Download the file from S3 into memory\n",
    "#     response = s3.get_object(Bucket=bucket_name, Key=path)\n",
    "\n",
    "#     # Read the data into a BytesIO buffer\n",
    "#     buffer = io.BytesIO(response['Body'].read())\n",
    "\n",
    "#     # Load numpy array from buffer\n",
    "#     books_df['embeddings'] = [embed for embed in np.load(buffer)]\n",
    "\n",
    "#     self.full_book_embeddings = books_df\n",
    "    \n",
    "#     print(self.full_book_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2dfe8-e336-40e1-9513-32531d9b6898",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69205f3-ddd5-441f-8e28-6ca195eb7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tower = model.user_model\n",
    "book_tower = model.book_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a44d4-202f-4881-b9b4-0142d793b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_model(dataset):\n",
    "    return model.evaluate(dataset, return_dict=True)\n",
    "\n",
    "metrics = evaluate_model(num_test_ds_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7b693-90ae-48f1-ac4d-467649392cb6",
   "metadata": {},
   "source": [
    "## Save Model + Book Embeddings & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce75a6-9306-4032-8609-4306cafb897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Save both models into separate folders\n",
    "model.user_model.save(\"export/user_model/1\")   # Save user model\n",
    "#model.book_model.save(\"export/book_model/1\")   # Save book model\n",
    "\n",
    "# Create tar.gz archive\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\", arcname=os.path.basename(\"export\"))\n",
    "\n",
    "print(\"✅ Both models saved and compressed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1ab73-2f61-4693-ba0e-eaa1bf2afdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Save both models into separate folders\n",
    "model.save(\"export/parent_model/1\")   # Save parent model\n",
    "\n",
    "# Create tar.gz archive\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\", arcname=os.path.basename(\"export\"))\n",
    "\n",
    "print(\"✅ Both models saved and compressed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3abca-b352-4e62-82a7-26cc93fbb2e7",
   "metadata": {},
   "source": [
    "Push the tar.gz files to the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55261995-76b7-454f-a33f-1e2366116c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "# instantiate clients\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=\"test_models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c5e96-faf9-43f4-8b73-15929f5fbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "# instantiate clients\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "file_name = 'embeddings/book_embeddings.npy'\n",
    "local_file_path = 'book_embeddings.npy'\n",
    "\n",
    "s3.upload_file(local_file_path, bucket_name, file_name)\n",
    "print(f\"✅ Embeddings uploaded to s3://{bucket_name}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e7399-5814-42b2-852f-bd8bcc362e20",
   "metadata": {},
   "source": [
    "## Load in a Saved Model + Book Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa8026-dac6-4854-b08b-6ed536fc73d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import boto3\n",
    "import tarfile\n",
    "import sagemaker\n",
    "\n",
    "# instantiate clients\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download model.tar.gz\n",
    "s3.download_file(bucket_name, \"test_models/model.tar.gz\", \"model.tar.gz\")\n",
    "\n",
    "# Unzip the model\n",
    "with tarfile.open(\"model.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"export\")\n",
    "\n",
    "# Load models\n",
    "user_model = tf.keras.models.load_model(\"export/user_model/1\")\n",
    "book_model = tf.keras.models.load_model(\"export/book_model/1\")\n",
    "\n",
    "print(\"✅ Both models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b05e66-811f-475c-a474-7e71de6995d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "# Instantiate clients\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# File path in S3\n",
    "file_name = 'embeddings/book_embeddings.npy'\n",
    "\n",
    "# Download the file from S3 into memory\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_name)\n",
    "\n",
    "# Read the data into a BytesIO buffer\n",
    "buffer = io.BytesIO(response['Body'].read())\n",
    "\n",
    "# Load numpy array from buffer\n",
    "book_embeddings_1 = np.load(buffer)\n",
    "\n",
    "print(\"✅ Embeddings loaded successfully\")\n",
    "print(\"Shape:\", book_embeddings_1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5f4e9-8671-4f0e-b7c3-74f5c44d262d",
   "metadata": {},
   "source": [
    "## Get Book Embeddings for Each Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad4265-4bd1-4cfc-82b4-a8e1c378e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117b0a3-acc7-4c8d-a2de-7cea03cb824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_embeddings[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf614c-58e2-496f-885b-4aa9bccca54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_unique['Embeddings'] = [embed.numpy() for embed in book_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8dc417-2ae3-424a-8194-efb3dbe95eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caeff51-4bbb-49e6-9e3a-b7a6cc50b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_genre_emb_df = pd.DataFrame(columns=[\"genre\", \"avg_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b9da4-972e-4711-82b4-0880baad74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_genre_emb_df = pd.DataFrame(columns=[\"genre\", \"avg_embedding\"])\n",
    "\n",
    "for i, genre in enumerate(books_df_unique['genre_consolidated'].unique()):\n",
    "    print(i, genre)\n",
    "    # print(books_df_unique[books_df_unique['genre_consolidated'] == genre]['Embeddings'])\n",
    "\n",
    "    embedding_matrix = np.array(books_df_unique[books_df_unique['genre_consolidated'] == genre]['Embeddings'].tolist())\n",
    "    \n",
    "    average_embedding = embedding_matrix.mean(axis=0)\n",
    "    \n",
    "    avg_genre_emb_df.loc[i] = [genre, average_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caeff08-7e2f-4a3f-b620-0eb2a93ffc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_genre_emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9949e-0992-4b43-9836-c68d815d46eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = avg_genre_emb_df\n",
    "\n",
    "df[\"avg_embedding\"] = df[\"avg_embedding\"].apply(lambda x: np.array(eval(x)) if isinstance(x, str) else np.array(x))\n",
    "\n",
    "X = np.vstack(df[\"avg_embedding\"].values)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df_pca[\"genre\"] = df[\"genre\"]\n",
    "\n",
    "fig = px.scatter_3d(df_pca, x=\"PC1\", y=\"PC2\", z=\"PC3\", text=\"genre\",\n",
    "                     color=\"genre\", opacity=0.8, title=\"3D PCA Visualization of Book Genres\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.7), textposition=\"top center\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a59ada-3c76-4270-9207-10e11c27debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pickle\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "bucket_name = sm_session.default_bucket()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "pickle_file_name = \"embeddings/avg_genre_embeddings.pkl\"\n",
    "avg_genre_emb_df.to_pickle(pickle_file_name)\n",
    "\n",
    "s3.upload_file(pickle_file_name, bucket_name, pickle_file_name)\n",
    "print(f\"✅ Pickle file uploaded to s3://{bucket_name}/{pickle_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68d3ae-357b-4d03-8b09-ea1b57d13ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
