{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f26477-93f0-4c57-bf39-d9b27bf9d7c5",
   "metadata": {},
   "source": [
    "In this notebook I will build off of my previous work with the TFRS pipline to simplify the model's towers and to also improve the users' embeddings by incorperating additional session metrics to pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62768ec6-1f07-4b54-a478-df4351130da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135a17f-5a1a-44da-b4b3-9261d17518a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f492ade-b829-483a-a600-642d49c895f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "import io\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "from typing import List, Union, Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import plotnine\n",
    "import gdown\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfd9ad-83b3-44a6-817f-40a7f53ae3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure GPUs are visible\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "if gpus:\n",
    "    # Set memory growth to avoid allocation errors\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Set logical device configuration for CPU\n",
    "cpus = tf.config.list_physical_devices('CPU')\n",
    "if cpus:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        cpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration()]\n",
    "    )\n",
    "\n",
    "print(\"Logical devices configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db5b8b-b45a-4d2c-94f7-6692aa2f588f",
   "metadata": {},
   "source": [
    "Initialize access to s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b922b-20d7-4dd7-a405-7e5dbb3ef8bf",
   "metadata": {},
   "source": [
    "To start off we import the datasets for books and their reviews, performing some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6800c51-b215-4095-b606-c5113bd45999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import books and reviews dataset\n",
    "# books_data_location = 's3://w210recsys/book_raw/books_data.csv'\n",
    "#review_location = 's3://w210recsys/book_raw/Books_rating.csv'\n",
    "\n",
    "books_data_location = \"s3://w210recsys/book_clean/books_data_clean.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6121462-e376-432a-aca5-776974fbf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_pickle(books_data_location) \n",
    "\n",
    "books_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e24fd-359c-434f-b294-d7dae1b470b3",
   "metadata": {},
   "source": [
    "Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36377777-5dfb-48c3-9218-4c06e951d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3747-58c2-45b5-b692-8b40324e7e6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Augment Dataset (Do Not Run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f6a4f-1b87-4129-b584-5cde78207737",
   "metadata": {},
   "source": [
    "As was demonstrated by Ben's EDA on user rating, we see an overwheling amount of ratings are positively skewwed and similarly that many people only have a few review they ever leave. The twin tower model will perform better if it gets example of both what the user likes and what they don't like so we want to augment user's reviews with books they did not interact with compared to the ones they did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f6bfb-293d-4a18-9563-d291a0f55268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aug_data(catalog, user_data_dict, user_last_dates, k_per_user):\n",
    "#     \"\"\"\n",
    "#     Returns a new book sample set from the catalog for multiple users at once, ensuring dates are not earlier than \n",
    "#     the last review date per user.\n",
    "\n",
    "#     Parameters:\n",
    "#     - catalog (pd.DataFrame): Full book catalog.\n",
    "#     - user_data_dict (dict): A dictionary mapping users to a set of interacted books.\n",
    "#     - user_last_dates (dict): A dictionary mapping users to their last review date.\n",
    "#     - k_per_user (int): Number of new books per user.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: A DataFrame with new book samples for all users.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Flatten all interacted books into a set (fast filtering)\n",
    "#     interacted_books = set.union(*user_data_dict.values())\n",
    "\n",
    "#     # Efficient filtering: Remove all interacted books at once\n",
    "#     filtered_catalog = catalog[~catalog[['title', 'author']].apply(tuple, axis=1).isin(interacted_books)]\n",
    "\n",
    "#     # Prepare a list to store new samples\n",
    "#     new_samples = []\n",
    "\n",
    "#     for user, _ in user_data_dict.items():\n",
    "#         # Randomly sample `k_per_user` books\n",
    "#         sampled_books = filtered_catalog.sample(n=min(k_per_user, len(filtered_catalog)), random_state=42).copy()\n",
    "#         sampled_books['user_id'] = user  # Assign user ID\n",
    "#         sampled_books['review_score'] = 0  # The user didn't interact with it\n",
    "\n",
    "#         # Get the last review date for this user\n",
    "#         last_review_date = user_last_dates.get(user, pd.Timestamp.now())  # Default to now if no history\n",
    "        \n",
    "#         # Ensure last_review_date is a pandas Timestamp (datetime64)\n",
    "#         last_review_date = pd.Timestamp(last_review_date)\n",
    "        \n",
    "#         # Generate random timedelta and subtract from last_review_date\n",
    "#         sampled_books['review_time'] = last_review_date - pd.to_timedelta(\n",
    "#             [random.randint(1, 30) for _ in range(len(sampled_books))], unit=\"D\"\n",
    "#         )\n",
    "\n",
    "#         new_samples.append(sampled_books)\n",
    "\n",
    "#     # Concatenate all samples into a single DataFrame\n",
    "#     return pd.concat(new_samples, ignore_index=True)\n",
    "\n",
    "# # Sample Data\n",
    "# temp = books_df#.iloc[0:100]\n",
    "\n",
    "# # Step 1: Convert user interactions into a dictionary {user_id: {(title, author), ...}}\n",
    "# user_data_dict = (\n",
    "#     temp.groupby('user_id')[['title', 'author']]\n",
    "#     .apply(lambda df: set(df.itertuples(index=False, name=None)))\n",
    "#     .to_dict()\n",
    "# )\n",
    "\n",
    "# # Step 2: Extract last review date per user\n",
    "# user_last_dates = (\n",
    "#     temp.groupby('user_id')['review_time']\n",
    "#     .max()\n",
    "#     .to_dict()\n",
    "# )\n",
    "\n",
    "# # Step 3: Call optimized function for batch augmentation\n",
    "# augmented_data = aug_data(books_df, user_data_dict, user_last_dates, k_per_user=3)\n",
    "\n",
    "# # Step 4: Append new recommendations to merged_df efficiently\n",
    "# temp = pd.concat([temp, augmented_data], ignore_index=True)\n",
    "\n",
    "# print(f\"Augmented dataset size: {temp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09785183-b12e-410e-af01-16ce3e71edd3",
   "metadata": {},
   "source": [
    "Save the data to pickle file in the s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bdbb4-68db-4db3-9de2-32a2060baf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.to_pickle(\"s3://w210recsys/aug_data/clean_augmented_data_v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a3b66-9427-4a82-aa5c-fb5968e017eb",
   "metadata": {},
   "source": [
    "Load the data to avoid overheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6964a-653a-442a-847f-d775e6738eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.read_pickle(\"s3://w210recsys/aug_data/clean_augmented_data_v1.pkl\")\n",
    "\n",
    "# merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453849a-6394-4746-83ef-8ea35f2a076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df[merged_df['user_id'] == 'A1SMFD252FTJP9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6685a9b-198f-4fce-9e6b-117a414bc16c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Session-ize Data + Split Dataset for Validation based on Users\n",
    "In the earlier versions of our twin-tower recommendations model we were splitting the data based on dates to isolate the last interaction as our validation data and passing in other interactions, line by line, in as training data. However, upon review we realized that this approach has some flaws:\n",
    "\n",
    "1. Data Leakage - By passing in the same users in training and testing we may be getting an inflated sense of how good the model is doing.\n",
    "2. Line by line data - The goal of our recommendation system is to take in a user's metrics at once and provide a user embedding that will likely set them closer tot he vooks they like in the embedding space. However, by passing in data line by line, we don't aggregate this data in the same way and the model may not be learning that\n",
    "\n",
    "For these reasons, we opt to split the data by users, holding out their last interaction as the label, rather than by date.\n",
    "\n",
    "Session-izing Data:\n",
    "\n",
    "We want to group the historical data for each user in a way that allows us to mimic the session data we will collect from users in deployment. The basic structure of which will be to summarize past interactions and hold out a separate book interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe4205-6e65-439c-8a5a-19492bf98ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test on a smaller sub-set of the data\n",
    "\n",
    "# merged_df = merged_df.sort_values(by=['user_id', 'review_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4644af-a0cd-4793-a92a-2fe414d04641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833f86c-596f-4686-83a9-328ee63c5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = merged_df[merged_df['user_id'] == 'A1SMFD252FTJP9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd33170-515b-4c3e-af7b-3fa5ea63a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def session_summary(user_data):\n",
    "\n",
    "#     \"\"\"\n",
    "#     session_summary takes in each user's session data and returns a summarized verison of it\n",
    "\n",
    "#     inputs:\n",
    "#     user_data: user's interaction data\n",
    "#     interest_cols: columns of interest wanting to be summarized\n",
    "\n",
    "#     outputs:\n",
    "#     summarized_data: the user's summarized data\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     user_data = user_data.sort_values(by=['user_id', 'review_time'])  # Sort by user & time\n",
    "    \n",
    "#     session_data = []\n",
    "    \n",
    "#     for user, user_df in user_data.groupby('user_id'):\n",
    "#         if len(user_df) < 2:\n",
    "#             continue  # Skip users with only one interaction\n",
    "        \n",
    "#         # Last interaction is the target (book user last interacted with)\n",
    "#         target_row = user_df.iloc[-1]\n",
    "#         target_book = target_row['title']\n",
    "#         target_book_rating = target_row['review_score']\n",
    "        \n",
    "#         # Previous interactions (session history3\n",
    "#         history_df = user_df.iloc[:-1]  # Exclude last row]\n",
    "\n",
    "#         summary = {\n",
    "#             'user_id': user,\n",
    "#             'liked_books': list(history_df.loc[history_df['review_score'] >= 3, 'title']),\n",
    "#             'disliked_books': list(history_df.loc[history_df['review_score'] < 3, 'title']),\n",
    "#             'liked_genres': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] >= 3, 'genre_consolidated'])))),\n",
    "#             'disliked_genres': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] < 3, 'genre_consolidated'])))),\n",
    "#             'liked_authors': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] >= 3, 'author'])))),\n",
    "#             'disliked_authors': list(filter(lambda x: x != \"\", list(set(history_df.loc[history_df['review_score'] < 3, 'author'])))),\n",
    "#             'liked_ratings': list(history_df.loc[history_df['review_score'] >= 3, 'review_score']),\n",
    "#             'disliked_ratings': list(history_df.loc[history_df['review_score'] < 3, 'review_score']),\n",
    "#             'target_book': target_book,\n",
    "#             'target_book_rating': target_book_rating\n",
    "#         }\n",
    "        \n",
    "#         session_data.append(summary)\n",
    "    \n",
    "#     return pd.DataFrame(session_data)\n",
    "\n",
    "# # Generate session summaries with held-out target sample\n",
    "# # session_summary(temp)\n",
    "\n",
    "# # Generate a full df of session summaries\n",
    "# # sessionized_df = session_summary(merged_df)\n",
    "# sessionized_df = session_summary(sampled_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3c66d-398a-4c6c-8f11-aef77b34e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessionized_df.to_pickle(\"s3://w210recsys/aug_data/cleaned_sessionized_data_v1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cdd4d-af93-4912-99c2-9ce4423e4f1e",
   "metadata": {},
   "source": [
    "To run this function in a timely manner I had to leverage a much larger compute instance than the one this notebook was created on. I saved the result to the team s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120e4f4-d899-48c0-a3ee-ed1b790c0dfc",
   "metadata": {},
   "source": [
    "## Load Sessionized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c11792-fed9-4bef-bbb6-691a195b1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionized_df = pd.read_pickle(\"s3://w210recsys/aug_data/cleaned_sessionized_data_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3155b19-89f4-47ec-af3f-89f955e75d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c43ec-68f3-4835-9e94-c303b0e616ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af4b54-71fc-4bc2-8122-2d7be65d9d4d",
   "metadata": {},
   "source": [
    "## Augment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69648a6c-71b7-44f8-bfb0-c643ac4673e2",
   "metadata": {},
   "source": [
    "Down the line I realized that when training we'll want to pass in more than just the book's title to our book tower so I want to augment the dataset with the author and the summaries of all of those books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972eaa46-9ed8-44f8-9277-266e8260935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05bae4-e1ce-4398-89a5-1f510d151597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Keep just the first instance of each book since we only care about the books' metadata, which should be the same regardless\n",
    "books_df_unique = books_df.drop_duplicates(subset=['title'], keep='first')\n",
    "\n",
    "# Convert books_df to a dictionary for fast lookup\n",
    "books_dict = books_df_unique.set_index('title')[['author', 'description', 'genre_consolidated']].to_dict(orient='index')\n",
    "\n",
    "target_book_author = []\n",
    "target_book_summary = []\n",
    "target_book_categories = []\n",
    "\n",
    "for title in tqdm(sessionized_df['target_book'], desc=\"Processing Books\"):\n",
    "    book_info = books_dict.get(title, {'author': '', 'description': '', 'genre_consolidated': ''})\n",
    "\n",
    "    target_book_author.append(book_info['author'])\n",
    "    target_book_summary.append(book_info['description'])\n",
    "    target_book_categories.append(book_info['genre_consolidated'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57b7d0-2ad4-4818-98fe-9f44648ac61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Though I'm keeping the column name as 'categories' it should reflect 'genre_consolidated' at all times henceforth\n",
    "\n",
    "sessionized_df['authors'] = target_book_author\n",
    "sessionized_df['description'] = target_book_summary\n",
    "sessionized_df['categories'] = target_book_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c249e5f9-b813-44a6-8aa0-99f98c83cbc6",
   "metadata": {},
   "source": [
    "Now that we have the sessionized data, we can move into the splitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382228f0-e2a4-48f9-b255-a7b48a6c3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = train_test_split(sessionized_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023b23b-b84c-4f87-a834-98f0ce69d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e26a4c-4ab0-48a0-bd28-c95f10b36bec",
   "metadata": {},
   "source": [
    "## Converting string input to numerical\n",
    "\n",
    "So the model, when running on CPUs, is able to take care of string entries to numerical via our lookup layers internally. However, when trying to utilize the GPU for faster training, it appears as through string tensor conversions from CPU to GPU aren't supported. As a result, I'm looking into converting our string entries to numerical values prior to passing it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244ecd6-ce51-40e9-94bb-720d4c116eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique values for user and book metadata\n",
    "unique_user_ids = sessionized_df['user_id'].astype(str).unique().tolist()\n",
    "unique_book_titles = books_df['title'].astype(str).unique().tolist()\n",
    "unique_genres = books_df['genre_consolidated'].astype(str).unique().tolist()\n",
    "unique_authors = books_df['author'].astype(str).unique().tolist()\n",
    "unique_summaries = books_df['description'].astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564eb473-7982-4f15-a5b8-5f511b63d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_user_ids), len(unique_book_titles), len(unique_genres), len(unique_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a7e5c-a089-46dd-958d-d4ff4d9fc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions = 64 # 64\n",
    "\n",
    "# Create a StringLookup layer for user_id\n",
    "user_id_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_user_ids,\n",
    "    mask_token=None,\n",
    "    name='user_id_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for user_id\n",
    "user_id_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_user_ids) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='user_id_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for book_title\n",
    "book_title_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_book_titles,\n",
    "    mask_token=None,\n",
    "    name='book_title_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for book_title\n",
    "book_title_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_book_titles) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_title_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for genre\n",
    "book_genre_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_genres,\n",
    "    mask_token=None,\n",
    "    name='book_genre_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for genre\n",
    "book_genre_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_genres) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_genre_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for authors\n",
    "book_authors_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_authors,\n",
    "    mask_token=None,\n",
    "    name='book_authors_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for authors\n",
    "book_authors_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_authors) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_authors_embedding'\n",
    ")\n",
    "\n",
    "# Create a StringLookup layer for description\n",
    "book_description_vocab_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=unique_summaries,\n",
    "    mask_token=None,\n",
    "    name='book_description_vocab'\n",
    ")\n",
    "\n",
    "# Create an Embedding layer for descriptions\n",
    "book_description_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(unique_summaries) + 1, \n",
    "    output_dim=embedding_dimensions, \n",
    "    name='book_description_embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87ff8e-6301-4ce3-9479-05cd77645bbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save the StringLookUp layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fc66e-b9ee-4d65-8bfb-95190f920305",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary\": unique_user_ids,\n",
    "    \"oov_token\": user_id_vocab_layer.oov_token\n",
    "}\n",
    "\n",
    "with open('user_id_vocab_layer.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f4ee5-6402-4191-ae32-52d86a8d998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A014566028TLL40XCY1YR\n",
    "\n",
    "with open(\"user_id_vocab_layer.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "user_id_vocab_layer = tf.keras.layers.StringLookup(vocabulary=config['vocabulary'], oov_token=config['oov_token'])\n",
    "\n",
    "input_data = \"A3A48XEYWLWH7T\"\n",
    "\n",
    "output = user_id_vocab_layer(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04321ddd-4146-4e38-b876-3e251810f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614b279-9887-4c9f-b454-3648b6a308b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "import boto3\n",
    "# import subprocess\n",
    "# import os\n",
    "# import pickle\n",
    "# import joblib\n",
    "# import tarfile\n",
    "# import shutil\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd63174-4ba0-437c-ba62-877b5d71d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3://w210recsys/model/recModel/modelFiles/\n",
    "bucket_name=\"w210recsys\"\n",
    "key_prefix=\"model/recModel/modelFiles\"\n",
    "s3_response = sm_session.upload_data(\"book_authors_vocab_layer.json\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcf188-f2db-4700-8f33-250aad74f18b",
   "metadata": {},
   "source": [
    "# Convert the information from books_df to be numerical using the string lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d9e78-abf5-42c5-b069-b53ae0444b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_books_df = books_df[['title', 'author', 'genre_consolidated', 'description']].copy()\n",
    "numerical_books_df.columns = ['title', 'authors', 'categories', 'description']\n",
    "\n",
    "numerical_books_df = numerical_books_df.drop_duplicates(subset=['title'], keep='first')\n",
    "\n",
    "\n",
    "numerical_books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f883e-168b-41e6-af5e-fd746aa5f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the lookup transformation to each column\n",
    "\n",
    "numerical_books_df['title'] = book_title_vocab_layer(numerical_books_df['title']).numpy()\n",
    "numerical_books_df['authors'] = book_authors_vocab_layer(numerical_books_df['authors']).numpy()\n",
    "numerical_books_df['categories'] = book_genre_vocab_layer(numerical_books_df['categories']).numpy()\n",
    "numerical_books_df['description'] = book_description_vocab_layer(numerical_books_df['description']).numpy()\n",
    "\n",
    "# Also add in the 'book_id'\n",
    "numerical_books_df['book_id'] = [i for i in range(0, numerical_books_df.shape[0])]\n",
    "\n",
    "numerical_books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9bcdf-e407-4b05-abb4-5f5e4e1006ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_sessionized_df = sessionized_df.copy()\n",
    "\n",
    "numerical_sessionized_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d3c1f-e985-4a58-b28d-df082be1937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numerical_sessionized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dfa69-84d2-493c-a5be-c537f3c2bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_sessionized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee81b1d-8161-402e-96cf-948f062a2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lookup layer in a vectorized manner\n",
    "def fast_lookup_list(values, lookup_layer):\n",
    "    values = values.apply(lambda x: x if isinstance(x, list) else [\"UNKNOWN\"])  # Ensure lists\n",
    "    tensor_input = tf.ragged.constant(values.tolist(), dtype=tf.string)  # Convert to ragged tensor\n",
    "    return lookup_layer(tensor_input).numpy().tolist()  # Apply lookup in batch and convert to list\n",
    "\n",
    "# Apply lookup in bulk for better performance\n",
    "numerical_sessionized_df['user_id'] = user_id_vocab_layer(numerical_sessionized_df['user_id'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['liked_books'] = fast_lookup_list(numerical_sessionized_df['liked_books'], book_title_vocab_layer)\n",
    "numerical_sessionized_df['disliked_books'] = fast_lookup_list(numerical_sessionized_df['disliked_books'], book_title_vocab_layer)\n",
    "\n",
    "numerical_sessionized_df['liked_genres'] = fast_lookup_list(numerical_sessionized_df['liked_genres'], book_genre_vocab_layer)\n",
    "numerical_sessionized_df['disliked_genres'] = fast_lookup_list(numerical_sessionized_df['disliked_genres'], book_genre_vocab_layer)\n",
    "\n",
    "numerical_sessionized_df['liked_authors'] = fast_lookup_list(numerical_sessionized_df['liked_authors'], book_authors_vocab_layer)\n",
    "numerical_sessionized_df['disliked_authors'] = fast_lookup_list(numerical_sessionized_df['disliked_authors'], book_authors_vocab_layer)\n",
    "\n",
    "numerical_sessionized_df['target_book'] = book_title_vocab_layer(numerical_sessionized_df['target_book'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['authors'] = book_authors_vocab_layer(numerical_sessionized_df['authors'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['description'] = book_description_vocab_layer(numerical_sessionized_df['description'].astype(str)).numpy()\n",
    "\n",
    "numerical_sessionized_df['categories'] = book_genre_vocab_layer(numerical_sessionized_df['categories'].astype(str)).numpy()\n",
    "\n",
    "# Increment the ratings up by 1 to leave 0 to be the padding\n",
    "\n",
    "def rating_shift(ratings):\n",
    "\n",
    "    if type(ratings) == list:\n",
    "        if len(ratings) == 0:\n",
    "            return ratings\n",
    "\n",
    "        return [entry + 1 for entry in ratings]\n",
    "\n",
    "    else:\n",
    "\n",
    "        return ratings + 1\n",
    "\n",
    "\n",
    "numerical_sessionized_df['liked_ratings'] = numerical_sessionized_df['liked_ratings'].apply(lambda x: rating_shift(x))\n",
    "numerical_sessionized_df['disliked_ratings'] = numerical_sessionized_df['disliked_ratings'].apply(lambda x: rating_shift(x))\n",
    "numerical_sessionized_df['target_book_rating'] = numerical_sessionized_df['target_book_rating'].apply(lambda x: rating_shift(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb55006-d5fe-44cc-a3d7-bc85a0dd24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_sessionized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba55ccd-56be-4900-8795-98db1f3ec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "HT = {}\n",
    "\n",
    "for col in numerical_sessionized_df.columns:\n",
    "    try:\n",
    "        for entry in numerical_sessionized_df[col]:\n",
    "            \n",
    "            if col not in HT:\n",
    "                HT[col] = [len(entry)]\n",
    "            else:\n",
    "                HT[col].append(len(entry))\n",
    "    except:\n",
    "        HT[col] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf051ce-5013-4211-bd43-2d2eada9f7c3",
   "metadata": {},
   "source": [
    "So later down the line I noticed that we can't call our mode for inferences with ragged tensors since their sizes can vary and the model needs fixed sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfb45d-2105-4266-aa45-9f85a10bf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in HT.keys():\n",
    "    print(col, int(np.percentile(HT[col], 90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d039e9-3e3e-47c6-8155-d387cd37c026",
   "metadata": {},
   "source": [
    "We need to consider what *most* of our samples' lengths are and add in some extra space for future inferences with more information. Since in the future I plan on having feed back on every 10 books or so I'll allocate ~20 slots for each list just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72b190-9e53-48b9-8c90-92738432782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HT['liked_books'] = 20\n",
    "HT['disliked_books'] = 20\n",
    "\n",
    "HT['liked_genres'] = 20\n",
    "HT['disliked_genres'] = 20\n",
    "\n",
    "HT['liked_authors'] = 20\n",
    "HT['disliked_authors'] = 20\n",
    "\n",
    "HT['liked_ratings'] = 20\n",
    "HT['disliked_ratings'] = 20\n",
    "\n",
    "HT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575014a4-3c86-4ab5-b23e-5190c7759170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_numpy_array(column, max_len, padding_value=0):\n",
    "    return column.apply(lambda x: np.pad(x[:max_len], (0, max_len - min(len(x), max_len)), constant_values=padding_value))\n",
    "\n",
    "def pad_column(column, max_len, padding_value=0):\n",
    "    return column.apply(lambda x: (x[:max_len] if len(x) > max_len else x + [padding_value] * (max_len - len(x))))\n",
    "\n",
    "for column, max_len in HT.items():\n",
    "    first_entry = numerical_sessionized_df[column].iloc[0]\n",
    "    \n",
    "    if isinstance(first_entry, np.ndarray):\n",
    "        numerical_sessionized_df[column] = pad_numpy_array(numerical_sessionized_df[column], max_len)\n",
    "        \n",
    "    elif isinstance(first_entry, list):\n",
    "        numerical_sessionized_df[column] = pad_column(numerical_sessionized_df[column], max_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ad4ed-4379-497f-bf15-f14d3473b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_sessionized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4950e-e806-4b67-95ab-8930b1f7674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_df, num_test_df = train_test_split(numerical_sessionized_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f241909-3e1a-4db9-b201-3e63a5a7d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfe142-9a3d-4783-a05d-71d885387c6d",
   "metadata": {},
   "source": [
    "## Save need data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36b93b-4734-4e33-b764-c0ed19ad4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš¨ Starting dataset creation...\")\n",
    "\n",
    "num_train_ds = tf.data.Dataset.from_tensor_slices({\n",
    "    'user_id': tf.constant(num_train_df['user_id'].tolist(), dtype=tf.int64),\n",
    "    'liked_books': tf.constant(num_train_df['liked_books'].tolist(), dtype=tf.int64),\n",
    "    'disliked_books': tf.constant(num_train_df['disliked_books'].tolist(), dtype=tf.int64),\n",
    "    'liked_genres': tf.constant(num_train_df['liked_genres'].tolist(), dtype=tf.int64),\n",
    "    'disliked_genres': tf.constant(num_train_df['disliked_genres'].tolist(), dtype=tf.int64),\n",
    "    'liked_authors': tf.constant(num_train_df['liked_authors'].tolist(), dtype=tf.int64),\n",
    "    'disliked_authors': tf.constant(num_train_df['disliked_authors'].tolist(), dtype=tf.int64),\n",
    "    'liked_ratings': tf.constant(num_train_df['liked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'disliked_ratings': tf.constant(num_train_df['disliked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'target_book': tf.constant(num_train_df['target_book'], dtype=tf.int64),\n",
    "    'authors': tf.constant(num_train_df['authors'], dtype=tf.int64),\n",
    "    'description': tf.constant(num_train_df['description'], dtype=tf.int64),\n",
    "    'categories': tf.constant(num_train_df['categories'], dtype=tf.int64),\n",
    "    'target_book_rating': tf.constant(num_train_df['target_book_rating'], dtype=tf.float32),\n",
    "})\n",
    "\n",
    "num_test_ds = tf.data.Dataset.from_tensor_slices({\n",
    "    'user_id': tf.constant(num_test_df['user_id'].tolist(), dtype=tf.int64),\n",
    "    'liked_books': tf.constant(num_test_df['liked_books'].tolist(), dtype=tf.int64),\n",
    "    'disliked_books': tf.constant(num_test_df['disliked_books'].tolist(), dtype=tf.int64),\n",
    "    'liked_genres': tf.constant(num_test_df['liked_genres'].tolist(), dtype=tf.int64),\n",
    "    'disliked_genres': tf.constant(num_test_df['disliked_genres'].tolist(), dtype=tf.int64),\n",
    "    'liked_authors': tf.constant(num_test_df['liked_authors'].tolist(), dtype=tf.int64),\n",
    "    'disliked_authors': tf.constant(num_test_df['disliked_authors'].tolist(), dtype=tf.int64),\n",
    "    'liked_ratings': tf.constant(num_test_df['liked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'disliked_ratings': tf.constant(num_test_df['disliked_ratings'].tolist(), dtype=tf.float32),\n",
    "    'target_book': tf.constant(num_test_df['target_book'], dtype=tf.int64),\n",
    "    'authors': tf.constant(num_test_df['authors'], dtype=tf.int64),\n",
    "    'description': tf.constant(num_test_df['description'], dtype=tf.int64),\n",
    "    'categories': tf.constant(num_test_df['categories'], dtype=tf.int64),\n",
    "    'target_book_rating': tf.constant(num_test_df['target_book_rating'], dtype=tf.float32),\n",
    "})\n",
    "\n",
    "print(\"âœ… Datasets successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2b6c5-c94f-4d83-8cc7-7fa57469676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in num_train_ds.take(1):\n",
    "\n",
    "    print(example)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ee82b-3eb1-470a-a128-1d113be7ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_ds_limited = num_train_ds#.take(500)  # Limit to 1000 samples\n",
    "num_test_ds_limited = num_test_ds#.take(500)  # Limit to 500 samples\n",
    "\n",
    "num_train_ds_cached = num_train_ds_limited.batch(128).cache()#.batch(128).cache()\n",
    "num_test_ds_cached = num_test_ds_limited.batch(128).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041accc-474c-4cb2-9aad-b630ac3ebfd2",
   "metadata": {},
   "source": [
    "## Build Twin-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9c491-3f1a-46c7-871f-76430344d639",
   "metadata": {},
   "source": [
    "Compared to the original version of our twin tower model, I want to simplify the input scheme to the user and book towers and incorporate some additional features for user embedding. Furthermore, we want to pass in sessionized user data separately from book data to each tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d81d4-a65f-474a-bf15-ed231618d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow.keras.layers as layers\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "class BooksTwoTowersModel(tfrs.Model):\n",
    "    def __init__(self, user_data: pd.DataFrame, book_metadata: pd.DataFrame, embedding_dimensions=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_projection_user = tf.keras.layers.Dense(embedding_dimensions, name='user_dense_projection')\n",
    "        \n",
    "        self.dense_projection_book = tf.keras.layers.Dense(embedding_dimensions, name='book_dense_projection')\n",
    "\n",
    "        self.user_model = UserModel(user_data, book_metadata, self.dense_projection_user, embedding_dimensions)\n",
    "\n",
    "        self.book_model = BookModel(book_metadata, embedding_dimensions, 10000, self.dense_projection_book)\n",
    "\n",
    "        self.candidate_ds = tf.data.Dataset.from_tensor_slices({\n",
    "            'title': tf.convert_to_tensor(book_metadata['title'].values, dtype=tf.int64),\n",
    "            'authors': tf.convert_to_tensor(book_metadata['authors'].values, dtype=tf.int64),\n",
    "            'description': tf.convert_to_tensor(book_metadata['description'].values, dtype=tf.int64),\n",
    "            'categories': tf.convert_to_tensor(book_metadata['categories'].values, dtype=tf.int64)\n",
    "        })\n",
    "\n",
    "        candidates = self.candidate_ds.batch(1).map(\n",
    "            lambda x: self.book_model(x), num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).map(\n",
    "            lambda x: tf.squeeze(x, axis=0)\n",
    "        )\n",
    "\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=candidates.batch(1),\n",
    "                ks=(10, 20, 50)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.full_book_embeddings = None\n",
    "        self.full_book_embeddings_copy = None\n",
    "\n",
    "    def compute_loss(self, features: Dict[str, tf.Tensor], training=False) -> tf.Tensor:\n",
    "\n",
    "        user_embeddings = self.user_model(features)\n",
    "\n",
    "        target_book_embeddings = self.book_model(features)\n",
    "\n",
    "        retrieval_loss = self.task(user_embeddings, target_book_embeddings, compute_metrics=not training)\n",
    "\n",
    "        return retrieval_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd74ab6-94e8-41e9-bc52-694ce9a143d5",
   "metadata": {},
   "source": [
    "### Book and User model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f294221-fe13-44a3-9b48-7b271f8e24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(tf.keras.Model):\n",
    "    '''\n",
    "    The book(query) tower that processes book data.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, book_data: pd.DataFrame, embedding_dimensions: int, text_vectorization_max_tokens: int, dense_projection_book): #, book_title_weight_layer, book_author_weight_layer, book_genre_weight_layer):\n",
    "        '''\n",
    "        :param book_data: DataFrame containing book information.\n",
    "        :param embedding_dimensions: Number of dimensions in embedding layer.\n",
    "        :param text_vectorization_max_tokens: Maximum number of tokens to vector.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Extract unique values for embeddings\n",
    "        self.feature_book_title_name = \"title\"\n",
    "        self.feature_author_name = \"authors\"\n",
    "        self.feature_genre_name = \"categories\"\n",
    "\n",
    "        unique_titles = book_data[self.feature_book_title_name].astype(str).unique()\n",
    "        unique_authors = book_data[self.feature_author_name].astype(str).unique()\n",
    "        unique_genres = book_data[self.feature_genre_name ].astype(str).unique()\n",
    "\n",
    "        self.dense_projection_book = dense_projection_book\n",
    "        \n",
    "        # Book Title embedding\n",
    "        self.book_title_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_titles) + 1, output_dim=embedding_dimensions, name='book_title_embedding')\n",
    "\n",
    "        # Book Author embedding\n",
    "        self.book_author_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_authors) + 1, output_dim=embedding_dimensions, name='book_author_embedding')\n",
    "\n",
    "        # Book Genere embedding\n",
    "        self.book_genre_emdedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_genres) + 1, output_dim=embedding_dimensions, name='book_genre_embedding')\n",
    "    \n",
    "        # print(\"Finsihed setting up book tower\\n\")\n",
    "\n",
    "    def call(self, book_data: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \n",
    "        # Handle case where 'target_book' might not exist\n",
    "        try:\n",
    "            if len(book_data['target_book'].shape) == 0:\n",
    "                book_data['target_book'] = tf.expand_dims(book_data['target_book'], axis=0)\n",
    "            \n",
    "            book_title_embed = self.book_title_embedding_layers(book_data['target_book'])\n",
    "        except KeyError:\n",
    "            if len(book_data['title'].shape) == 0:\n",
    "                book_data['title'] = tf.expand_dims(book_data['title'], axis=0)\n",
    "                \n",
    "            book_title_embed = self.book_title_embedding_layers(book_data['title'])\n",
    "        \n",
    "        if len(book_data['authors'].shape) == 0:\n",
    "            book_data['authors'] = tf.expand_dims(book_data['authors'], axis=0)\n",
    "            book_data['categories'] = tf.expand_dims(book_data['categories'], axis=0)\n",
    "            \n",
    "        book_author_embed = self.book_author_embedding_layers(book_data['authors'])\n",
    "        book_genre_embed = self.book_genre_emdedding_layers(book_data['categories'])\n",
    "        \n",
    "        # Concatenation without expand_dims\n",
    "        concatenated_embeddings = tf.concat([\n",
    "            book_title_embed,\n",
    "            book_author_embed,\n",
    "            book_genre_embed\n",
    "            ], axis=-1)  # Use last axis for feature concat\n",
    "    \n",
    "        # Apply projection to 64D embedding\n",
    "        projected_embeddings = self.dense_projection_book(concatenated_embeddings)\n",
    "    \n",
    "        return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81053df9-b155-4c12-be1f-f9755fc87992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, user_data: pd.DataFrame, book_metadata: pd.DataFrame, dense_projection_user, embedding_dimensions=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Extract unique values from user and book metadata\n",
    "        unique_user_ids = user_data['user_id'].astype(str).unique().tolist()\n",
    "        unique_book_titles = book_metadata['title'].astype(str).unique().tolist()\n",
    "        unique_genres = book_metadata['categories'].astype(str).unique().tolist()\n",
    "        unique_authors = book_metadata['authors'].astype(str).unique().tolist()\n",
    "\n",
    "        self.dense_projection_user = dense_projection_user\n",
    "        \n",
    "        # User embedding\n",
    "        self.user_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_user_ids) + 1, output_dim=embedding_dimensions, name='user_id_embedding')\n",
    "\n",
    "        # Book embedding\n",
    "        self.book_title_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_book_titles) + 1, output_dim=embedding_dimensions, name='book_embedding')\n",
    "\n",
    "        # Genre embedding\n",
    "        self.genre_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_genres) + 1, output_dim=embedding_dimensions, name='genre_embedding')\n",
    "\n",
    "        # Author embedding\n",
    "        self.author_embedding_layers = tf.keras.layers.Embedding(input_dim=len(unique_authors) + 1, output_dim=embedding_dimensions, name='author_embedding')\n",
    "        \n",
    "        # print(\"Finsihed setting up user tower\\n\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        if str(type(inputs)) != \"<class 'tensorflow.python.framework.ops.EagerTensor'>\":\n",
    "            pass\n",
    "        else:\n",
    "            inputs = {\n",
    "                'user_id': tf.expand_dims(inputs[0][0], axis = 0),\n",
    "                'liked_books': tf.expand_dims(inputs[1], axis = 0),\n",
    "                'disliked_books': tf.expand_dims(inputs[2], axis = 0),\n",
    "                'liked_genres': tf.expand_dims(inputs[3], axis = 0),\n",
    "                'disliked_genres': tf.expand_dims(inputs[4], axis = 0),\n",
    "                'liked_authors': tf.expand_dims(inputs[5], axis = 0),\n",
    "                'disliked_authors': tf.expand_dims(inputs[6], axis = 0),\n",
    "                'liked_ratings': tf.expand_dims(inputs[7], axis = 0),\n",
    "                'disliked_ratings': tf.expand_dims(inputs[8], axis = 0)\n",
    "            }\n",
    "        \n",
    "        user_embed = self.user_embedding_layers(inputs['user_id'])\n",
    "\n",
    "        def pool_embeddings(embedding_layer, input_list, weights, embedding_dim=64, pad_value=0):\n",
    "            # Get embeddings\n",
    "            embeddings = embedding_layer(input_list)\n",
    "        \n",
    "            # Create mask\n",
    "            mask = tf.not_equal(input_list, pad_value)\n",
    "            mask = tf.expand_dims(mask, axis=-1)\n",
    "        \n",
    "            # Zero out padded embeddings\n",
    "            embeddings = tf.where(mask, embeddings, tf.zeros_like(embeddings))\n",
    "        \n",
    "            # Normalize weights (zero-safe)\n",
    "            weight_sum = tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
    "            weight_sum = tf.where(weight_sum == 0, tf.ones_like(weight_sum), weight_sum)\n",
    "            weights = weights / weight_sum\n",
    "            \n",
    "            # Expand weights dims\n",
    "            expanded_weights = tf.expand_dims(weights, axis=-1)\n",
    "            \n",
    "            # Weighted Embeddings\n",
    "            weighted_embeddings = embeddings * expanded_weights\n",
    "        \n",
    "            # Sum + Pool\n",
    "            summed_embeddings = tf.reduce_sum(weighted_embeddings, axis=1)\n",
    "            valid_counts = tf.reduce_sum(tf.cast(mask, tf.float32), axis=1)\n",
    "            valid_counts = tf.where(valid_counts == 0, tf.ones_like(valid_counts), valid_counts)\n",
    "            pooled_embeddings = summed_embeddings / valid_counts\n",
    "        \n",
    "            # Fix NaNs\n",
    "            pooled_embeddings = tf.where(tf.math.is_nan(pooled_embeddings), tf.zeros_like(pooled_embeddings), pooled_embeddings)\n",
    "        \n",
    "            return pooled_embeddings\n",
    "        \n",
    "        # Process liked books\n",
    "        liked_books_embed = pool_embeddings(self.book_title_embedding_layers, inputs['liked_books'], inputs['liked_ratings'])\n",
    "\n",
    "        # Process disliked books\n",
    "        disliked_books_embed = pool_embeddings(self.book_title_embedding_layers, inputs['disliked_books'], inputs['disliked_ratings'])\n",
    "\n",
    "        # Process liked genres\n",
    "        liked_genres_embed = pool_embeddings(self.genre_embedding_layers, inputs['liked_genres'], inputs['liked_ratings'])\n",
    "\n",
    "        # Process disliked genres\n",
    "        disliked_genres_embed = pool_embeddings(self.genre_embedding_layers, inputs['disliked_genres'], inputs['disliked_ratings'])\n",
    "\n",
    "        # Process liked authors\n",
    "        liked_authors_embed = pool_embeddings(self.author_embedding_layers, inputs['liked_authors'], inputs['liked_ratings'])\n",
    "\n",
    "        # Process disliked authors\n",
    "        disliked_authors_embed = pool_embeddings(self.author_embedding_layers, inputs['disliked_authors'], inputs['disliked_ratings'])\n",
    "\n",
    "        # Concatenate everything into a single user representation\n",
    "        try:\n",
    "            concatenated_embeddings = tf.concat([\n",
    "                user_embed,\n",
    "                liked_books_embed,\n",
    "                disliked_books_embed,\n",
    "                liked_genres_embed,\n",
    "                disliked_genres_embed,\n",
    "                liked_authors_embed,\n",
    "                disliked_authors_embed\n",
    "            ], axis=1)\n",
    "        except:\n",
    "            return inputs\n",
    "\n",
    "        projected_embeddings = self.dense_projection_user(concatenated_embeddings)\n",
    "\n",
    "        # print(f\"projected_embeddings.shape: {projected_embeddings.shape}\\n\")\n",
    "        \n",
    "        return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7487d3b-99d7-4e72-8121-8950452ff390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "\n",
    "# Enable Multi-GPU Training\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of GPUs being used: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = BooksTwoTowersModel(\n",
    "        user_data=numerical_sessionized_df,\n",
    "        book_metadata=numerical_books_df,\n",
    "        embedding_dimensions=64, # Change this for embedding sizes to change (64 default val)\n",
    "    )\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14609b5-066f-4aa5-a65b-e9e5cce76452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(num_train_ds_cached, epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00d063-ed13-4a26-9320-80f23fbd8103",
   "metadata": {},
   "source": [
    "See if you can get recommendations directly with filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2485ef7-3ba9-48c7-9f49-6a46aaf40a55",
   "metadata": {},
   "source": [
    "## Save model.user_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fa03e-56c7-4fad-9c22-566516134bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "import boto3\n",
    "import subprocess\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import tarfile\n",
    "import shutil\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session()\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86da6dc-ec2c-4621-a63c-615f9b76e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.tar.gz to required S3 bucket\n",
    "# s3://w210recsys/testModels/model.tar.gz\n",
    "bucket_name=\"w210recsys\"\n",
    "\n",
    "## Real Model\n",
    "# key_prefix=\"model/recModel\"\n",
    "\n",
    "# Test Model\n",
    "key_prefix = \"testModels\"\n",
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a69da-b177-4112-83f6-447a1901e8c5",
   "metadata": {},
   "source": [
    "### Model Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f053147-c2be-4bbf-8a1e-9b72f6145748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Fiction / Mystery & Detective', 'Young Adult Fiction / General',\n",
    "#        'Drama / General', 'Juvenile Fiction / Fantasy & Magic',\n",
    "#        'Juvenile Fiction / General', 'Fiction / World Literature',\n",
    "#        'Fiction / Romance', 'Political Science / General',\n",
    "#        'Fiction / Literary', 'Business & Economics / General',\n",
    "#        'Juvenile Fiction / Legends, Myths, Fables',\n",
    "#        'Juvenile Fiction / Science Fiction',\n",
    "#        'Fiction / Fairy Tales, Folk Tales, Legends & Mythology',\n",
    "#        'Fiction / Science Fiction', 'Fiction / Classics',\n",
    "#        'Religion / General', 'Fiction / General', 'Fiction / Ghost',\n",
    "#        'Fiction / Action & Adventure', 'Juvenile Nonfiction / General',\n",
    "#        'Juvenile Fiction / Fairy Tales & Folklore',\n",
    "#        'Fiction / War & Military', 'History / Maritime History & Piracy',\n",
    "#        'Juvenile Fiction / Thrillers & Suspense',\n",
    "#        'Comics & Graphic Novels / Superheroes',\n",
    "#        'Literary Criticism / General', 'Science / General',\n",
    "#        'Reference / General', 'History / General',\n",
    "#        'Fiction / Occult & Supernatural', 'Philosophy / General',\n",
    "#        'Computers / General',\n",
    "#        'Biography & Autobiography / Personal Memoirs', 'Art / General',\n",
    "#        'Fiction / Visionary & Metaphysical',\n",
    "#        'Family & Relationships / General', 'Fiction / Thrillers',\n",
    "#        'Health & Fitness / General', 'Fiction / Anthologies',\n",
    "#        'Biography & Autobiography / General', 'Fiction / Sea Stories',\n",
    "#        'Fiction / Erotica', 'Fiction / Sagas',\n",
    "#        'Fiction / Magical Realism', 'Fiction / Biographical',\n",
    "#        'History / Expeditions & Discoveries', 'Education / General',\n",
    "#        'Juvenile Fiction / Nursery Rhymes', 'Humor / Topic',\n",
    "#        'Nature / General', 'True Crime / Murder', 'Psychology / General',\n",
    "#        'Social Science / General', 'Photography / General',\n",
    "#        'Religion / Theology', 'Fiction / Dystopian',\n",
    "#        'History / Wars & Conflicts', 'Body, Mind & Spirit / General',\n",
    "#        'Fiction / Short Stories', 'History / Social History',\n",
    "#        'Games & Activities / General', 'Fiction / Family Life',\n",
    "#        'Comics & Graphic Novels / General', 'Fiction / City Life',\n",
    "#        'Biography & Autobiography / Literary Figures',\n",
    "#        'Juvenile Fiction / Short Stories', 'Fiction / Crime',\n",
    "#        'Travel / Essays & Travelogues',\n",
    "#        'Technology & Engineering / General', 'Drama / Shakespeare',\n",
    "#        'History / Historiography', 'Bibles / General',\n",
    "#        'History / Indigenous Peoples of the Americas',\n",
    "#        'Cooking / Individual Chefs & Restaurants',\n",
    "#        'Performing Arts / General', 'Fiction / Noir', 'Poetry / General',\n",
    "#        'History / Military', 'Cooking / General', 'Travel / General',\n",
    "#        'Music / General', 'Sports & Recreation / General',\n",
    "#        'True Crime / General', 'Religion / Christian Theology',\n",
    "#        'Language Arts & Disciplines / General',\n",
    "#        'Crafts & Hobbies / General', 'Pets / General',\n",
    "#        'Young Adult Nonfiction / General', 'House & Home / General',\n",
    "#        'Literary Collections / General', 'Humor / General',\n",
    "#        'Antiques & Collectibles / General', 'Study Aids / General',\n",
    "#        'Foreign Language Study / General', 'Medical / General',\n",
    "#        'Law / General', 'Mathematics / General',\n",
    "#        'History / Historical Geography', 'Architecture / General',\n",
    "#        'Transportation / General', 'Gardening / General',\n",
    "#        'Design / General']\n",
    "\n",
    "# Profile one\n",
    "liked_genres = ['Mathematics / General']\n",
    "disliked_genres = []\n",
    "\n",
    "# Profile two\n",
    "# liked_genres = ['Religion / General' [ 'Medical / General']\n",
    "# disliked_genres = [] # base has some 'Family & Relationships / General' books\n",
    "# disliked_genres = ['Family & Relationships / General'] # Commenting this in actually gave more of these as recs????\n",
    "\n",
    "# Profile three\n",
    "# liked_genres = ['Law / General', 'True Crime / General']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile four\n",
    "# liked_genres = ['Performing Arts / General', 'Humor / Topic']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile five (Popular)\n",
    "# liked_genres = ['Religion / General'] #, 'Fiction / World Literature']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile five (Un-Popular)\n",
    "# liked_genres = ['Study Aids / General', 'Young Adult Nonfiction / General']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile six (Un-Popular)\n",
    "# liked_genres = ['Nature / General', ' Health & Fitness / General']\n",
    "# disliked_genres = []\n",
    "\n",
    "# Profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755a8de-6a01-4c0a-ad1b-7b5c108ddc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_books = books_df[books_df['genre_consolidated'].isin(liked_genres)].sample(5)\n",
    "\n",
    "sampled_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a98f6-69f6-4aff-9a05-b6911c159cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_books['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175ab86-f142-4bba-8b03-03752b7821a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each book\n",
    "\n",
    "sampled_encoded_liked_title = book_title_vocab_layer(sampled_books['title']).numpy()\n",
    "sampled_encoded_liked_authors = book_authors_vocab_layer(sampled_books['author']).numpy()\n",
    "sampled_encoded_liked_genres = book_genre_vocab_layer(sampled_books['genre_consolidated']).numpy()\n",
    "\n",
    "sampled_encoded_disliked_title = np.array([], dtype='int64')\n",
    "sampled_encoded_disliked_authors = np.array([], dtype='int64')\n",
    "sampled_encoded_disliked_genres = np.array([], dtype='int64')\n",
    "\n",
    "sampled_liked_rating = [6 for title in sampled_encoded_liked_title]\n",
    "\n",
    "encoded_disliked_genres = []\n",
    "\n",
    "for genre in disliked_genres:\n",
    "    encoded_disliked_genres.append(book_genre_vocab_layer(genre).numpy())\n",
    "    \n",
    "disliked_ratings = [1 for title in encoded_disliked_genres]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05713d-802e-42c1-a10b-5b05a5842a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_encoded_liked_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cfb84-769e-4cc1-a618-39590f8d0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_encoded_disliked_title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b2e94-f97d-4445-b333-2e52c8ffa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_info = ({\n",
    "    'user_id': [0], # Doesn't matter\n",
    "    'liked_books': sampled_encoded_liked_title.tolist(),\n",
    "    'disliked_books': [],\n",
    "\n",
    "    'liked_genres': sampled_encoded_liked_genres.tolist(),\n",
    "    'disliked_genres': encoded_disliked_genres,\n",
    "    \n",
    "    'liked_authors': sampled_encoded_liked_authors.tolist(),\n",
    "    'disliked_authors':[],\n",
    "    \n",
    "    'liked_ratings': sampled_liked_rating,\n",
    "    'disliked_ratings': disliked_ratings,\n",
    "})\n",
    "\n",
    "sample_user = []\n",
    "\n",
    "for col in sample_user_info:\n",
    "\n",
    "    # print(col, sample_user_info[col])\n",
    "    \n",
    "    sample_user_info[col].extend([0]*(20 - len(sample_user_info[col])))\n",
    "\n",
    "    # print(sample_user_info[col])\n",
    "    \n",
    "    sample_user.append(sample_user_info[col])\n",
    "\n",
    "sample_user = tf.cast(sample_user, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592e8a6-5e87-4828-adaf-b6484bc047d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703486c-72df-45bd-b982-88be199ad8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.user_model.predict([[i for i in range(20)], [i for i in range(20)], [i for i in range(20)]])\n",
    "\n",
    "user_embedding = model.user_model.predict(sample_user)\n",
    "\n",
    "# user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992b639-face-45b9-b3db-2db6223dc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44aca1-c7fe-4f05-b434-2f2e51e3d88f",
   "metadata": {},
   "source": [
    "Extract the books' embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceefb14-d12c-4e9d-bb23-a13bf10ebf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tower = model.book_model\n",
    "\n",
    "book_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'target_book': tf.constant(numerical_books_df['title'].tolist(), dtype=tf.int64),\n",
    "    'authors': tf.constant(numerical_books_df['authors'].tolist(), dtype=tf.int64),\n",
    "    'categories': tf.constant(numerical_books_df['categories'].tolist(), dtype=tf.int64),\n",
    "    'description': tf.constant(numerical_books_df['description'].tolist(), dtype=tf.int64),\n",
    "}).batch(128)  # Optional batching\n",
    "\n",
    "book_embeddings = []\n",
    "\n",
    "for batch in book_dataset:\n",
    "    batch_embeddings = book_tower(batch)\n",
    "    book_embeddings.append(batch_embeddings)\n",
    "\n",
    "book_embeddings = tf.concat(book_embeddings, axis=0)\n",
    "print(\"Book Embeddings Shape:\", book_embeddings.shape)\n",
    "\n",
    "books_df_unique['Embeddings'] = [embed.numpy() for embed in book_embeddings]\n",
    "\n",
    "books_df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a5817-ac30-49c9-b067-8306b324021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_unique.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029d6a9-b187-475d-8fe8-302700863f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argsort(cos_similarities[0])[-40:][::-1]\n",
    "cos_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b865893-c577-41fa-aebf-2c0c8e7a1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "filtered_books_df_unique = books_df_unique\n",
    "\n",
    "book_embeddings = np.array(filtered_books_df_unique['Embeddings'].tolist())\n",
    "\n",
    "# Normalize user embedding to unit vector (L2 normalization)\n",
    "# user_embedding /= np.linalg.norm(user_embedding)\n",
    "\n",
    "# Normalize all book embeddings to unit vectors (L2 normalization)\n",
    "# book_embeddings /= np.linalg.norm(book_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Calculate the cosine similarity between user_embedding and all book embeddings\n",
    "cos_similarities = cosine_similarity(user_embedding.reshape(1, -1), book_embeddings)\n",
    "#cos_similarities = cosine_similarity(test, book_embeddings)\n",
    "# Get the indices of the top 6 closest books\n",
    "top_k_indices = np.argsort(cos_similarities[0])[-40:][::-1]  # Top 6 indices with highest similarity\n",
    "\n",
    "# Print the results (book indices and cosine similarity scores)\n",
    "for i, idx in enumerate(top_k_indices):\n",
    "\n",
    "    print(f\"{filtered_books_df_unique.iloc[idx]['title']} | {filtered_books_df_unique.iloc[idx]['genre_consolidated']} | {cos_similarities[0][idx]}\")\n",
    "    # print(f\"Recommendation {i+1}: Book Index {idx} (Cosine Similarity: {cos_similarities[0][idx]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fc091-8723-4a8c-a2ef-68f1e5dfc609",
   "metadata": {},
   "source": [
    "Mimic subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167112a-072c-4bf3-a20a-2db2047df630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map recs to each category\n",
    "\n",
    "# Algebra (Actualites scientifiques et industrielles) | Mathematics / General\n",
    "# College algebra, | Mathematics / General\n",
    "# Fundamentals of Data Structures in C++ | Computers / General\n",
    "# Finite Element Analysis: From Concepts to Applications | Mathematics / General\n",
    "# Independent Component Analysis: A Tutorial Introduction (Bradford Books) | Mathematics / General\n",
    "# Modern Algebra | Mathematics / General\n",
    "\n",
    "sub_liked_books = [\n",
    "    \"Algebra (Actualites scientifiques et industrielles)\",\n",
    "    \"College algebra,\",\n",
    "    \"Fundamentals of Data Structures in C++\",\n",
    "    \"Finite Element Analysis: From Concepts to Applications\",\n",
    "    \"Independent Component Analysis: A Tutorial Introduction (Bradford Books)\",\n",
    "    \"Modern Algebra\"\n",
    "]\n",
    "sub_disliked_books = [\n",
    "    \"CFS traced to childhood trauma, emotional instability, stress.(Across Specialties)(chronic fatigue syndrome): An article from: Clinical Psychiatry News\"\n",
    "]\n",
    "\n",
    "# Next, extract the encoded metadata of each book\n",
    "\n",
    "sub_book_titles = book_title_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['title']).numpy()\n",
    "sampled_encoded_liked_title = np.concatenate([sampled_encoded_liked_title, sub_book_titles])\n",
    "\n",
    "sub_book_authors = book_authors_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['author']).numpy()\n",
    "sampled_encoded_liked_authors = np.concatenate([sampled_encoded_liked_authors, sub_book_authors])\n",
    "\n",
    "sub_book_genres = book_genre_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['genre_consolidated']).numpy()\n",
    "sampled_encoded_liked_genres = np.concatenate([sampled_encoded_liked_genres, sub_book_genres])\n",
    "\n",
    "#####\n",
    "\n",
    "sub_book_titles_dis = book_title_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_disliked_books)]['title']).numpy()\n",
    "sampled_encoded_disliked_title = np.concatenate([sampled_encoded_disliked_title, sub_book_titles_dis])\n",
    "\n",
    "sub_book_authors_dis = book_authors_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_disliked_books)]['author']).numpy()\n",
    "sampled_encoded_disliked_authors = np.concatenate([sampled_encoded_disliked_authors, sub_book_authors_dis])\n",
    "\n",
    "sub_book_genres_dis = book_genre_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_disliked_books)]['genre_consolidated']).numpy()\n",
    "sampled_encoded_disliked_genres = np.concatenate([sampled_encoded_disliked_genres, sub_book_genres_dis])\n",
    "\n",
    "####\n",
    "\n",
    "# sampled_encoded_disliked_title.extend(book_title_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['title']).numpy()\n",
    "# sampled_encoded_disliked_authors.extend(book_authors_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['author']).numpy()\n",
    "# sampled_encoded_disliked_genres.extend(book_genre_vocab_layer(books_df_unique[books_df_unique['title'].isin(sub_liked_books)]['genre_consolidated']).numpy()\n",
    "\n",
    "sampled_liked_rating = [6 for title in sampled_encoded_liked_title]\n",
    "    \n",
    "sampled_disliked_ratings = [1 for title in sampled_encoded_disliked_title]\n",
    "\n",
    "# sampled_encoded_liked_title, sampled_encoded_liked_authors, sampled_encoded_liked_genres, sampled_encoded_disliked_title,sampled_encoded_disliked_authors, sampled_encoded_disliked_genres, sampled_liked_rating, sampled_disliked_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1dc02-51f7-4ca8-a595-ebdea926ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_encoded_liked_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44867e3-da55-4685-9f36-6567f29288d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_encoded_disliked_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4b876-9503-485c-a80a-c671dd5083a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_info = ({\n",
    "    'user_id': [0], # Doesn't matter\n",
    "    'liked_books': sampled_encoded_liked_title.tolist(),\n",
    "    'disliked_books': sampled_encoded_disliked_title.tolist(),\n",
    "\n",
    "    'liked_genres': sampled_encoded_liked_genres.tolist(),\n",
    "    'disliked_genres': sampled_encoded_disliked_genres.tolist(),\n",
    "    \n",
    "    'liked_authors': sampled_encoded_liked_authors.tolist(),\n",
    "    'disliked_authors': sampled_encoded_disliked_authors.tolist(),\n",
    "    \n",
    "    'liked_ratings': sampled_liked_rating,\n",
    "    'disliked_ratings': sampled_disliked_ratings\n",
    "})\n",
    "\n",
    "sample_user = []\n",
    "\n",
    "for col in sample_user_info:\n",
    "\n",
    "    # print(col, sample_user_info[col])\n",
    "    \n",
    "    sample_user_info[col].extend([0]*(20 - len(sample_user_info[col])))\n",
    "\n",
    "    # print(sample_user_info[col])\n",
    "    \n",
    "    sample_user.append(sample_user_info[col])\n",
    "\n",
    "sample_user = tf.cast(sample_user, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ddbb8-4eb5-45a9-8ece-a9263b6a6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.user_model.predict([[i for i in range(20)], [i for i in range(20)], [i for i in range(20)]])\n",
    "\n",
    "user_embedding = model.user_model.predict(sample_user)\n",
    "\n",
    "# user_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d343cbeb-ecdb-4e61-b84d-1953690ee301",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df_unique.shape, filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a46039-d944-42aa-84f5-5a7da36f4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_liked_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328a853-bea8-4018-86af-27b9f89b10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_books = sub_liked_books\n",
    "excluded_books.extend(sub_disliked_books)\n",
    "\n",
    "# Assuming your DataFrame is called df and the column containing books is 'book_id'\n",
    "filtered_df = books_df_unique[~books_df_unique['title'].isin(excluded_books)]\n",
    "\n",
    "book_embeddings = np.array(filtered_books_df_unique['Embeddings'].tolist()) \n",
    "\n",
    "# Normalize user embedding to unit vector (L2 normalization)\n",
    "# user_embedding /= np.linalg.norm(user_embedding)\n",
    "\n",
    "# Normalize all book embeddings to unit vectors (L2 normalization)\n",
    "# book_embeddings /= np.linalg.norm(book_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Calculate the cosine similarity between user_embedding and all book embeddings\n",
    "cos_similarities = cosine_similarity(user_embedding.reshape(1, -1), book_embeddings)\n",
    "\n",
    "# Get the indices of the top 6 closest books\n",
    "top_k_indices = np.argsort(cos_similarities[0])[-20:][::-1]  # Top 6 indices with highest similarity\n",
    "\n",
    "# Print the results (book indices and cosine similarity scores)\n",
    "for i, idx in enumerate(top_k_indices):\n",
    "\n",
    "    print(f\"{filtered_books_df_unique.iloc[idx]['title']} | {filtered_books_df_unique.iloc[idx]['genre_consolidated']}\")\n",
    "    # print(f\"Recommendation {i+1}: Book Index {idx} (Cosine Similarity: {cos_similarities[0][idx]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26255d6-ca62-4c33-8556-75b23055c7f9",
   "metadata": {},
   "source": [
    "## Saving Auxilliary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11897f15-fc83-4428-8784-ef16e714959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tower = model.book_model\n",
    "\n",
    "book_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'target_book': tf.constant(numerical_books_df['title'].tolist(), dtype=tf.int64),\n",
    "    'authors': tf.constant(numerical_books_df['authors'].tolist(), dtype=tf.int64),\n",
    "    'categories': tf.constant(numerical_books_df['categories'].tolist(), dtype=tf.int64),\n",
    "    'description': tf.constant(numerical_books_df['description'].tolist(), dtype=tf.int64),\n",
    "}).batch(128)  # Optional batching\n",
    "\n",
    "book_embeddings = []\n",
    "\n",
    "for batch in book_dataset:\n",
    "    batch_embeddings = book_tower(batch)\n",
    "    book_embeddings.append(batch_embeddings)\n",
    "\n",
    "book_embeddings = tf.concat(book_embeddings, axis=0)\n",
    "print(\"Book Embeddings Shape:\", book_embeddings.shape)\n",
    "\n",
    "books_df_unique['Embeddings'] = [embed.numpy() for embed in book_embeddings]\n",
    "\n",
    "books_df_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef9be4-fc3c-4b6f-94d5-fa99bf73963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9302e-f7a4-4d4a-9e6a-689b22d24cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"book_embeddings.npy\", book_embeddings.numpy())\n",
    "books_df.to_csv('books_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4dcfbb-c5ea-4fa1-bcf4-844e19e5c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.tar.gz to required S3 bucket\n",
    "#s3://w210recsys/model/recModel/modelFiles/\n",
    "bucket_name=\"w210recsys\"\n",
    "key_prefix=\"model/recModel/modelFiles\"\n",
    "s3_response = sm_session.upload_data(\"book_embeddings.npy\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cd476-2b9a-4fe8-8863-f78c65d6e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model.tar.gz to required S3 bucket\n",
    "#s3://w210recsys/model/recModel/modelFiles/\n",
    "bucket_name=\"w210recsys\"\n",
    "key_prefix=\"model/recModel/modelFiles\"\n",
    "s3_response = sm_session.upload_data(\"books_df.csv\", bucket=bucket_name, key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2dfe8-e336-40e1-9513-32531d9b6898",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69205f3-ddd5-441f-8e28-6ca195eb7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tower = model.user_model\n",
    "book_tower = model.book_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a44d4-202f-4881-b9b4-0142d793b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_model(dataset):\n",
    "    return model.evaluate(dataset, return_dict=True)\n",
    "\n",
    "metrics = evaluate_model(num_test_ds_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7b693-90ae-48f1-ac4d-467649392cb6",
   "metadata": {},
   "source": [
    "## Save Model + Book Embeddings & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ce75a6-9306-4032-8609-4306cafb897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Save both models into separate folders\n",
    "model.user_model.save(\"export/user_model/1\")   # Save user model\n",
    "#model.book_model.save(\"export/book_model/1\")   # Save book model\n",
    "\n",
    "# Create tar.gz archive\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\", arcname=os.path.basename(\"export\"))\n",
    "\n",
    "print(\"âœ… Both models saved and compressed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1ab73-2f61-4693-ba0e-eaa1bf2afdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Save both models into separate folders\n",
    "model.save(\"export/parent_model/1\")   # Save parent model\n",
    "\n",
    "# Create tar.gz archive\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\", arcname=os.path.basename(\"export\"))\n",
    "\n",
    "print(\"âœ… Both models saved and compressed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3abca-b352-4e62-82a7-26cc93fbb2e7",
   "metadata": {},
   "source": [
    "Push the tar.gz files to the s3 bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
