{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from typing import List, Union, Dict, Text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import plotnine\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import books and reviews dataset\n",
    "data_location = 's3://w210recsys/book_raw/books_data.csv'\n",
    "review_location = 's3://w210recsys/book_raw/Books_rating.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_csv(data_location) \n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(review_location) \n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_df['Title'].nunique()) \n",
    "books_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = books_df.drop_duplicates(subset=['Title']).reset_index(drop=True)\n",
    "books_df.dropna(subset=['Title'], inplace=True)\n",
    "books_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ratings_df.dropna(subset=['Title', 'User-ID']).reset_index(drop=True)\n",
    "ratings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['review_date'] = pd.to_datetime(ratings_df['review/time'], unit='s')\n",
    "ratings_df = ratings_df.drop(columns=['review/time'])\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns titles to lower case\n",
    "books_df.columns = books_df.columns.str.lower()\n",
    "print(f'Books DF columns: {books_df.columns}')\n",
    "\n",
    "ratings_df.columns = ratings_df.columns.str.lower()\n",
    "print(f'Ratings DF columns: {ratings_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format column title\n",
    "ratings_df.columns = ratings_df.columns.str.replace('/', '_')\n",
    "print(f'Ratings DF columns: {ratings_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_date = (ratings_df['review_date'].max() - datetime.timedelta(days=365)).date()\n",
    "validation_start_date = pd.Timestamp(validation_start_date)\n",
    "validation_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RAM Killer ####\n",
    "# Define file paths (on Google Drive or local path)\n",
    "# train_file_path = \"/home/sagemaker-user/train_df.csv\"\n",
    "# test_file_path = \"/home/sagemaker-user/test_df.csv\"\n",
    "\n",
    "train_file_path = \"s3://w210recsys/book_raw/train_df.csv\"\n",
    "test_file_path = \"s3://w210recsys/book_raw/test_df.csv\"\n",
    "\n",
    "# Check if the train/test files exist\n",
    "if not os.path.exists(train_file_path) or not os.path.exists(test_file_path):\n",
    "    print(\"Train/test split files do not exist. Creating them now...\")\n",
    "\n",
    "    # Sort the dataframe by user_id and timestamp\n",
    "    ratings_df = ratings_df.sort_values(by=['user_id', 'review_date'])\n",
    "\n",
    "    # Create train/test splits using groupby and apply with progress bar\n",
    "    train_df = ratings_df.groupby('user_id').progress_apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n",
    "    test_df = ratings_df.groupby('user_id').progress_apply(lambda x: x.iloc[-1:]).reset_index(drop=True)\n",
    "\n",
    "    # Save the splits to CSV files on Google Drive\n",
    "    train_df.to_csv(train_file_path, index=False)\n",
    "    test_df.to_csv(test_file_path, index=False)\n",
    "\n",
    "    print(f\"Training set saved at: {train_file_path}\")\n",
    "    print(f\"Test set saved at: {test_file_path}\")\n",
    "else:\n",
    "    print(f\"Train/test split files already exist. Loading them...\")\n",
    "\n",
    "    # Load the saved train/test splits from CSV files\n",
    "    train_df = pd.read_csv(train_file_path)\n",
    "    test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets into tensor datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(dict(train_df[['user_id', 'title', 'review_score']]))\n",
    "\n",
    "for x in train_ds.take(5).as_numpy_iterator():\n",
    "    print(x)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(dict(test_df[['user_id', 'title', 'review_score']]))\n",
    "\n",
    "for x in test_ds.take(5).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Vocabularies\n",
    "unique_user_ids = train_df['user_id'].unique()\n",
    "unique_titles = train_df['title'].unique()\n",
    "unique_review_scores = train_df['review_score'].unique()\n",
    "\n",
    "# Candidates for retrieval Task\n",
    "candidate_ds = tf.data.Dataset.from_tensor_slices(dict(\n",
    "    train_df[['title']].drop_duplicates()\n",
    "))\n",
    "\n",
    "for x in candidate_ds.take(5).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache train dataset & Candidate dataset\n",
    "train_size = train_df.shape[0]\n",
    "cached_train = train_ds.shuffle(train_size).batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User/Query Model\n",
    "class UserModel(tf.keras.Model):\n",
    "    '''\n",
    "    The user(query) tower\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_user_ids: np.ndarray,\n",
    "                 feature_user_id_name: str,\n",
    "                 embedding_dimensions: int):\n",
    "        '''\n",
    "        Params\n",
    "        :param unique_user_ids: array of unique user ids\n",
    "        :param feature_user_id_name: name of the feature\n",
    "        :param embedding_dimension: number of dimensions in embedding layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.feature_user_id_name = feature_user_id_name\n",
    "\n",
    "        self.user_embedding_layers = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_user_ids,\n",
    "                    mask_token=None,\n",
    "                    name='user_id_vocab',\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(unique_user_ids) + 1,\n",
    "                    output_dim=embedding_dimensions,\n",
    "                    name='user_id_embedding',\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def call(self, inputs: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        return self.user_embedding_layers(inputs[self.feature_user_id_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookModel(tf.keras.Model):\n",
    "    '''\n",
    "    The book(query) tower\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_titles: np.ndarray,\n",
    "                 feature_book_title_name: str,\n",
    "                 embedding_dimensions: int,\n",
    "                 text_vectorization_max_tokens: int):\n",
    "        '''\n",
    "        Params\n",
    "        :param unique_titles: array of unique titles\n",
    "        :param unique_review_scores: array of unique review scores\n",
    "        :param feature_book_title_name: name of the column title\n",
    "        :param embedding_dimensions: number of dimensions in embedding layer\n",
    "        :param text_vectorization_max_tokens: maximum number of tokens to vector\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.feature_book_title_name = feature_book_title_name\n",
    "\n",
    "        # Book Title embedding\n",
    "        self.book_embedding_layers = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_titles,\n",
    "                    mask_token=None,\n",
    "                    name='book_id_vocab',\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    input_dim=len(unique_titles) + 1,\n",
    "                    output_dim=embedding_dimensions,\n",
    "                    name='book_id_embedding',\n",
    "                ),\n",
    "            ],\n",
    "            name='book_id_embedding',\n",
    "        )\n",
    "\n",
    "    def call(self, inputs: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        return tf.concat([\n",
    "            self.book_embedding_layers(inputs[self.feature_book_title_name]),\n",
    "            # add more embedding layers as needed\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run the book model on the dataset and save the embeddings \n",
    "# book_model = BookModel(\n",
    "#     unique_titles=unique_titles,\n",
    "#     feature_book_title_name='title',\n",
    "#     embedding_dimensions=32,\n",
    "#     text_vectorization_max_tokens=10000,\n",
    "# )\n",
    "\n",
    "# book_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# book_model.fit(cached_train, epochs=1)\n",
    "# # save the book embeddings\n",
    "# book_embeddings = book_model.layers[0].get_weights()[0]\n",
    "# book_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooksTwoTowersModel(tfrs.Model):\n",
    "    '''\n",
    "    Two-Towers books recommender model\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 unique_user_ids: np.ndarray,\n",
    "                 unique_titles: np.ndarray,\n",
    "                 unique_review_scores: np.ndarray,\n",
    "                 candidate_ds: tf.data.Dataset,\n",
    "                 feature_user_id_name: str = 'user_id',\n",
    "                 feature_book_title_name: str = 'title',\n",
    "                 feature_review_score_name: str = 'review_score',\n",
    "                 embedding_dimensions: int = 64):\n",
    "        '''\n",
    "        Instantiate query tower, candidate tower, and retrieval task.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.feature_user_id_name = feature_user_id_name\n",
    "        self.feature_book_title_name = feature_book_title_name\n",
    "        self.feature_review_score_name = feature_review_score_name\n",
    "\n",
    "        # Query Tower\n",
    "        self.user_model = UserModel(\n",
    "            unique_user_ids=unique_user_ids,\n",
    "            feature_user_id_name=feature_user_id_name,\n",
    "            embedding_dimensions=embedding_dimensions,\n",
    "        )\n",
    "\n",
    "        # Candidate Tower\n",
    "        text_vectorization_max_tokens = len(unique_titles) + len(unique_review_scores)\n",
    "\n",
    "        book_model_raw = BookModel(\n",
    "            unique_titles=unique_titles,\n",
    "            feature_book_title_name=feature_book_title_name,\n",
    "            embedding_dimensions=embedding_dimensions,\n",
    "            text_vectorization_max_tokens=text_vectorization_max_tokens,\n",
    "        )\n",
    "\n",
    "        # Dense projection layer to equate final tower output dims\n",
    "        self.book_model = tf.keras.Sequential(\n",
    "            [\n",
    "                book_model_raw,\n",
    "                tf.keras.layers.Dense(\n",
    "                    units=embedding_dimensions,\n",
    "                    name='book_dense_projection',\n",
    "                ),\n",
    "            ],\n",
    "            name='book_sequential',\n",
    "        )\n",
    "\n",
    "        # Retrieval Task\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=candidate_ds.batch(128).map(self.book_model),\n",
    "                ks=(10, 20, 50)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self,\n",
    "                     features: Dict[Text, tf.Tensor],\n",
    "                     training=False) -> tf.Tensor:\n",
    "        '''\n",
    "        Get embeddings for users and books.\n",
    "        Compute dot product and retrieve candidates.\n",
    "        '''\n",
    "        user_embeddings = self.user_model({\n",
    "            self.feature_user_id_name: features[self.feature_user_id_name],\n",
    "        })\n",
    "\n",
    "        book_embeddings = self.book_model({\n",
    "            self.feature_book_title_name: features[self.feature_book_title_name],\n",
    "        })\n",
    "\n",
    "        # Sample weight logic\n",
    "        review_scores = tf.cast(features[self.feature_review_score_name], tf.float32)\n",
    "        sample_weight = tf.where(review_scores >= 4, 1.0, 0.0)\n",
    "\n",
    "        return self.task(user_embeddings, book_embeddings, compute_metrics=not training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup log dir for tensorboard\n",
    "LOG_DIR = \"/home/sagemaker-user/logs\"\n",
    "\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model = BooksTwoTowersModel(\n",
    "    unique_user_ids=unique_user_ids,\n",
    "    unique_titles=unique_titles,\n",
    "    unique_review_scores=unique_review_scores,\n",
    "    candidate_ds=candidate_ds,\n",
    "    embedding_dimensions=64,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(\n",
    "    cached_train,\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BruteForce layer\n",
    "index = tfrs.layers.ann.BruteForce(model.user_model)\n",
    "index.index(cached_train.batch(4096).map(lambda x: x['user_id']), model.user_model)\n",
    "\n",
    "\n",
    "index.index_from_dataset(tf.convert_to_tensor(unique_user_ids), model.user_model)\n",
    "\n",
    "index.index_from_dataset(\n",
    "    tf.data.Dataset.from_tensor_slices(dict(train_df[['user_id']])).batch(4096).map(lambda x: x['user_id']),\n",
    "    model.user_model\n",
    ")\n",
    "\n",
    "# Get recommendations\n",
    "_, titles = index(np.array(['276726']))\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.user_model as a .pkl file\n",
    "import pickle\n",
    "\n",
    "# Save the user model\n",
    "with open('user_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model.user_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the book embeddings\n",
    "book_embeddings = model.book_model.layers[0].get_weights()[0]\n",
    "book_embeddings.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = pd.DataFrame({\n",
    "    'user_id': test_df['user_id'].sample(1).values,\n",
    "    'title': test_df['title'].sample(1).values,\n",
    "    'review_score': test_df['review_score'].sample(1).values,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the full test set\n",
    "test_predictions = model.predict(test_ds.batch(4096))\n",
    "\n",
    "# Compute metrics\n",
    "metrics = model.evaluate(test_ds.batch(4096), return_dict=True)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test user\n",
    "test_user = pd.DataFrame({\n",
    "    'user_id': [1] * len(unique_titles),\n",
    "    'title': unique_titles,\n",
    "    'review_score': [0] * len(unique_titles),\n",
    "})\n",
    "\n",
    "\n",
    "# generate user embeddings based on test_user, load the book embeddings and predict recommendations\n",
    "test_user_ds = tf.data.Dataset.from_tensor_slices(dict(test_user))\n",
    "test_user_embeddings = model.user_model({\n",
    "    'user_id': test_user_ds.map(lambda x: x['user_id']),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useing amazon sagemaker deploy the model\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import boto\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Get the default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Save the model\n",
    "model.save(\"model\")\n",
    "\n",
    "# Upload the model to S3\n",
    "model_path = sagemaker_session.upload_data(\"model\", bucket, key_prefix=\"model\")\n",
    "\n",
    "# Create a TensorFlowModel\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    framework_version=\"2.4.1\",\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embeddigns\n",
    "user_embeddings = model.user_model.user_embedding_layers.get_weights()[0]\n",
    "book_embeddings = model.book_model.layers[0].book_embedding_layers.get_weights()[0]\n",
    "\n",
    "# Save the embeddings\n",
    "np.save('user_embeddings.npy', user_embeddings)\n",
    "np.save('book_embeddings.npy', book_embeddings)\n",
    "\n",
    "# Load the embeddings\n",
    "user_embeddings = np.load('user_embeddings.npy')\n",
    "book_embeddings = np.load('book_embeddings.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '210-api-9v8zSIhV-py3.11 (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/zfenton/.cache/pypoetry/virtualenvs/210-api-9v8zSIhV-py3.11/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class BooksRecommendation(tfrs.layers.factorized_top_k.TopK):\n",
    "    def __init__(self, user_embeddings: np.ndarray, book_embeddings: np.ndarray, k: int):\n",
    "        super().__init__(k=k)\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.book_embeddings = book_embeddings\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_id = inputs['user_id']\n",
    "        user_embedding = tf.gather(self.user_embeddings, user_id)\n",
    "        return self._call(user_embedding, self.book_embeddings)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'user_embeddings': self.user_embeddings, 'book_embeddings': self.book_embeddings, 'k': self.k}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.k)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "    \n",
    "    def compute_output_signature(self, input_signature):\n",
    "        return tf.TensorSpec(shape=(input_signature['user_id'].shape[0], self.k), dtype=tf.float32)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'user_embeddings': self.user_embeddings, 'book_embeddings': self.book_embeddings, 'k': self.k}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.k)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revrieval_model = BooksRecommendation(user_embeddings, book_embeddings, k=10)\n",
    "\n",
    "# Get recommendations for a user\n",
    "user_id = 1\n",
    "user_embedding = user_embeddings[user_id]\n",
    "recommended_books = revrieval_model({'user_id': user_id})\n",
    "recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tfrs.layers.ann.BruteForce(user_embeddings, book_embeddings, metric='cosine')\n",
    "index.index(cached_train.map(lambda x: x['title']))\n",
    "\n",
    "# Get recommendations\n",
    "_, titles = index(np.array(['276726']))\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve recommendations for top 10\n",
    "recommendations = model.recommend(cached_train, k=10)\n",
    "recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "210-api-9v8zSIhV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
